{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7: Regularization - Bagging, Early Stopping and Dropout (deadline: 22 Dec, 23:59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1. Regularization: Bagging (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Study the effects of **bagging** regularization on Decision Tree based methods against a single instance of such a classifier.\n",
    "\n",
    "Bagging, briefly mentioned in the Lecture 6, refers to an ensemble machine learning method. The Bagging scheme, suggested to be used in this exercise, samples instances from the training data with replacement and creates multiple training subsets. For each of these subsets, a new regressor is constructed internally and finally, all combined to produce the result. For more details read: \n",
    "\n",
    "1. Scikit Learn Documentation for Bagging Regressor. http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html#id6\n",
    "\n",
    "2. Bootstrap Aggregating Wikipedia Article. https://en.wikipedia.org/wiki/Bootstrap_aggregating\n",
    "\n",
    "Implement a bagging regularization scheme using ***DecisionTreeRegressor***, a Decision Tree based classifier from the python package ***sklearn.tree***. To implement the bagging scheme you can use ***BaggingRegressor*** available in the python package ***sklearn.ensemble***. Fill in the code pieces marked by \"# TODO\" in the following notebook to complete this assignment. Finally, comment on the results you obtain.\n",
    "\n",
    "Note: to run the following code you will need to download **data.csv** from the NNIA's resource page on Piazza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define Estimators: Create an array of two estimators.\n",
    "# First a \"Tree\" using \"DecisionTreeRegressor\" and second a Regularized version obtain using \"BaggingRegressor\"\n",
    "# on this Tree, labelled \"Bagging (Tree)\". (2 points)\n",
    "\n",
    "estimators = [(\"Tree\", DecisionTreeRegressor()#TODO\n",
    "              ),\n",
    "              (\"Bagging (Tree)\", BaggingRegressor(DecisionTreeRegressor())#TODO\n",
    "              )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_estimators = len(estimators)\n",
    "np.random.seed(0)\n",
    "\n",
    "#Load Data\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "#Drop Player Field\n",
    "data = data.drop('Player', axis=1)\n",
    "\n",
    "#Set variable y with 'Salary' column and then drop it from data\n",
    "y = data['Salary'].as_matrix()\n",
    "data = data.drop('Salary', axis=1)\n",
    "\n",
    "#Convert data to an numpy array x\n",
    "x = data.as_matrix()\n",
    "\n",
    "# Split x in X_train (80 %) and X_test (20 %),  while constructing the corresponding y_train and y_test\n",
    "n_train = np.int(0.8 * len(x) )\n",
    "n_test = len(x) - n_train\n",
    "X_train = x[:n_train,:]\n",
    "y_train = y[:n_train]\n",
    "X_test = x[n_train:,:]\n",
    "y_test = y[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree: 152196.4062 (error)\n",
      "Bagging (Tree): 64003.2045 (error)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoEAAADgCAYAAAB8ZLxuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJztnXeYXVXVh981M6SRQhJCSELowY/QISCCnyiB0KRKVSAI\nggooFlTAgiJFREURgS/SQYGIlEgPIAooYIAgJIAJAUwlIQkppM7M+v5YZ3PP3Nw+t55Z7/Pc5567\nT9vnlnV/Z5W9RVVxHMdxHMdxuhZNte6A4ziO4ziOU31cBDqO4ziO43RBXAQ6juM4juN0QVwEOo7j\nOI7jdEFcBDqO4ziO43RBXAQ6juM4juN0QVwEOo7jOE6dIyJTROTTFTr2IBF5Q0R6VuL4ec79GxE5\nvdrndQwXgU6XQESWxx7tIrIy9voLte6f4zj1j4i8E7Mdi0XkQREZXo1zq+p2qvpUhQ5/HnCzqq6M\nxGawjW0isir2+oIKnPsK4Ici0lKBYzt5cBHodAlUtXd4AP8FDo21/SF9ezdIjuNk4dDIjgwB3gN+\nW+P+dAoR6Q6MBW6Hj8RmsJVPA2fHbOWlGfbvlK1U1VnAW8BnO3McpzRcBDoOICIXi8hdInKHiCwD\nThSRJhG5QETeEpH3ReROEekf22dvEXlORD4Qkcki8qkaXoLjOFVEVVcBdwMjQ5uIHCIiL4vIUhGZ\nKSI/ju8jIieLyLsislBEfhh5FveL1vUUkVsiD+PrIvJdEZkV2ze+7Y9FZLyI3CoiyyLv3ajYtrtG\n/VgmIn+KbNvFWS7l48AHkRjLi4h8SUT+LiJXicgi4Aex9jei/j8c95CKyEgReVxEFkXbfC7tsE8B\nhxRyfqe8uAh0nBRHAn8E+gF3Ad/EDNOngE2A5cBVAJGBmwBcCAzAwin3iMjA6nfbcZxqIyK9gOOA\n52LNHwInAxtgtuOrInJEtP1I4BrgC5gXsR8wLLbvhcDmwJbA/sCJebpwGHBndK4JwNXReboB9wI3\nY7bpDsy2ZWMH4M0850pnL+B1YBBweSTqvgMcHrU9j9lSRKQ3MBG4FdgIu/5xIvKx2PFeB3Yqsg9O\nGXAR6DgpnlHVv6hqu6quBL4CXKCqs6O7/p8Ax4hIE2boJ6jqo9H2jwCvAAfWrvuO41SB+0TkA2AJ\nJtauCCtU9SlVfTWyCf/GBNg+0eqjgb+o6jOqugb4EaCx4x4LXKqqiyOv3FV5+vGMqj6kqm3AbaRE\n1J5AC3CVqq5V1XuAF3IcZwNgWQHXHee/qnqtqrbFbOWlqvqmqrYCFwN7iMgwTBj+R1VvVdVWVX0R\nuA97PwLLon44VcZFoOOkmJn2elPgL1G49wPg1ah9I2Az4ISwLlq/JzC0et11HKcGHKGqGwA9gLOB\nv4nIxgAi8nER+auILBCRJZg42jDabygxG6OqK4CFseN2WM+69iidebHlFUCPKD9vKDBbVeMCM9ex\nFgN98pwrnfTjbQb8LmYL3wfasQjKZsDeabbyOMwbGugDfFBkH5wy4CLQcVJo2utZwP6qukHs0UNV\n52FG8Ka0deur6hXrHtZxnKQRecHuAdqAT0bNf8RCs8NVtR9wHSDRurmYKAIsBxCIp490WA+UWnU8\nFxgmIhJry3WsfwPbFHmOdFs5EzgtzR72VNXno3VPpK3rrapnx/bfFoukOFXGRaDjZOc64FIR2RRA\nRDYSkcOidbcBR4rI/iLSLCI9ROQzIuKeQMfpAohxONAfy2kD82gtUtVVIrIH8PnYLncDh4rIXlHe\n3o9JCUSA8cD5ItI/CqPGRVIx/BMTpmeLSEvUxz1ybP8CsEF0zlK5Dvi+iGwLICIbiEgI904AthOR\nz4vIetFjj7ScwH2AhztxfqdEXAQ6TnZ+BTwCPBFVDP8D2B1AVd/Bkq1/CCzAhp35Nv6bcpyk8xcR\nWQ4sBS4BxqrqlGjdmcBFkb34ESbsAIi2+RpWzDEXKzSbD6yONrkIiz68DTyOicawrmCifMOjgNOw\nEOuJwAPZjhVtfzP5C1FynfNPmL38k4gsxbyLB0TrlkTLJ2LXPQ+4DOgOEInPEcBfSj2/UzrSMW3A\ncRzHcZxKE1XNfgCMUNW3M6z/KnC8qu6zzs7Fn+t54DpVvSnL+kHYmIC7RIUeVUNEfgNMUdVx1Tyv\nY7gIdBzHcZwqICKHAk9gYeBfYmP07aqqKiJDsOFh/ol5xh4ErlbVX5dwnn2wYV/ex4ZkuQ7YUlXn\nluVCnMTgsyI4juM4TnU4HMsnFmAS5ukLnphuwP8BW2AewjuxcQVL4WNYKHp9YAZwtAtAJxPuCXQc\nx3Ecx+mCeBK74ziO4zhOF8RFoOM4juM4ThfEcwILYMMNN9TNN9+81t1wGpFXX4U1a2DgQEjKd2jx\nYpgxAzbeGIZ1Zmix+uXFF198X1UH1bof5cDtl+N0PQq1YS4CC2DzzTdn0qRJte6G04gMGwZz5sD+\n+8Mdd9S6N+Xh9tvhpJPg85+HX/6y1r2pCCLybq37UC7cfjlO16NQG+bhYMepJGvWdHxOAqtXd3x2\nHMdxGhIXgY5TSZIoAsO1uAh0HMdpaFwEOk4lSaLXLInX5DiO0wVxEeg4lUI1mZ5AF4GO4ziJwEWg\n41SKtjYTguAi0HEcx6k7XAQ6TqWIC78kiUDPCXQcx0kELgIdp1LEhV+SBJN7Ah3HcRJBxUSgiNwo\nIvNF5LVY2xUi8oaI/FtE7hWRDWLrzheR6SLypogcEGvfTURejdZdJSIStXcXkbui9udFZPPYPmNF\nZFr0GBtr3yLadnq0b7dKXb/jdBBJSfIEhutataq2/agwbsMcx0k6lfQE3gwcmNY2EdheVXcE/gOc\nDyAiI4Hjge2ifa4RkeZon2uB04ER0SMc8zRgsapuDVwJXB4dawBwIfBxYA/gQhHpH+1zOXBltM/i\n6BiOUxmSGg7uOp7Am3Eb5jhOgqmYCFTVvwOL0toeU9XW6OVzwCbR8uHAnaq6WlXfBqYDe4jIEKCv\nqj6nqgrcChwR2+eWaPluYHR0h30AMFFVF6nqYsxoHxit2zfalmjfcCzHKT9B+PXokSwR2EVyAt2G\nOY6TdGqZE3gq8HC0PAyYGVs3K2obFi2nt3fYJzLKS4CBOY41EPggZsDjx1oHETlDRCaJyKQFCxYU\nfXGO85FY6t07WSKw63gC81G3Nsztl+M4hVATESgi3wdagT/U4vyFoKrjVHWUqo4aNCgR88g71SaI\npN69kyWYXATWvQ1z++U4TiFUXQSKyCnAZ4EvROERgNnA8Nhmm0Rts0mFW+LtHfYRkRagH7Awx7EW\nAhtE26Yfy3HKj3sCE4nbMMdxkkJVRaCIHAh8FzhMVVfEVk0Ajo+q5bbAkqdfUNW5wFIR2TPKhzkZ\nuD+2T6iaOxp4MjLIjwJjRKR/lEw9Bng0WvfXaFuifcOxHKf8BOHXp48tf6QXGpwukhOYCbdhjuMk\niZb8m5SGiNwBfBrYUERmYdVu5wPdgYnRKAnPqepXVHWKiIwHpmIhlrNUtS061JlYlV5PLP8m5ODc\nANwmItOx5O3jAVR1kYj8FPhXtN1FqhqSu78H3CkiFwMvR8dwnMoQF4GqNoNIS8V+ctWji3gC3YY5\njpN0RJPinaggo0aN0kmTJtW6G06j8cgjcNBBcNRRcM89sHw5rL9+rXvVefbcE55/3pbb28HEUKIQ\nkRdVdVSt+1EO3H45TtejUBvmM4Y4TqWIF4ZAcvIC49exdm3t+uE4juN0CheBjlMp4oUh8deNTjwM\nnPCQsOM4TpJxEeg4lSKeExh/3ei4CHQcx0kELgIdp1K4CHQcx3HqGBeBjlMp0sPBSRFMa9ZAr162\nnJRrchzH6YK4CHScSpHUwpDVq6Fv39Sy4ziO05C4CHScSpHkwhAXgY7jOA2Pi0DHqRRJzAlUNeEX\nrmnVqtr2x3EcxykZF4GOUymC6AsDRCfBa9bWZkLQPYGO4zgNj4tAx6kUq1dDt27Qvbu9ToInMIg+\nF4GO4zgNj4tAx6kUa9aYCOzWLfW60XER6DiOkxhcBDpOpQgiMImewJAT6CLQcRynYXER6DiVIome\nwHAN7gl0HMdpeFwEOk6lWLPGvIBBBCZBMIVr6Nev42vHcRyn4XAR6DidYeHC7OtCYUiSPIEeDnYc\nx0kMLgIdp1SmToVBg+CVVzKv7wrhYB8n0HEcp2GpmAgUkRtFZL6IvBZrGyAiE0VkWvTcP7bufBGZ\nLiJvisgBsfbdROTVaN1VIiJRe3cRuStqf15ENo/tMzY6xzQRGRtr3yLadnq0b7dKXb/TBZg508bM\nmzUr8/okF4Z0gZxAt2GO4ySdSnoCbwYOTGs7D3hCVUcAT0SvEZGRwPHAdtE+14hIc7TPtcDpwIjo\nEY55GrBYVbcGrgQuj441ALgQ+DiwB3BhzFBfDlwZ7bM4OobjlEbwgq1cmXl9knMCu0Y4+GbchjmO\nk2AqJgJV9e/AorTmw4FbouVbgCNi7Xeq6mpVfRuYDuwhIkOAvqr6nKoqcGvaPuFYdwOjozvsA4CJ\nqrpIVRcDE4EDo3X7Rtumn99xiieIv1wisFs3WG+91OtGJ4i+nj2hpSXRItBtmOM4SafaOYGDVXVu\ntDwPGBwtDwNmxrabFbUNi5bT2zvso6qtwBJgYI5jDQQ+iLZNP9Y6iMgZIjJJRCYtWLCgmGt0ugr5\nRGAoDBExIZgEERiuIYS5EywCs9AQNsztl+M4hVCzwpDorlhrdf58qOo4VR2lqqMGDRpU6+449Uih\nnkCw5ySIwCD6uneHHj26ogj8iHq2YW6/HMcphGqLwPei8AjR8/yofTYwPLbdJlHb7Gg5vb3DPiLS\nAvQDFuY41kJgg2jb9GM5TvEE8ZetQjYuArt3T54I7JqeQLdhjuMkhmqLwAlAqHQbC9wfaz8+qpbb\nAkuefiEKuywVkT2jfJiT0/YJxzoaeDK6M38UGCMi/aNk6jHAo9G6v0bbpp/fcYqnEE9gqAzu1i0Z\ngslFoNswx3ESQ0v+TUpDRO4APg1sKCKzsGq3nwHjReQ04F3gWABVnSIi44GpQCtwlqq2RYc6E6vS\n6wk8HD0AbgBuE5HpWPL28dGxFonIT4F/RdtdpKohuft7wJ0icjHwcnQMxymNQnMCITnh4PScwASP\nE+g2zHGcpFMxEaiqJ2RZNTrL9pcAl2RonwRsn6F9FXBMlmPdCNyYoX0GNuSC43Serp4TmHBPoNsw\nx3GSjs8Y4jil4jmBiRaBjuM4ScdFoOOUSrGewCQIJheBjuM4icFFoOOUSrGFIUnwBK5ZA01N0Nzs\nItBxHKfBcRHoOKXSFQtDVq9OCVsXgY7jOA2Ni0DHKZVcIrCtDdrbky0Cu/hg0Y7jOI2Oi0DHKZVQ\nEJKpMCQ+lAokx2sWD3En5Zocx3G6KC4CHadUcnkC00VgkjyBcWGb4HECHcdxko6LQMcplUJEYNIK\nQzwn0HEcJzG4CHScUsklAoM4SqIn0EWg4zhOInAR6DilUkw4OCmDRacPgO0i0HEcp2FxEeg4pZJr\nxpBMOYFJEEyZPIGqte2T4ziOUxIuAh2nVDwn0J7Xrq1dfxzHcZyScRHoOKXQ1mairrkZWlvtEacr\n5AT26JFqcxzHcRoOF4GOUwohBDxggD2newOTOkRMek4guAh0HMdpUFwEOk4pBNFXqAjs3t1mEEn3\nGDYamcLBPlag4zhOQ+Ii0HFKIYi+/v3tOV0IZfIExtsblUwi0D2BjuM4DUlNRKCIfFNEpojIayJy\nh4j0EJEBIjJRRKZFz/1j258vItNF5E0ROSDWvpuIvBqtu0pEJGrvLiJ3Re3Pi8jmsX3GRueYJiJj\nq3ndToIo1BMYLwyJtzcqLgIBt2GO4ySDqotAERkGfB0YparbA83A8cB5wBOqOgJ4InqNiIyM1m8H\nHAhcIyLN0eGuBU4HRkSPA6P204DFqro1cCVweXSsAcCFwMeBPYAL44bacQomnwjMVBgCjS8CPSfQ\nbZjjOImhVuHgFqCniLQAvYA5wOHALdH6W4AjouXDgTtVdbWqvg1MB/YQkSFAX1V9TlUVuDVtn3Cs\nu4HR0R32AcBEVV2kqouBiaSMruMUTrGFIUEwNboIdE9gwG2Y4zgNT9VFoKrOBn4B/BeYCyxR1ceA\nwao6N9psHjA4Wh4GzIwdYlbUNixaTm/vsI+qtgJLgIE5jrUOInKGiEwSkUkLFiwo4UqdRJPuCSw0\nJ7DRBZOLwIawYW6/HMcphFqEg/tjd7lbAEOB9UXkxPg20V1xTachUNVxqjpKVUcNGjSoll1x6pH0\nwpBChoiJtzcqa9Z0+XECG8GGuf1yHKcQahEO3g94W1UXqOpa4B5gL+C9KDxC9Dw/2n42MDy2/yZR\n2+xoOb29wz5RuKYfsDDHsRynOLpiYUhrqw1z08VzAnEb5jhOQqiFCPwvsKeI9IpyXEYDrwMTgFDp\nNha4P1qeABwfVcttgSVPvxCFXZaKyJ7RcU5O2ycc62jgyejO/FFgjIj0j+7mx0RtjlMc+TyBSSwM\nCdfk4wS6DXMcJxG0VPuEqvq8iNwNvAS0Ai8D44DewHgROQ14Fzg22n6KiIwHpkbbn6WqbdHhzgRu\nBnoCD0cPgBuA20RkOrAIq8xDVReJyE+Bf0XbXaSqiyp4uU5SKWWwaGhsr1k2EVjra7rwQpg5E268\nsSqncxvmOE5SqLoIBFDVC7FhDuKsxu6oM21/CXBJhvZJwPYZ2lcBx2Q51o1Adf4tnORSaGHIeuvZ\ncxI8gfUqbP/xD3j33aqe0m2Y4zhJwGcMcZxSKKQwpKUFmqKfWBJEYL16ApcuheXLa9sHx3GcBsRF\noOOUQiE5gUEkgYvASrJ0KSxbVts+OI6TbB59FI48ErSmA5eUHReBjlMKK1daqHe99UzgZfIEBuEH\nyRgsup5F4PLlVrnsOI5TCZ56Cu67L3GFcHlFoIg0i8gvqtEZx2kYVq6Enj1tuWfP/CIwCYNF12tO\n4JIl9rxiRcbVbW1tnHvuuVXskOM4iSNEGxKWepJXBEZVbJ+sQl8cp3FIF4GZCkMyicAkeQKbmswT\nWksR2NYGH35oy1lCws3NzTzzzDNV7JTjOIkjoSKw0Orgl0VkAvAn4MPQqKr3VKRXjlPvxEVgjx6F\newKTJALDci3DI3Hht2wZDBmScbNddtmFww47jGOOOYb111//o/ajjjqq0j10HCcJBPGXsPzjQkVg\nD2y0+n1jbYqNlO84XY9Vq3KHg7tCYUhYrqUncOnS1HKOO/RVq1YxcOBAnnzyyY/aRMRFoOM4hdGV\nPYGq+sVKd8RxGopicwLrJX+uM6TnBEJ9icAcd+g33XRTFTrjVBRV+PnP4dRTwedDdqpNQkVgQdXB\nIrKJiNwrIvOjx59FZJP8ezpOHbBmDbz8cnmP6TmBqeVaisBQFAI5jfOsWbM48sgj2Wijjdhoo434\n3Oc+x6xZs6rQQadsTJsG551nFZqOU22CfemKIhC4CZvLcmj0+EvU5jj1z+23w+67w8KF5TtmsZ7A\nMHOIi8DyUqAn8Itf/CKHHXYYc+bMYc6cORx66KF88Yse4GgoguBPWE6W0yB0ZU8gMEhVb1LV1uhx\nM+D+eKcxmDnTqkgXLCjfMfMVhqTnBDY12QwiLgLLS4EicMGCBXzxi1+kpaWFlpYWTjnlFBaU8/vg\nVJ7wWbsIdGpB+N4l7PtXqAhcKCInRmMGNovIiVihiOPUP8EDGA8ddpaVK038QWGeQDDB1MgisN5z\nAnPcoQ8cOJDbb7+dtrY22trauP322xk4cGAVOuiUjfD7jX/mjlMtung4+FTgWGAeMBc4GvBYitMY\nVEoEFhMOBnvdyIUhmTyBPXrUjwjMcYd+4403Mn78eDbeeGOGDBnC3Xff7cUijYZ7Ap1asWZN6iY4\nYSIwb3WwiDQDR6nqYVXoj+OUn/fft+dKisB8hSFgrxvZE5gtHFzLP+UlS0DE3tss/Whra+Oee+5h\nwoQJVe6cU1Y8J9CpFfHvXMJEYKEzhpxQhb44TmWotCewkMGiofFFYOh7veUE9ukDfftmNc7Nzc3c\ncccdVe6YU3bcE+jUirhtSdj3r9DBop8VkauBu+g4Y8hLFemV45STIALLmUuULxycXhgCjS8CV6+2\nApfm5lRbPYjAfv2s+jqHcd577705++yzOe644zrMGLLrrrtWo5dOOXAR6NSKBHsCCxWBO0fPF8Xa\nlI4ziBSMiGwAXA9sHx3nVOBNTGRuDrwDHKuqi6PtzwdOA9qAr6vqo1H7bsDNQE/gIeAcVVUR6Q7c\nCuyGFbAcp6rvRPuMBX4QdeViVb2llGtwGohyh4PXrrVq47gIDG1BIGUrDGn0nMB0YVvra1q61LyA\nzc05xcHkyZMB+NGPfvRRm4h0mEGkGNyG1QAPBzu1oiuLQBFpAq5V1fFlPO9vgEdU9WgR6Qb0Ai4A\nnlDVn4nIecB5wPdEZCRwPLAdNkbh4yKyTRSmvhY4HXgeM6AHAg9jxnaxqm4tIscDlwPHicgA4EJg\nFGa4XxSRCcFQOwlkzZrUj7ZcIjB4/eIiMLT37p06b9LCwfUsApuashrn9vZ2vvrVr3LssceW88xu\nw6qNewKdWhFsS0tL4kRgITmB7cB3y3VCEekHfAq4ITr+GlX9ADgcCHe0twBHRMuHA3eq6mpVfRuY\nDuwhIkOAvqr6nKoqdtcc3ycc625gtIgIcAAwUVUXRUZzImZ0naQSHyC6XCIwFIGki8B4cUgSRWA9\nejeXLDER2KdPVnHQ1NTEz3/+87Kd0m1YjXBPoFMrwndu440T9/0rdIiYx0XkXBEZLiIDwqPEc24B\nLABuEpGXReR6EVkfGKyqc6Nt5gGDo+VhwMzY/rOitmHRcnp7h31UtRVYAgzMcSwnqYRQMFTOExjG\nC4znBSZRBNazJ7B375x36Pvttx+/+MUvmDlzJosWLfroUSJuw2qBewKdWhG+c0OGJM4TWGhO4HHR\n81mxNgW2LPGcuwJfU9XnReQ3WNgkdWDLidESjl02ROQM4AyATTfdtJZdcTpD8ASKVCccDNDeDq2t\nmQVTegFJI1GvIrBfP8vHzCEO7rrrLgB+97vffdQmIsyYMaOUs9a9DUuk/QoicMWKjvm3jlNpgvAb\nMgRmz65tX8pMQSJQVbco4zlnAbNU9fno9d2YAX1PRIao6twoTDI/Wj8bGB7bf5OobXa0nN4e32eW\niLQA/bDk6tnAp9P2eSpTJ1V1HDAOYNSoUTUVpE4nCCJw6NDqicBMM2uE1+UcpqbaZBKBYbBoVRPa\n1SZ4Altbc4rAt99+u5xnrXsblkj7Ff/tLFsGG2xQu744XYsEewJzhoNF5Lux5WPS1l1ayglVdR4w\nU0Q+FjWNBqYCE4CxUdtY4P5oeQJwvIh0F5EtgBHAC1HYZamI7Bnlypyctk841tHAk1HOzaPAGBHp\nLyL9gTFRm5NUggjccsv6EIGNHA7OlhMY1lWbtjYzyPFwsHbUO/FcwD/96U8d1l1wwQUlndZtWI1Y\nujT1W/OQsFNNli0zz/OGG2a0M41MvpzA42PL56et60wy8teAP4jIv7HhZy4FfgbsLyLTgP2i16jq\nFGA8ZmQfAc6KquoAzsSGaZgOvIVV1YElbA8UkenAt4hCNaq6CPgp8K/ocVHU5iSVkBNYDREYCkOS\nKgKzhYPDumoThEAoDGlrW2fmljvvvPOj5csuu6zDukceeaQzZ3cbVk3a200EbhI5Tl0EOtVk2TKz\nMX362HexkdN60sgXDpYsy5leF4yqTsaGOEhndJbtLwEuydA+CRunK719FXBMenu07kbgxmL66zQw\nCxdCr14weHD1CkOCIHIRWFlCjljfvinxt2xZ6nMBNHbHrml37+mvi8FtWJX58EPzvgwbBtOmuQh0\nqsvy5RZtCEOALV9u/ysJIJ8nULMsZ3rtOPXHwoXmwu/XzwRY+hy/pRDEXhB/2cLB9VZE0VnqVQT2\n69fROMeQWJ6ipOUspr926phwAzcsKoR2EehUk7gnEBKVF5jPE7iTiCzFvH49o2Wi1z0q2jPHKQfv\nvw8DB5pQAPsz6dHJr67nBKaoBxEYBouGdcTBK6+8Qt++fVFVVq5cSd++fQHzAq4qxw2BUx3CZ+3h\nYKcWBBGY5WazkckpAlXVa/CdxmbhwnVF4ODBuffJh+cEpqgXERi8emnioK2tDScBuAh0akl6ODhB\n379CB4t2nMYkkwjsLPlyAl0EVofwWYbqYEjUHboTI3zWLgKdWtBVPYGO0/C8/34qJxAqIwLTw8FB\nECUtJ3DNmszjBEJ5ci2LJe4JbG+3ZRcHySR81p4T6NQCF4GO04C0tcEHH1TOE5ivMCSTJ7C9vXFn\nO1i9un5zAltbbTlBxtmJEX63gwfbb8dFoFNNQjg4gYUhHg52ksvixTasRFwEBuHQGVatMgEY8tCa\nm2G99QoTgfH1jUa9hYOXLrXPIG6cXRwkk3gleJ8+5fkdO06hJNgT6CLQSS5htpBKhINjY9EB9rqQ\nwpD4+kajHkVgnz5WGewiMNkE0RcEv3/OTrVobTXbHheBCfr+uQh0kkuYLWTgQAsZQuVEYI8eyfcE\n1tsQMUuWpD7Xbt3MG5ugO3QnxpIl9gfc3Owi0Kkuwab07m12vqkpUXbGRaCTXIIncOBA+/Po3buy\nnsBCCkPi6xuJ1lbLZ6yna1q6NCUCwcVBklm6NOXN98/ZqSbhu9anj6Wf9OnjItBxGoJ4OBjsT6Qa\nIjCJnsB6FLYuArsOca9v377+OTvVIy4CwZwJLgIdpwGIh4PBRWBnqFcRGLxDkDjj7MRwT6BTK+Lh\n4PCcoO+fi0AnuSxcaHli4cfbt68XhpRKtmuq9TiB7gnsGsQ/a/+cnWrinkDHaVAWLrRQcBjKpZye\nwPT5h+OFIcErVk9FFJ2lHj2B8RAhuDhIMvHP2j9np5q4CHScBuX991OhYKh+ODhdMDWyJzCbCAzX\nVA85gQkzzk6MTOFg1dr2yekapIeDvTDEcRqEMG9woNoicL31Om6TRBHY1GTXWW0R2NZmhtg9gV2D\ndE9gGLvNcSqNewIrg4g0i8jLIvJA9HqAiEwUkWnRc//YtueLyHQReVNEDoi17yYir0brrhKxuJ+I\ndBeRu6IcCgXEAAAgAElEQVT250Vk89g+Y6NzTBORsdW7YqfqVFsExnMCm5vXnRqukUVgtpxAqM2c\nyMEIxwtDqigC3X5VkbY2+PDDjp5AcMHvVIdMIjBB371aegLPAV6PvT4PeEJVRwBPRK8RkZHA8cB2\nwIHANSIS/l2vBU4HRkSPA6P204DFqro1cCVweXSsAcCFwMeBPYAL48baSRghJzDQr58Jtc6KsEIG\ni84klhpZBGbzBIa2aovA+LzBgereobv9qhbpn7WLQKeaLF9ueeW9etlr9wR2HhHZBDgEuD7WfDhw\nS7R8C3BErP1OVV2tqm8D04E9RGQI0FdVn1NVBW5N2ycc625gdHSXfQAwUVUXqepiYCIpw+skCdXM\nnkDovDewkMGis3nMwvpGo95EYPgM08PBa9ZkFtlXXgm33VaWU7v9qjIuAp1asmyZCb9QYBhyAhOS\nk1orT+Cvge8C7bG2wao6N1qeBwyOlocBM2PbzYrahkXL6e0d9lHVVmAJMDDHsZyksXSp5Q2VWwSq\nmjcxX05gJrHUyJ7AbMUuoa0ePIFBHGS6S7/2WnjwwXKd3e1XNQmftYeDnVqwbFnqOwcmCFVT9r7B\nqboIFJHPAvNV9cVs20R3xjWV2SJyhohMEpFJCxYsqGVXnFJIny0EyiMC16wxA5BJBK5ZY/lLSQ4H\nZ/NwVjtJP1s4GDKLgzlzYMiQTp/W7VcNSPf6hmcXgU41WL48ZVsgt51pQGrhCdwbOExE3gHuBPYV\nkduB96IQCdHz/Gj72cDw2P6bRG2zo+X09g77iEgL0A9YmONY66Cq41R1lKqOGjRoUGlX6tSO9NlC\noDwiMNz9ZRKBYGIpySIwkyewR4/68gSmG+dly6ywYOjQcpzZ7Ve1cU+gU0syeQIhMXmBVReBqnq+\nqm6iqptjCdNPquqJwAQgVLuNBe6PlicAx0cVc1tgCdQvRKGXpSKyZ5Qvc3LaPuFYR0fnUOBRYIyI\n9I8SqsdEbU7SCJ7ATCIw/KmUQjYRGAaPXrkyuwj0nMDykS4MIHs4eM4cey6DCHT7VQPSPYEuAp1q\nknAR2FLrDsT4GTBeRE4D3gWOBVDVKSIyHpgKtAJnqWpbtM+ZwM1AT+Dh6AFwA3CbiEwHFmHGGlVd\nJCI/Bf4VbXeRqi6q9IV1SdraLJG2qUZpp5UKB+fzBK5cmb0wpJE9gfWWE5ipMCRbmCaIwDKEg3Pg\n9qtSeGGIU0uWL4dhsdTbXLnHDUhNRaCqPgU8FS0vBEZn2e4S4JIM7ZOA7TO0rwKOyXKsG4EbS+2z\nUyB77QX77w8XX1yb89cqHBw8gUkrDMmXE1jtP+QgDOK5OtnEwdyoXqM84eCPcPtVJdK9vgnLyXLq\nnGyewIR8/+rJE+gkBVWYPLmjF67aLFxoXsgNNki1BU9CNURgJrEUZhBpZBGYzRMYRHe1WLrUDHPc\n05wtTFMdT6BTKZYssc95/fXtdUuL/d46k9bhOIWS8HCwTxvnlJ9Fi0zoBA9MLVi4EAYM6CgS1lvP\nBvyspAgMg1FnEoFhFpEkisBa5ATGQ8GQ2xPYq9e62zuNQfiswzht4FMEOtUjW3Wwi0DHyUIQf8ED\nUwvSB4oOdHbquCACQyFIIF4Yki0nEGojmMpBvU0bt3Rpx6IQyC4C58yxUHBcRDiNQ3ze4ECjiMA/\n/xkefjj/dk590t5uIwvEPYGeE+g4eQgicP58G7C5pQZfs/ffzywC+/atXU4gmIhqVE9gU1Pmz7IW\n4wRmEgY9elgfM4WDPRTcuGTz+jaCCPz+980OHXRQrXvilEKwJR4OdpwiCB5AVXjvvdr0odKewFJy\nAqGxRWA2YVurcQLThYFIZnEwd27Zi0KcKpLJ69u3b/2LwPZ2ePttmDkz/7ZOfRKEXjwc3L27pfXU\n+/evQFwEOuUnngtYq7zAhQszF6a4CCyNXCKwXnICITWvZ0DVPYGNTqOGg2fPtt/6nDkWEXEaj/Ad\ni3sCRUwUuifQcbJQDyIwWzi4UiIw5ATmKgyBxs4JrKdryiYCe/fuKA7KO1uIUwuy5X/WuwicMcOe\n29pqWyTnlE4mEQguAh0nJ3PmpH40tSgOWbHCxFglRGDIfStlsGhIricwzKlcLTIJA1hXHFRojECn\nijSqJzCIQID//rd2/XBKJ1M4GNaNODQwLgKd8jN3Luy0k7nNa3EHnGm2kEC1wsFJLAzJJQKhetfV\n1mYCoJBwsI8R2Pg0amGIi8DGJ5cnsN6/fwXiItApP3PnwvDhsNFGtfEEZpotJNCvn3kK164t7dgr\nV1oFahj4OZD0nMBcwrbacyIHkVdIONg9gY3NmjXmfc8UDv7wQ7shqFdmzIBBg2zZi0MaEw8HO06R\nhET8oUPN+1JLT2A2EQilzzawcqUJvvQx51pa7JEvJ7BRRWC+sQ/DNtUgfS7ZOOkeIvcENjbZPutG\nGKttxgzYYQfo3989gY1KtnCwi0DHycKSJSaEhgwxIVgLT2C+cDCUHhIOIjATPXqkvIz1VERRDgoJ\nB1drrMB8IjA9HOyzhTQu6fMGB7INDF5PzJgBW24Jm27qIrBRyeYJ9JxAx8lC8PwNGVI7T2C+cDBU\nRgTG5zPtjCdQFS67DN55p7Q+VoJCRGA9eAIzhYN9tpDGJfxOC50isF5YvtwGy99yS0uNcRHYmITv\nV5i3OuCeQMfJQvD8DR1qj/feq37eTvAEDhiw7rpyhYMz0bMnfPCBLXemMOSdd+CCC+DXvy6tj5Ug\nV05gGB6nWiIwCINs1cErV6bGZfMxAhubbII/vK5XERiKQrbayjyBnhPYmCxfbpGE5uaO7V4Y4jhZ\nSPcEtrfbHXEpqML48RZiLYaFC00gpBdvQOU9geG4nfEETptmzxMnltbHStBIOYFgRQOQyk91GpNs\ngr/ePYFBBIZw8KJFifEcdSmWLVs3FAwmAj/80P7fGhwXgU55iYvA8Odbal7g3/4Gxx0Hf/xjcftl\nmzIOyiMCg+crnULCwYXkBE6fbs9Tp8KsWaX1s9w0UjgYzHir2vfRPYGNS77CkEYRgeDewFKYNSs1\nLFctyCYC+/Qx+1LLvpWJqotAERkuIn8VkakiMkVEzonaB4jIRBGZFj33j+1zvohMF5E3ReSAWPtu\nIvJqtO4qEUv8EZHuInJX1P68iGwe22dsdI5pIjK2elfeRZg71/In+vRJ/fmWmhd47732PGVKcftl\nmy0EKl8YUk5PINSPN7BRRGC8arRCs4W4DasijVoYMmOG9bl/f8sJBM8LLJbWVthxR8uPrhXLl69b\nGQyptlK8u8uXpyIVdUAtPIGtwLdVdSSwJ3CWiIwEzgOeUNURwBPRa6J1xwPbAQcC14hICNBfC5wO\njIgeB0btpwGLVXVr4Erg8uhYA4ALgY8DewAXxg21UwZCDpZI5zyBqnDffbb8+uvF7VtpT2AhOYGd\nFYE77giDB9ePCMw3bRxUXwRmMs5xcVC5MQLdhlWLRi0MCZXBIu4JLJU334TFi+H552vXh1zh4LC+\nWI47Dk44oXP9KiNVF4GqOldVX4qWlwGvA8OAw4Fbos1uAY6Ilg8H7lTV1ar6NjAd2ENEhgB9VfU5\nVVXg1rR9wrHuBkZHd9gHABNVdZGqLgYmkjK6TjmIh98GDy591pDJk+3Oef31ixeBs2fDxhtnXtet\nW0ePXbEUmhPYmcKQ6dNhxAjYf38TgfWQd1JPnsAlS8wwpydrQ0fjXKExAt2GVZGlS238zfQUjEYR\ngWA3IU1N7gkslldesed//7t2fcgnAov1BKrCP/8JTz9d3Wk2c1DTnMAoxLEL8DwwWFWDWpgHDI6W\nhwHxW6hZUduwaDm9vcM+qtoKLAEG5jiWUy7CkBxghRmDBpXmCbzvPjOcX/qSGc9Cf2yLF1sfRo7M\nvk3fvpUTgUEI5fIE5hJLra32BzJiBIwZY6HtYAxrSb2NE5ht3L94ODheqV4h3IZVmCVLzHufPsRP\nz55mH+pRBLa3w9tvp0RgSwsMG+YisFgmT7bnefNKLy7sLOUOB7/3nv1HffBB3QwBVjMRKCK9gT8D\n31DVDuN1RHfFNZXJInKGiEwSkUkLFiyoZVcai/QhOUodK/C++2DvvWGffez1G28Utl/IH9xuu+zb\ndGb+4FWrcucEBnKFTnN5AmfOtMGmt94a9tvP2h57rLS+lpN68gTmEoFxT2C8SKkC1LMNS4z9yvZZ\ni5jgL3Wop0oyZ479xrfaKtU2fLiHg4vllVdMQAO8+mpt+pCrMASKF4FTp6aWX3qp9H6VkZqIQBFZ\nDzOef1DVe6Lm96LwCNFzkP6zgeGx3TeJ2mZHy+ntHfYRkRagH7Awx7HWQVXHqeooVR01KMz/6OQm\nJOLH/3RLmTVkxgwLARx5JGy7rbUVGhKutAjM5wkM5PIEtrVlHzsxFIWMGGHv4w471IcIzJUTWO1x\nAgvxBIZwcIVmC6l3G5YY+7V0aebxIGHdKQLrhbfesufgCQSfNWTuXDjiiNRA/oUweTIcGGU61Cok\nXO6cwHiR48svl96vMlKL6mABbgBeV9VfxVZNAEKl21jg/lj78VG13BZY8vQLUdhlqYjsGR3z5LR9\nwrGOBp6M7swfBcaISP8omXpM1OaUg0yJ+KV4AkNByOGH2910S0vHO6hcTJliP9CQjJ2JaojAXDmB\nYN6+TIThYbbe2p733x+eeab4sRLLSWurhbjqxRO4ZEnh4eAKzBbiNqyK5Pus61EExoeHCYQBo+sh\nv7cWPPAA3H8/PFrgVzWEgPfbz/5DaiECVcsfDp4yxSrGt9++64pAYG/gJGBfEZkcPQ4GfgbsLyLT\ngP2i16jqFGA8MBV4BDhLVYMb5UzgeizR+i3g4aj9BmCgiEwHvkVUpaeqi4CfAv+KHhdFbU45yJSI\nP2SI/aCLmTXkvvusOnbLLS2vcJttivMEbrtt7j/+UkVge7sJnc56AiF7SHjaNCuGCe/hmDG27d//\nXnx/y0UQd/UiAnN5h3r1sucQDq5MKNhtWLXI9Vn37Vu/IrCpqeON6PDh9vto5NB8Z3jxxY7P+Qh5\n0DvtZP8FtRCBK1aYECxnYcjUqZavvuuudRMObqn2CVX1GSDbP/ToLPtcAlySoX0SsH2G9lXAMVmO\ndSNwY6H9dYogUw7W0KEmnhYsyF6xG2f+fHj2WfjhD1Nt225beE7I1KmpEEI2ShWBofChHCIwm2Ca\nPt28gEHE/u//msiaODH/dVWKIFjrSQRm8w41NaWmdJozB3bbreyndxtWRZYsyZ7aUc+ewE037Thj\nUXyYmMGDM++XZIoVgaEoJIjAq66yiERLFSVL+G6VKydQ1ZwURx9t/2m33moOkkL+FyuIzxjilI9s\n4WAoPC/wL38x0XjEEam2bbc1cZRPZCxaZD+qXPmAULoIDKPDd7YwBHJ7AkMoGMyz9clP1jYvsJCK\n5/h2lSaXCISUOPB5gxuffPmf9SoC46FgSInArpgXuHatefJELARaSEj8lVfsPevf30Tg6tUdB9Gv\nBkHgZQoHd+tmgrQYEfjee/Yftd12sMsu1lYHIWEXgU75mDPHhFA8fBMEYaF5gffdB5ttZneAgW23\nNcORzwgUUhQC1r/ly4sLUUN+EdjZcHB8eJg4Y8bAa6+VPv1eZ8kXDm5qMq9HNURge7v98ecSgb17\n23u1YoXPG9zIqDZmYYiLwI5MmWL2bv/97fMqRMxNngw772zLO+5oz9UOCefyBIqkIg6FEvLaR45M\nXZuLQCdRhByseD5eMZ7A5cst7HnkkR2PEcb8y5cXWIwIhOKHlyhGBOYrDMkkAuPDw8QZM8aeH3+8\n8L6Wk3wiMKyrxjiBwejm8wT+5z+27CKwcVm1yn4PjeQJXL7cUlriw8OAebR69eqaIjCEgM84o+Pr\nbKxcabOFBEfA//yPed2qPV5qLhEIJgKL8QTG/5/69bPvSB3kBboIdMpHfKDoQMh3KMQT+OijJjji\noWCAj33MRGEhIrBPn9RcndmohggsJXQaHx4mzo472qDbtQoJ58sJDOuq4Ql87z17zuYdAvsOvP22\nLXs4uHHJNm9wIIjAOpl5AUh979I9gWH6uK44VuBLL9lndeihFinKJwJfe808/sFb1q2bRYOq7QnM\nFQ4O7cWIwKlTYYMNUv+Ju+zinkAnYWSqxuzWDTbcsDBP4P3325y/e+/dsb1nT9h88/zDxEyZYl7D\nfEOClDp/cBCB6VNYBTqbE5g+PEygqclCKY89VpshJvLlBIJdezVE4G232ee7777Zt+ndOxXqd09g\n45Jt3uBAnz7mKaxWLmohZBojMNBVxwp88UWrhu3Wzbx7+URgvDI4UIsK4XyewD59ivcEbrdd6v9p\nl10sdSDMNx/nuuvgqaeK6m6puAh0yke2RPyhQ/N7Atvb4eGH4aCDMleAjRxZmCcw13Rxgc6KwErl\nBE6bZiGjTO/hwQdbhfWkSYX3t1wUGg6O/xmrln+qp9WrYdw4OOSQzH+ygbjRdk9g4xI8gYUMDF5u\nVPPP8Z2JTGMEBoYP73oisLXVRN2uu9rr3XYzz2Cum9lXXrEbuS22SLXtuKN5URcvrmx/4xQSDi70\nuxcqg+P/T+E9CZXQgXfegTPPhBNOqEq6g4tApzysWGFGO9Of7pAh+T2B//qXjSZ/8MGZ12+7reWJ\nZCvmeP99Ex358gGhfkVg+vAwcQ480NoffLDw/paLYkXg6tXwxS9a2OOf/yxfP/78Z/uMzz4793bB\naFdothCnSoTfZ65wMFTmj/Lyyy36UGye64wZ1t/+/dddt+mmNnpBPXkuK83UqfYehqGadtvNPq8Q\n9cjE5MnmBWyKyZNQHFLN6ePKGQ6ePz9VGRzIViF8zTVm6+fNg0vWGVWq7LgIdMpDpuFhAoV4Ah96\nyH70BxyQef2225rxDDk36YRQcSVFYKHjBDY1ZR/PKp8nMD0fMDBwIOy5Z3EisL0dnn8enn6646PY\nAU6LyQlcuNAKWW65BZqb4YYbijtXLq6+2t6f/ffPvV0w2hWYLcSpIvk8gaG93CJQFX7/e7NZjzxS\n3L6hMjjT9y5UCM/OOFNpMgmFD0EEBu9XtpBwe7t5AuOhYKhNhXA5C0MyFS0OHmwOkrgIXLECrr8e\njjoKTjkFfvWrig+N4yKwK6Fqc/tWgkyzhQQKmTXkwQfhE5+AAQMyr883h3ChlcFQeU9grty5bIUh\nbW32B5KeDxjnkEPMeM6bV1h/b7vNhOOnPtXxEar0CqWQnMDu3a3/e+4Jzz0Ht98OJ54I48eXZ8q7\nl14yr+KZZ3b0EGQiGG0PBTc2hRSGQPlF4AsvpMK648cXt++MGetWBgfKOUzMihXlT7eoBC++aGJp\nm23s9Xbbma3IJgLfecc+z1AUEhgyxG6Eqy0Ce/TIfkNfTE5gfHiYOOnFIXfeaSHvs8+Gyy6z83/r\nW8X3vQhcBHYVZs2Cz3zGDNHCheU/fqbZQgJDh5rIyTZ5+Lx5ZhSyhYKhMBHYty9sskn+vlZKBIbC\nkHxiCdb1BP73v5bkns0TCCYCwXInC+G666yy+vHHU4+TToI//alwIQmFh4OnTLEk5yefhC98AcaO\nNUMa5oLuDL/7nYV3Tzkl/7ZBHHhRSGNTSGEIlF8E3nGHfZ+POw4mTCj8Jqa93SIV2fJVw6gF5RCB\nn/+8ectqOad4Ibz4ogm6cOO23nrm1csmAjMVhYB5VqtdHJJt3uBAsZ7ADTZY9/9x113tP23lSnPS\n/Pa3sMMOdrO+8cY2c9YDDxRu80vARWAj0Npqd6SlJCqDfYl23tny7hYtslBHuckVDs43VmD4ggeR\nk4nwA8pWIVxoZTCYge/Xr/gxmsrpCUz/LIPLP5cncKedYNiwwkLCr71mHrkvfxlGj049fvAD+z7l\nCtO+8UbH/hUiArfe2iZFf/75VHX3pz5lA3/femv+/uZi4UL44x9NwG6wQf7tg+F2T2D9UMowLrUo\nDGlrM2/MIYfA6adb5KTQP+A5c+x3k00EhhvUzorAp5+2kRTmzauMLS8Xra2W35c+dWOu4pDJk00w\nbr/OTIomAl99tTwjJLS25j/OsmXZQ8GQEoGF9Cfb/9Muu9h37tVX4R//sOs/++zUduecY17Ub3yj\n9P//PLgIbAT+7//srvRXvypuv9Wr4ZvftPGZhg83t/Po0ZZbtXZtefs4Z44JnEzh3Hyzhjz0kG0T\n8j6yse22uT2BhVQGg/3AzjwT7r67uETjSorAkCidyxMoYt7Sxx7LbxCuv97OddJJHdu32ca+A+PG\nZQ7Pv/SSvY9f/WqqLZwr13X9/vd2Fx//A2xqsvNPnNi52U5uusnyMc86q7Dt3RNYX7S2wqc/Dd/5\nTnH7zZ1rNx7ZvneVEIF//auNRXnCCbDPPrDRRnDXXYXt+9BD9pwtJaVnTzteZ8YKVIXvfc9ucD7x\nCbjiioqJg07z5ptmMzOJwKVLU8PpxHnlFbNRvXqtu27HHc3zGUL1pdLWZp/t0Ufn3q4QEQj5vbGh\nMjjT9yJeHHL11XaT+4UvpNZ36wZXXmmD3//2t7nPUyIuAuudDz+En/7Uli+/vPAS+bVrYb/94Ne/\nhq99zfKpwh3F7NlWaVlO5s4193UmT1wuT+DatSZqDj44vxcviMB0r8KCBfYoJB8wcO655mG48MLC\n9ylUBObymGXLCcw1PEycQw4x4/TMM9m3WbXK8gGPPNLGaEznq181b0T40wqowre/bc8332zexHhf\nc11XU1PmXL2TTrI75T/8IedlZaWtzarl9tnHwiSF4CKwvrjpJvj73+GXvyw8nPfii3Zjceih2bep\nhAi84w477iGHWC7Y0UdbJCVf2G/VKrjoIhNm6eOcxunsMDETJpgt//GP7TF7duc97ZUihHwzicD4\n+jiZikIC5SoOueUW87rde2/uAfgLCQeH7XKRqTI4sPnmJvwefNCcEqeeCuuv33Gbgw+2x09+Ulwa\nT4G4CKx3rr7a7kyvucZyZH7+88L2+8UvTCjcfDNcdVUqX+3gg83bdOWV5R1pP9NsIYFcs4Y8+6zd\nFeYKBQdGjjSDn15dV0xlcGDAAPOS3ntv4WHhlSstp6W5OfP6QjyB2XICcw0PE2f0aDt+rpDwvfea\n0fnSlzKvP+ww+6yuvbZj+4QJNkDpT35if4Tnn2/thYjAbGyzjf0x3nJLad+3hx6yPKtCvYCQEtK5\nQutOdVi+HH70Ixg1yv7sCvEGLlsGxx9vXrPrrsu+XblF4OrVdnN81FGp3/Kxx9rvPl8KxjXXmF26\n9NLcv+H0AaNVzf78+c8m5q691mz3LbesG2ZsbYULLrDf1KmnWpX8brvBz35m6yrFe+8VP886mMjr\n1cvykuNst53ZsHQR+MEHVhiSXhQSGDnSbjQ7IwKXL7eUmD32MAH2ve9lD+fm8wSGdflEYLaiELDv\nyi67wF/+Yu/xmWdmPsaVV5o9zjT0UGdRVX/keey2225aExYvVu3fX/WQQ+z1F76g2rOn6uzZufd7\n4w3V7t1Vjz468/qrr1YF1X/8o3x9HTlS9cgjs68fOFD1K19Zt/0731Fdbz3VpUvzn+PJJ63fjz3W\nsf13v7P2mTOL6/MHH3R8f/NxzjmqfftmX9/ertrUpLrDDrnPCaq//GXH9o99TPWoowrrx5gxqv/z\nP9nXf+YzqltsodrWln2bCy9UFVF96y17vXq16ogRdtw1a1Qvu8z6+be/qf7sZ7b84YeF9S+d666z\n/SdNKm6/995T3XRT1c03tz4Vw6uvFrd9GsAkrQPbU45HzeyXquqPf5yyNVdeacuPPJJ7n5NPtt/R\nU0/lP36PHqrnnluevt5777r9a21VHTIkt21bssTs25gx+c9xzjmqvXurLlum+n//p7rzznbOTI+D\nD7b/gMANN1j73Xen2u65x9r++Mfir7cQnn5atVs31c9+VnXt2uL2/eQnVffaK/O6UaPMTsX529/s\nWh56KPsxP/ax3J9FPsL38dlnVf/wB1u+7bbM2267bfb/UNXU9+Wll3Kf87e/te2y/W9/61u2vtD/\noQIp1IbV3EA1wqNsRrS9XfWBB1SvuEJ15cr82//gB/YRvfyyvZ4+XbWlJbOYCrS1qf7v/5q4mTs3\n8zbLlqn266d67LHFX0M2+vdXPfPM7Ot32EH18MPXbR85UnX06MLOMXeuvR+/+U3H9jPPNHHW3l54\nfwOXXGLH/Oc/8297xhmqgwfn3mb99VVzfV9WrLDzXXZZqq211YTw975XWJ9//Ws7RhBwcaZNs3WX\nXJL7GLNmqTY3q373u/b6N7+x/R58MNXPYcNUP/5x1Z/8xNYV+ycQWLTIbkq+/vXC91mzRvVTn7I/\n+WLFYxlwEVgG5s6138PnPmevV69W3Wor1e23t+98Jm67zb5rP/pRYecYNEj1y18uT3+POcaOl/49\n//rX7fu7ZEnm/cLv41//yn+OX/7Stu3Tx5533FH12mtVJ0+23+6cOXaea64xm7D11nZDs2KF6iab\nqO6xR0c719ZmNnT77XPf9JXCW2+pbrihvSdg9q9QG9vaap/9176Wef2Xv2z/QeF4c+aYMGxpUZ03\nL/txjznGbgxXry7uWlRNhPXqZcdQtfdr111VN9ss8//x8OGqp5yS/XgTJ9r78ve/5z7vV77S8VrT\nufNOO87DDxd0GYXiIjDXRcOBwJvAdOC8fNuXxYg++6zdGYW7vB12UH3llezbv/ee/YiOO65j+5ln\n2g9l2rTM+11zjR3/xhtz9+fcc00EvPtucdeRiZUr7ZwXX5x9mzFjVHffvWPb22/bfr/6VWHnaW83\nsZlu9PfZR/UTnyimxymWLTNDt//+Hdvb2swwv/SSie/581VPOMG8UrnYcMPcfWlttWv+yU9SbTNm\nWNvvf19Yn4PQu+qqddedd559rvm8xap2R73hhvZn3b+/vQdxQxU8D9ttZ56ZznDMMXauQo33WWfZ\nuf/wh86dt0TqWQTWxH6Vwpe/bLbqP/9JtY0fb5/rDTesu/20aeYl++QnC7/h2Gor1c9/vvN9XbrU\nbos9jnkAAA1uSURBVDjOOmvddc88Y32+/fZ11y1YYIKuUC/+s8+aIDjxRFvOJaqeeUZ1441TQhoy\ne0eDcL7//sL6UAgffGCesP797fM7/3w7x09/Wtj+U6fa9jfdlHn9uHG2fto0+x8cPtyuM9813H67\n7bf33tmdHNk49VQT1tOnp9oef1zXicy0taleeqlFSr7//ezH++c/Na/nUtVuZrN5RFXtP+Hppwu7\nhiJwEZjdgDYDbwFbAt2AV4CRufYp2YiuXav6wguqhx1mb/XgwSbS7r/flrt1M69gpju4b3zD/szf\nfLNj+5w5djdzwgnr7jNzphmk/fbLf8f2zjv2xx48QYHWVtVVq4q7ziBiMhn2wNixdicbJ4Rx33ij\n8HPtvbe9d+PG2d2xqomL004rrs9xrrhCP7qjmzJF9YIL7O4wU4hmu+1yH2v4cNVPfzr3Nk1Ndo7A\no49mN/DZ2GYb1QMO6Ni2Zo39aRx2WGHHeOwxO+/Ikdanf/+74/rWVrtesDSEzvDAA3ace+7Jv+31\n19u25QrzlUC9isCK26/XX8/upSuGqVPNfqV7gtrbVffc00Ksy5db25o1qn/+s30P+/cv7sZ0551V\nDz208/299Vb9KEyYTlub2a5M5zn3XPvtTJnS+T5kYvZsu6kE1YMOyrzN2rWW/pHuJUxnzRpzLuT7\nb1i71m7aW1pU//pXa2tvVz3pJM0p7OIEYZpuUwIvvmjrTz/dhP+wYfnDqoE77jB7NHRoYREcVROa\nIhZ6TeeAA+x7t2iR3ewfcID17bjjcqcpvfaabTd+fPZt2tstVeD00wvrZxkp1IZlGQo70ewBTFfV\nGQAicidwOJBlALoimDbNxpR65RUb72fKFEs27tsXLr7YKnND5c8nPmEzN3znO1Z9dtJJljS86aaW\nLHrNNTYwbhhpPTBkiB3n0kut6neHHaxQobnZqj7b2mxImXwFBpttZgnQ48bZvLTPPWcVfM8+a4nQ\nu+xiVW577WWzQPTrZ9Vy4bF6tRWqLFmSmh82V2VrmDru17+2Crnhw62AYaut1r3GXFxxhY2jdMYZ\nliQ9dqwNQl1MUUg6Z55plYsHHmjl/s3NlnT94x9bIu6SJVa8snQp7L577mP16JG7MASswGLSJHsv\nli+39x6KK2Q45BD7jrz1lhW59O5tyevz5mUvCEln9Gg759SpNiZaevVtc7MlnR96aGlFIXHGjLHv\nwOc+Z+/hQQfZ+7377h0Lbf7xD/sejxlj53bSqZz9Wr7cvgO9e9vA8qNH2wgDm21mw3L85z/2ePdd\ns2lDhqQeG25o+/XpY8/nnWe27oc/7HgOEfut7b23bdOrlxWvzZ9vY2D+8Y+pmTUKoU+f0gpDVM1u\nzJplBR3XXGPX+YlPrLttU5MViPz2t1a8EMaqnD3bCvdOOqnw4amKZehQK9b6/e/h8MMzb9PSYgUO\nX/mKFTtsvHHqsWaNFVa9/bZda3u7zbwxapQVlYwaBVtsYfZ9gw3sc/3GN6xq9vrrbWgfsM/t+uvN\nhp9+utn/9nb7PkybZgUd/fpZf4cOtf/Anj1Tg/yns/32Zid///tUYcSwYYW9J8cfb8c98kgbNeDq\nq+2/rLXVHm1t9mhvT92+f/vbdn0/+MG6x7v8cuvDl75ktnjhQitIOuOM3P+joTo4/fu3erW9HzNm\n2JirCxdW7vtRDgpRikl6AEcD18denwRcnWufgu+kgwdj0CDzxp17rt0Rvf9+5u3b2y1s27evruNx\n6tYt+x1xKBjJ5KlKLzjIRQhzhMfIkZa/cN55FmLt2TPzObI9cnn0HnrI3P3p+2TLGclFe7t5zQ47\nzO7uMhWLFMsdd9g1/+Y3uXNS8rH77vlDQ1tt1fE96N49/118OqFIJv0xdGhxuXvXXWff12yhlfZ2\nC2dstlnhx8zG9OkWBt9zz9Tn1q2bXX9Li3lUwN6fhQs7f75OQP16Aitnv1assPD7qadm94SD2Z71\n1stvDy69NPu5jj7atmlutlzhBx4ozQP52c/ad6lHD3v07GmRkp497Xu13np2juZmW+7e3dZl6v9F\nF2U/z3PPpbZrarL9w2PGjOL7XW7WrLECrpNPNi/ejjuqbrSReVz32svCzz/8oRXnnHaa6k472XuS\n7bPL5oVfsqRjMUuvXnasI45Q3XdfKywL/2f5CmVOOcVC+cEjXCwLF9o5Cv1/uvLK7Mc6+WTbZsSI\nVA5+PhYtSh1bxGxY9+4p2xYeffoUfswyUqgNE9u26yAiRwMHquqXotcnAR9X1bPTtjsDCJOsfgzL\nwSmEDYEs86PVLY3WZ+9v5Wm0Ppe7v5up6qAyHq8sVMF+gX/2labR+guN12fvb4E2rCuGg2cDw2Ov\nN4naOqCq44BxxR5cRCap6qjSu1d9Gq3P3t/K02h9brT+doKK2i9ovPfS+1t5Gq3P3t/C6YqDRf8L\nGCEiW4hIN+B4YEKN++Q4jlMIbr8cxykbXc4TqKqtInI28ChWaXejqk6pcbccx3Hy4vbLcZxy0uVE\nIICqPgQ8lHfD0igpBFNjGq3P3t/K02h9brT+lkyF7Rc03nvp/a08jdZn72+BdLnCEMdxHMdxHKdr\n5gQ6juM4juN0eVwElhEROVBE3hSR6SJyXq37k46I3Cgi80XktVjbABGZKCLTouf+texjHBEZLiJ/\nFZGpIjJFRM6J2uu5zz1E5AUReSXq80+i9rrtM4CINIvIyyLyQPS63vv7joi8KiKTRWRS1FbXfa53\n6t1+gduwSuP2qzrUk/1yEVgmRKQZ+B1wEDASOEFE6m2Y8JuxeUfjnAc8oaojgCei1/VCK/BtVR0J\n7AmcFb2n9dzn1cC+qroTsDNwoIjsSX33GeAc4PXY63rvL8BnVHXn2NAKjdDnuqRB7Be4Das0br+q\nR33Yr0JGlPZHQSP5fwJ4NPb6fOD8WvcrQz83B16LvX4TGBItDwHerHUfc/T9fmD/Rukz0At4Cfh4\nPfcZG2vuCWBf4IFG+F4A7wAbprXVdZ/r+dEo9ivqm9uw6vTV7Vfl+lw39ss9geVjGDAz9npW1Fbv\nDFbVudHyPGBwLTuTDRHZHNgFeJ4673MUmpgMzAcmqmq99/nXwHeB9lhbPfcXQIHHReRFsdkxoP77\nXM80qv2CBvncG8WGuf2qCnVjv7rkEDFOZlRVRaTuysVFpDfwZ+AbqrpUYpN612OfVbUN2FlENgDu\nFZHt09bXTZ9F5LPAfFV9UUQ+nWmbeupvjE+q6mwR2QiYKCJvxFfWaZ+dClOvn3sj2TC3X1WhbuyX\newLLR0HTOdUh74nIEIDoeX6N+9MBEVkPM55/UNV7oua67nNAVT8A/orlMNVrn/cGDhORd4A7gX1F\n5Hbqt78AqOrs6Hk+cC+wB3Xe5zqnUe0X1Pnn3qg2zO1X5agn++UisHw06nROE4Cx0fJYLGelLhC7\nXb4BeF1VfxVbVc99HhTdQSMiPbH8nzeo0z6r6vmquomqbo59Z59U1ROp0/4CiMj6ItInLANjgNeo\n4z43AI1qv6COP/dGs2FuvypP3dmvWidIJukBHAz8B3gL+H6t+5Ohf3cAc4G1WM7PacBALKl2GvA4\nMKDW/Yz195NY7sS/gcnR4+A67/OOwMtRn18DfhS1122fY33/NKnE6rrtL7Al8Er0mBJ+a/Xc50Z4\n1Lv9ivroNqyy/XX7Vfl+1pX98hlDHMdxHMdxuiAeDnYcx3Ecx+mCuAh0HMdxHMfpgrgIdBzHcRzH\n6YK4CHQcx3Ecx+mCuAh0HMdxHMfpgrgIdBoWERkoIpOjxzwRmR173a2I45wqIhtnWXe7iLwtIq+I\nyH9E5BYRGVrAMb8lIj2KuR7HcboWbsOcWuMi0GlYVHWhqu6sqjsD1wFXhtequqaIQ50KZDSgEd9U\n1Z2A/wFeBZ6MZgHIxbcAN6CO42TFbZhTa1wEOolERMaKyAvRHfU1ItIkIi0icpuIvCoir4nI10Xk\nOGBn4K58d9+q2q6qvwAWYaO8IyLjRGSSiEwRkR9Fbd8ENgKeFpHHs23nOI6TDbdhTjVwEegkDrEJ\nz48E9orusFuwKYV2AzZU1R1UdXvgVlW9CxvF/7gi7r5fwu6oAc5T1VHATsD+IjJSVa/E5n38X1Xd\nL9t2Zbpcx3EShtswp1q4CHSSyH7A7sAkEZkM7ANsBUwHPiYiV4nIAcCSEo8vseUTROQlzKhuC2Qz\njIVu5ziO4zbMqQotte6A41QAAW5U1R+us0JkR+Ag4Czgc8AZJRx/Z+BBERkBnAPsoaofiMjtZMih\nKXQ7x3GcCLdhTlVwT6CTRB4HjhWRDeGjCrxNRWQQIKr6J+BHwK7R9suAPvkOKsY3sYm+JwJ9o32X\nisgQ4IDY5vFj5trOcRwnHbdhTlVwT6CTOFT1VRH5CfC4iDQBa4GvAG3ADSIigALfi3a5CbheRFZi\nd7rpOTVXRsfrCfwT2FdV10ahkanAG8C7wLOxfcZF558J7J9jO8dxnA64DXOqhahqrfvgOI7jOI7j\nVBkPBzuO4ziO43RBXAQ6juM4juN0QVwEOo7jOI7jdEFcBDqO4ziO43RBXAQ6juM4juN0QVwEOo7j\nOI7jdEFcBDqO4ziO43RBXAQ6juM4juN0Qf4fJrJm2gXxxPUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f90c2b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Figures and report error using the different ensemble methods\n",
    "plt.figure(figsize=(10,3))\n",
    "\n",
    "# Loop over estimators to compare\n",
    "for n, (name, estimator) in enumerate(estimators):\n",
    "    \n",
    "    # Compute predictions\n",
    "    y_predict = np.zeros(n_test)\n",
    "    \n",
    "    # Train the estimator (1 point)\n",
    "    # TODO\n",
    "    estimator.fit(X_train, y_train)\n",
    "    #Predict results using the estimator on X_test (1 point)\n",
    "    # TODO\n",
    "    y_predict = estimator.predict(X_test)\n",
    "    y_error = np.zeros(n_test)\n",
    "    \n",
    "    # Compute the mean sqaured error using y_test and y_predict and store it in y_error (1 point)\n",
    "    # TODO\n",
    "    y_error = np.square(y_predict - y_test)\n",
    "    print(\"{0}: {1:.4f} (error)\".format(name,np.mean(y_error)))\n",
    "    \n",
    "    # Plot the Result\n",
    "    plt.subplot(1,n_estimators,n+1)\n",
    "    plt.plot(np.arange(n_test), y_error, \"r\", label=\"$error(x)$\")\n",
    "    plt.ylim([0, 1300000])\n",
    "    plt.xlabel('Test Data')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title(name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Explain the differences (in 2-3 sentences) between the plots you obtain for **Tree** and **Bagging (Tree)**. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer ### \n",
    "\n",
    "As we can see from the plots, the DecisionTree error has much higher bias and variance, leading to the nature of the plot. However, the Bagging implementation not only has less bias, so we see smaller magnitude of the peaks compared to the DecisionTree, but we also see much less variance in case of Bagging. This goes to show that Bagging here does a very nice job of regularizing, as the variance decreases, which means the generalization error also decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Points**: 7.0 of 7.0\n",
    "**Comments**:\n",
    "> - None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2. Regularization: Early Stopping (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** To study how increasing neurons of a neural network (model complexity) affects the Early Stopping threshold.\n",
    "\n",
    "Download **MNIST** dataset from the NNIA's resource page on Piazza. We first update the feedforward neural network code below (at \"#TODO\") to calculate training and validation error at every 100 iterations of the training scheme. Then using this code for a different number of neurons (50 and 200), plot the variation of training and validation error for every 100th iteration up to 700 iterations. Studying these plots answer the questions given below.\n",
    "\n",
    "Note: to calculate the validation error use the test set as a proxy validation set. Also, notice the extra arguments that need to be provided to calculate the validation error at every 100 iterations in the fit function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "from __future__ import division\n",
    " \n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, \n",
    "                               '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, \n",
    "                               '%s-images-idx3-ubyte' % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', \n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, \n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", \n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, \n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    " \n",
    "    return images, labels\n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\" Feedforward neural network / Multi-layer perceptron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_output : int\n",
    "        Number of output units, should be equal to the\n",
    "        number of unique class labels.\n",
    "        \n",
    "    n_features : int\n",
    "        Number of features (dimensions) in the target dataset.\n",
    "        Should be equal to the number of columns in the X array.\n",
    "        \n",
    "    n_hidden : int (default: 30)\n",
    "        Number of hidden units.\n",
    "        \n",
    "    l1 : float (default: 0.0)\n",
    "        Lambda value for L1-regularization.\n",
    "        No regularization if l1=0.0 (default)\n",
    "        \n",
    "    l2 : float (default: 0.0)\n",
    "        Lambda value for L2-regularization.\n",
    "        No regularization if l2=0.0 (default)\n",
    "        \n",
    "    epochs : int (default: 500)\n",
    "        Number of passes over the training set.\n",
    "        \n",
    "    eta : float (default: 0.001)\n",
    "        Learning rate.\n",
    "        \n",
    "    alpha : float (default: 0.0)\n",
    "        Momentum constant.\n",
    "        \n",
    "    decrease_const : float (default: 0.0)\n",
    "        Decrease constant. Shrinks the learning rate\n",
    "        after each epoch via eta / (1 + epoch*decrease_const)\n",
    "        \n",
    "    shuffle : bool (default: True)\n",
    "        Shuffles training data every epoch if True to prevent circles.\n",
    "        \n",
    "    minibatches : int (default: 1)\n",
    "        Divides training data into k minibatches for efficiency.\n",
    "        Normal gradient descent learning if k=1 (default).\n",
    "        \n",
    "    random_state : int (default: None)\n",
    "        Set random state for shuffling and initializing the weights.\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    cost_ : list\n",
    "      Sum of squared errors after each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "      \n",
    "    def __init__(self, n_output, n_features, n_hidden=30,\n",
    "                 l1=0.0, l2=0.0, epochs=50, eta=0.001,\n",
    "                 alpha=0.0, decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, random_state=1):\n",
    "\n",
    "        self.n_output = n_output\n",
    "        self.r = np.random.RandomState(random_state)\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.w1, self.w2 = self._initialize_weights()\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "\n",
    "    def _encode_labels(self, y, k):    \n",
    "        \"\"\"Encode the labels using one-hot representation\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        y : y represents target values.\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        onehot array\n",
    "\n",
    "        \"\"\"\n",
    "       \n",
    "        onehot = np.zeros((k, y.shape[0]))\n",
    "        for idx, val in enumerate(y):\n",
    "            onehot[val, idx] = 1.0\n",
    "        \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        \n",
    "        w1 = self.r.uniform(-1.0, 1.0,\n",
    "                               size=self.n_hidden*(self.n_features + 1))\n",
    "        w1 = w1.reshape(self.n_hidden, self.n_features + 1)\n",
    "        w2 = self.r.uniform(-1.0, 1.0,\n",
    "                               size=self.n_output*(self.n_hidden + 1))\n",
    "        w2 = w2.reshape(self.n_output, self.n_hidden + 1)\n",
    "        \n",
    "        return w1, w2\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Compute logistic function (sigmoid)\n",
    "\n",
    "        Uses scipy.special.expit to avoid overflow\n",
    "        error for very small input values z.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        return expit(z)\n",
    "\n",
    "    def _sigmoid_gradient(self, z):\n",
    "        \"\"\"Compute gradient of the logistic function\"\"\"\n",
    "        \n",
    "        sg = self._sigmoid(z)\n",
    "        return sg * (1.0 - sg)\n",
    "\n",
    "    def _add_bias_unit(self, X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        \n",
    "        if how == 'column':\n",
    "            X_new = np.ones((X.shape[0], X.shape[1] + 1))\n",
    "            X_new[:, 1:] = X\n",
    "        elif how == 'row':\n",
    "            X_new = np.ones((X.shape[0] + 1, X.shape[1]))\n",
    "            X_new[1:, :] = X\n",
    "        else:\n",
    "            raise AttributeError('`how` must be `column` or `row`')\n",
    "        \n",
    "        return X_new\n",
    "\n",
    "    def _feedforward(self, X, w1, w2):\n",
    "        \"\"\"Compute feedforward step\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        a1 : array, shape = [n_samples, n_features+1]\n",
    "            Input values with bias unit.\n",
    "        z2 : array, shape = [n_hidden, n_samples]\n",
    "            Net input of hidden layer.\n",
    "        a2 : array, shape = [n_hidden+1, n_samples]\n",
    "            Activation of hidden layer.\n",
    "        z3 : array, shape = [n_output_units, n_samples]\n",
    "            Net input of output layer.\n",
    "        a3 : array, shape = [n_output_units, n_samples]\n",
    "            Activation of output layer.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        a1 = self._add_bias_unit(X, how='column')\n",
    "        z2 = w1.dot(a1.T)\n",
    "        a2 = self._sigmoid(z2)\n",
    "        a2 = self._add_bias_unit(a2, how='row')\n",
    "        z3 = w2.dot(a2)\n",
    "        a3 = self._sigmoid(z3)\n",
    "        \n",
    "        return a1, z2, a2, z3, a3\n",
    "\n",
    "    def _L2_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        \n",
    "        return (lambda_/2.0) * (np.sum(w1[:, 1:] ** 2) +\n",
    "                                np.sum(w2[:, 1:] ** 2))\n",
    "\n",
    "    def _L1_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L1-regularization cost\"\"\"\n",
    "        \n",
    "        return (lambda_/2.0) * (np.abs(w1[:, 1:]).sum() +\n",
    "                                np.abs(w2[:, 1:]).sum())\n",
    "\n",
    "    def _get_cost(self, y_enc, output, w1, w2):\n",
    "        \"\"\"Compute cost function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, shape = (n_labels, n_samples)\n",
    "            one-hot encoded class labels.\n",
    "        output : array, shape = [n_output_units, n_samples]\n",
    "            Activation of the output layer (feedforward)\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float\n",
    "            Regularized cost.\n",
    "\n",
    "        \"\"\"\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1.0 - y_enc) * np.log(1.0 - output)\n",
    "        cost = np.sum(term1 - term2)\n",
    "        L1_term = self._L1_reg(self.l1, w1, w2)\n",
    "        L2_term = self._L2_reg(self.l2, w1, w2)\n",
    "        cost = cost + L1_term + L2_term\n",
    "        \n",
    "        return cost\n",
    "\n",
    "    def _get_gradient(self, a1, a2, a3, z2, y_enc, w1, w2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        a1 : array, shape = [n_samples, n_features+1]\n",
    "            Input values with bias unit.\n",
    "        a2 : array, shape = [n_hidden+1, n_samples]\n",
    "            Activation of hidden layer.\n",
    "        a3 : array, shape = [n_output_units, n_samples]\n",
    "            Activation of output layer.\n",
    "        z2 : array, shape = [n_hidden, n_samples]\n",
    "            Net input of hidden layer.\n",
    "        y_enc : array, shape = (n_labels, n_samples)\n",
    "            one-hot encoded class labels.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        grad1 : array, shape = [n_hidden_units, n_features]\n",
    "            Gradient of the weight matrix w1.\n",
    "        grad2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Gradient of the weight matrix w2.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # backpropagation\n",
    "        sigma3 = a3 - y_enc\n",
    "        z2 = self._add_bias_unit(z2, how='row')\n",
    "        sigma2 = w2.T.dot(sigma3) * self._sigmoid_gradient(z2)\n",
    "        sigma2 = sigma2[1:, :]\n",
    "        grad1 = sigma2.dot(a1)\n",
    "        grad2 = sigma3.dot(a2.T)\n",
    "\n",
    "        # regularize\n",
    "        grad1[:, 1:] += self.l2 * w1[:, 1:]\n",
    "        grad1[:, 1:] += self.l1 * np.sign(w1[:, 1:])\n",
    "        grad2[:, 1:] += self.l2 * w2[:, 1:]\n",
    "        grad2[:, 1:] += self.l1 * np.sign(w2[:, 1:])\n",
    "\n",
    "        return grad1, grad2\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, shape = [n_samples]\n",
    "            Predicted class labels.\n",
    "\n",
    "        \"\"\"\n",
    "        if len(X.shape) != 2:\n",
    "            raise AttributeError('X must be a [n_samples, n_features] array.\\n'\n",
    "                                 'Use X[:,None] for 1-feature classification,'\n",
    "                                 '\\nor X[[i]] for 1-sample classification')\n",
    "\n",
    "        a1, z2, a2, z3, a3 = self._feedforward(X, self.w1, self.w2)\n",
    "        y_pred = np.argmax(z3, axis=0)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X, y, print_progress=False, validation_freq=0, X_val=None, y_val=None):\n",
    "        \"\"\" Learn weights from training data.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        y : array, shape = [n_samples]\n",
    "            Target class labels.\n",
    "        print_progress : bool (default: False)\n",
    "            Prints progress as the number of epochs\n",
    "            to stderr.\n",
    "        validation_freq : int (default: 0)\n",
    "            For the value \"i\" it takes, it calculates the \n",
    "            train set and validation set error every \"ith\" iteration\n",
    "        X_val : array, shape = [n_validation_samples, n_features]\n",
    "            the validation set X values, to be provided \n",
    "            when validation_freq > 0\n",
    "        y_val : array, shape = [n_validation_samples]\n",
    "            the validation set y values, to be provided\n",
    "            when validation_freq > 0\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.cost_ = []\n",
    "        self.train_err_ = []\n",
    "        self.val_err_ = []\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        y_enc = self._encode_labels(y, self.n_output)\n",
    "\n",
    "        delta_w1_prev = np.zeros(self.w1.shape)\n",
    "        delta_w2_prev = np.zeros(self.w2.shape)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress:\n",
    "                print('\\nEpoch: %d/%d' % (i+1, self.epochs))\n",
    "            \n",
    "            # Implement a code block to check the training and validation error (in percentage) for every given number\n",
    "            # of iterations stored in validation frequency. The training error is calculated on input X and validation\n",
    "            # error on input X_val. Store the training and validation error in self.train_err_ and self.val_err_            # \n",
    "            # respectively. This will help plotting the errors in the following code (2 points)\n",
    "            # TODO\n",
    "            \n",
    "            if i % validation_freq == 0:\n",
    "                if X_val is not None and y_val is not None:\n",
    "                    train_predict = self.predict(X)\n",
    "                    train_err = 0\n",
    "                    for i, val in np.ndenumerate(train_predict):\n",
    "                        if train_predict[i] != y[i]:\n",
    "                            train_err += 1\n",
    "                    train_err /= y.shape[0]\n",
    "                    train_err *= 100\n",
    "                    self.train_err_.append(train_err)\n",
    "\n",
    "                    val_predict = self.predict(X_val)\n",
    "                    val_err = 0\n",
    "                    for i, val in np.ndenumerate(val_predict):\n",
    "                        if val_predict[i] != y_val[i]:\n",
    "                            val_err += 1\n",
    "                    val_err /= y_val.shape[0]\n",
    "                    val_err *= 100\n",
    "                    self.val_err_.append(val_err)\n",
    "            \n",
    "            if self.shuffle:\n",
    "                idx = self.r.permutation(y_data.shape[0])\n",
    "                X_data, y_enc = X_data[idx], y_enc[:, idx]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                a1, z2, a2, z3, a3 = self._feedforward(X_data[idx],\n",
    "                                                       self.w1,\n",
    "                                                       self.w2)\n",
    "                cost = self._get_cost(y_enc=y_enc[:, idx],\n",
    "                                      output=a3,\n",
    "                                      w1=self.w1,\n",
    "                                      w2=self.w2)\n",
    "                self.cost_.append(cost)\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(a1=a1, a2=a2,\n",
    "                                                  a3=a3, z2=z2,\n",
    "                                                  y_enc=y_enc[:, idx],\n",
    "                                                  w1=self.w1,\n",
    "                                                  w2=self.w2)\n",
    "\n",
    "                delta_w1, delta_w2 = self.eta * grad1, self.eta * grad2\n",
    "                self.w1 -= (delta_w1 + (self.alpha * delta_w1_prev))\n",
    "                self.w2 -= (delta_w2 + (self.alpha * delta_w2_prev))\n",
    "                delta_w1_prev, delta_w2_prev = delta_w1, delta_w2\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 10000, columns: 784\n",
      "Rows: 10000, columns: 784\n",
      "Number of hidden units: 50\n",
      "Val errs: [92.06, 12.82, 11.21, 10.56, 9.379999999999999, 8.72, 8.63, 8.540000000000001]\n",
      "Training accuracy: 94.34%\n",
      "Test accuracy: 91.48%\n",
      "Number of hidden units: 200\n",
      "Val errs: [88.75, 9.2, 7.5, 6.93, 6.67, 6.529999999999999, 6.550000000000001, 6.61]\n",
      "Training accuracy: 98.08%\n",
      "Test accuracy: 93.38%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAngAAADgCAYAAACQPRs9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112d08128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load train and test data\n",
    "X_train, y_train = load_mnist('mnist/', kind='train')\n",
    "r = np.random.RandomState(1)\n",
    "idx = r.choice(y_train.shape[0],10000,replace=False)\n",
    "X_train, y_train = X_train[idx], y_train[idx]\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))\n",
    "    \n",
    "X_test, y_test = load_mnist('mnist/', kind='t10k')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))\n",
    "\n",
    "neurons = [50, 200]\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "for n, hidden in enumerate(neurons):\n",
    "    #create a MLP object\n",
    "    print(\"Number of hidden units:\",hidden)\n",
    "    nn = MLP(n_output=10, \n",
    "             n_features=X_train.shape[1], \n",
    "             n_hidden=hidden, \n",
    "             l2=0.0, \n",
    "             l1=0.0, \n",
    "             epochs=800, \n",
    "             eta=0.001,\n",
    "             alpha=0.001,\n",
    "             decrease_const=0.00001,\n",
    "             minibatches=50, \n",
    "             shuffle=False,\n",
    "             random_state=1)\n",
    "    \n",
    "    # Fit the data with NN\n",
    "    nn.fit(X_train, y_train, print_progress=False,\n",
    "             validation_freq=100,\n",
    "             X_val=X_test, y_val=y_test)\n",
    "    \n",
    "    print(\"Val errs:\",nn.val_err_)\n",
    "    \n",
    "    iter = [ i*100 for i in range(800 // 100)]\n",
    "    plt.subplot(1,len(neurons),n+1)\n",
    "    plt.plot(iter, nn.train_err_, label='Training Error')\n",
    "    plt.plot(iter, nn.val_err_, label='Validation Error')\n",
    "    plt.yscale('log')\n",
    "    plt.ylim([0, 20])\n",
    "    plt.ylabel('Error')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(\"Neurons: {0} (error)\".format(hidden))\n",
    "    \n",
    "    # Print the training accuracy and test accuracy in percentage for each value of hidden layer size. (0.5 point) \n",
    "    # TODO\n",
    "    \n",
    "    pred_labels_training = nn.predict(X_train)\n",
    "    train_acc = 0\n",
    "    #print (pred_labels_training)\n",
    "    #print (y_train)\n",
    "    for index,val in np.ndenumerate(pred_labels_training):\n",
    "        if pred_labels_training[index] == y_train[index]:\n",
    "            train_acc += 1\n",
    "    train_acc /= y_train.shape[0]\n",
    "    print('Training accuracy: %.2f%%' % (train_acc * 100))\n",
    "    \n",
    "    pred_labels_test = nn.predict(X_test)\n",
    "    test_acc = 0\n",
    "    #print (pred_labels_test)\n",
    "    #print (y_test)\n",
    "    for index,val in np.ndenumerate(pred_labels_test):\n",
    "        if pred_labels_test[index] == y_test[index]:\n",
    "            test_acc += 1\n",
    "    test_acc /= y_test.shape[0]\n",
    "    print('Test accuracy: %.2f%%' % (test_acc * 100))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Using these plots and the related variables from the code to make suggestions for an early stopping criteria for each hidden layer size. (1.5 points)\n",
    "\n",
    "b) As the number of neurons are increased, you will observe differences in the early stopping criteria for each hidden layer size. Why do you observe such differences? (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers ###\n",
    "\n",
    "a) For number of hidden units as 50, the stopping criteria is around 600 epochs. For 200, it is evident from the grpah that it is around 400. From the corresponding graphs we can see that the validation error becomes more or less constant from this point and eventually starts rising again. The criteria used can be thought of error(epoch-1) <= error(epoch), where epoch is the particular epoch where we analyze the error on the vlaidation data.\n",
    "\n",
    "\n",
    "b) As we see, as the number of neurons increase, the stopping criteria with respect to epochs decreases. This is expected because as we increase the number of hidden units, the complexity or capacity of our network increases. Hence, for regularization and decreasing generalization error, we should be stopping much before as compared to a lower value of hidden units. More the number of units, more is the complexity, and if we train for longer, more is the chance of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Points**: 6.0 of 6.0\n",
    "**Comments**:\n",
    "> - Very good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3. Regularization: Dropout (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** To implement and study dropout for neural networks.\n",
    "\n",
    "Implement dropout for layer 2 in the three-layered network you developed for Exercise 2. A simple dropout implementation creates a mask ($r^{(l)}_j$) for every neuron $j$ of the hidden layer $l$ by drawing from a [Bernoulli distribution](https://en.wikipedia.org/wiki/Bernoulli_distribution) with probability $p$.\n",
    "$$ r^{(l)}_j \\sim Bernoulli(p) $$\n",
    "This mask is then applied to the hidden layer output ($h^{(l)}$) to obtain the regularized hidden layer activation $\\hat{h}^{(l)}$\n",
    "$$ \\hat{h}^{(l)} = r^{(l)} * h^{(l)}$$\n",
    "However, such an implementation requires the layerl be multiplied by the dropout coefficient $p$ at evaluation time to balance the larger number of active units during testing.\n",
    "$$ \\hat{h}^{(l)} = p * h^{(l)}$$\n",
    "Such an implementation requires the code to switch between different code blocks for forward-pass evaluation during training and testing. Hence, a smoother way to implement dropout is to use ***inverted dropout*** where the mask generated at the training is multiplied by the inverse of the dropout coefficient.\n",
    "$$ r^{(l)}_j \\sim Bernoulli(p) * \\frac{1}{p}$$\n",
    "This scheme allows the scaling to be learned during training and hence, no switching between code blocks is required.\n",
    "\n",
    "Update the code below (specified by #TODO) to implement inverted dropout for a hidden layer size of 50 neurons. Running the code will show you variation of training and test accuracy for the dropout values given in the code below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 5000, columns: 784\n",
      "Rows: 5000, columns: 784\n",
      "Dropout: 0.10\n",
      "Training accuracy: 51.58%\n",
      "Test accuracy: 49.00%\n",
      "Dropout: 0.30\n",
      "Training accuracy: 82.56%\n",
      "Test accuracy: 81.26%\n",
      "Dropout: 0.50\n",
      "Training accuracy: 88.82%\n",
      "Test accuracy: 87.30%\n",
      "Dropout: 0.70\n",
      "Training accuracy: 91.34%\n",
      "Test accuracy: 89.02%\n",
      "Dropout: 0.90\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "from __future__ import division\n",
    " \n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, \n",
    "                               '%s-labels-idx1-ubyte' % kind)\n",
    "    images_path = os.path.join(path, \n",
    "                               '%s-images-idx3-ubyte' % kind)\n",
    "        \n",
    "    with open(labels_path, 'rb') as lbpath:\n",
    "        magic, n = struct.unpack('>II', \n",
    "                                 lbpath.read(8))\n",
    "        labels = np.fromfile(lbpath, \n",
    "                             dtype=np.uint8)\n",
    "\n",
    "    with open(images_path, 'rb') as imgpath:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", \n",
    "                                               imgpath.read(16))\n",
    "        images = np.fromfile(imgpath, \n",
    "                             dtype=np.uint8).reshape(len(labels), 784)\n",
    " \n",
    "    return images, labels\n",
    "  \n",
    "\n",
    "class MLP(object):\n",
    "    \"\"\" Feedforward neural network / Multi-layer perceptron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_output : int\n",
    "        Number of output units, should be equal to the\n",
    "        number of unique class labels.\n",
    "        \n",
    "    n_features : int\n",
    "        Number of features (dimensions) in the target dataset.\n",
    "        Should be equal to the number of columns in the X array.\n",
    "        \n",
    "    n_hidden : int (default: 30)\n",
    "        Number of hidden units.\n",
    "        \n",
    "    l1 : float (default: 0.0)\n",
    "        Lambda value for L1-regularization.\n",
    "        No regularization if l1=0.0 (default)\n",
    "        \n",
    "    l2 : float (default: 0.0)\n",
    "        Lambda value for L2-regularization.\n",
    "        No regularization if l2=0.0 (default)\n",
    "        \n",
    "    epochs : int (default: 500)\n",
    "        Number of passes over the training set.\n",
    "        \n",
    "    eta : float (default: 0.001)\n",
    "        Learning rate.\n",
    "        \n",
    "    alpha : float (default: 0.0)\n",
    "        Momentum constant.\n",
    "        \n",
    "    decrease_const : float (default: 0.0)\n",
    "        Decrease constant. Shrinks the learning rate\n",
    "        after each epoch via eta / (1 + epoch*decrease_const)\n",
    "        \n",
    "    shuffle : bool (default: True)\n",
    "        Shuffles training data every epoch if True to prevent circles.\n",
    "        \n",
    "    minibatches : int (default: 1)\n",
    "        Divides training data into k minibatches for efficiency.\n",
    "        Normal gradient descent learning if k=1 (default).\n",
    "        \n",
    "    random_state : int (default: None)\n",
    "        Set random state for shuffling and initializing the weights.\n",
    "    \n",
    "    dropout : float (default: 0.5)\n",
    "        Set the dropout coefficient\n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    cost_ : list\n",
    "      Sum of squared errors after each epoch.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "      \n",
    "    def __init__(self, n_output, n_features, n_hidden=30,\n",
    "                 l1=0.0, l2=0.0, epochs=50, eta=0.001,\n",
    "                 alpha=0.0, decrease_const=0.0, shuffle=True,\n",
    "                 minibatches=1, random_state=1, dropout = 0.5):\n",
    "\n",
    "        self.n_output = n_output\n",
    "        self.r = np.random.RandomState(random_state)\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.w1, self.w2 = self._initialize_weights()\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.eta = eta\n",
    "        self.alpha = alpha\n",
    "        self.decrease_const = decrease_const\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatches = minibatches\n",
    "        # Initialize the class variable \"dropout\" like other variables above. Also, initialize a variable mask to None.\n",
    "        # This will allow sharing dropout information during forward and backward pass of the neural networks. Note \n",
    "        # that the __init__ function has already been modified to include dropout coefficient as an argument. (0.5 points)\n",
    "        #TODO\n",
    "        self.dropout = dropout\n",
    "        self.mask_ = None\n",
    "\n",
    "    def _encode_labels(self, y, k):    \n",
    "        \"\"\"Encode the labels using one-hot representation\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        y : y represents target values.\n",
    "\n",
    "        Returns\n",
    "        -----------\n",
    "        onehot array\n",
    "\n",
    "        \"\"\"\n",
    "       \n",
    "        onehot = np.zeros((k, y.shape[0]))\n",
    "        for idx, val in enumerate(y):\n",
    "            onehot[val, idx] = 1.0\n",
    "        \n",
    "        return onehot\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights with small random numbers.\"\"\"\n",
    "        \n",
    "        w1 = self.r.uniform(-1.0, 1.0,\n",
    "                               size=self.n_hidden*(self.n_features + 1))\n",
    "        w1 = w1.reshape(self.n_hidden, self.n_features + 1)\n",
    "        w2 = self.r.uniform(-1.0, 1.0,\n",
    "                               size=self.n_output*(self.n_hidden + 1))\n",
    "        w2 = w2.reshape(self.n_output, self.n_hidden + 1)\n",
    "        \n",
    "        return w1, w2\n",
    "\n",
    "    def _sigmoid(self, z):\n",
    "        \"\"\"Compute logistic function (sigmoid)\n",
    "\n",
    "        Uses scipy.special.expit to avoid overflow\n",
    "        error for very small input values z.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        return expit(z)\n",
    "\n",
    "    def _sigmoid_gradient(self, z):\n",
    "        \"\"\"Compute gradient of the logistic function\"\"\"\n",
    "        \n",
    "        sg = self._sigmoid(z)\n",
    "        return sg * (1.0 - sg)\n",
    "\n",
    "    def _add_bias_unit(self, X, how='column'):\n",
    "        \"\"\"Add bias unit (column or row of 1s) to array at index 0\"\"\"\n",
    "        \n",
    "        if how == 'column':\n",
    "            X_new = np.ones((X.shape[0], X.shape[1] + 1))\n",
    "            X_new[:, 1:] = X\n",
    "        elif how == 'row':\n",
    "            X_new = np.ones((X.shape[0] + 1, X.shape[1]))\n",
    "            X_new[1:, :] = X\n",
    "        else:\n",
    "            raise AttributeError('`how` must be `column` or `row`')\n",
    "        \n",
    "        return X_new\n",
    "\n",
    "    def _feedforward(self, X, w1, w2):\n",
    "        \"\"\"Compute feedforward step\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        a1 : array, shape = [n_samples, n_features+1]\n",
    "            Input values with bias unit.\n",
    "        z2 : array, shape = [n_hidden, n_samples]\n",
    "            Net input of hidden layer.\n",
    "        a2 : array, shape = [n_hidden+1, n_samples]\n",
    "            Activation of hidden layer.\n",
    "        z3 : array, shape = [n_output_units, n_samples]\n",
    "            Net input of output layer.\n",
    "        a3 : array, shape = [n_output_units, n_samples]\n",
    "            Activation of output layer.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        a1 = self._add_bias_unit(X, how='column')\n",
    "        z2 = w1.dot(a1.T)\n",
    "        a2 = self._sigmoid(z2)\n",
    "        a2 = self._add_bias_unit(a2, how='row')\n",
    "        \n",
    "        # Implement inverted dropout using class variables dropout and activation variable (a2) for the forward\n",
    "        # pass for the second hidden layer below. To create the mask you will have to use self.r.binomial for \n",
    "        # generating the bernoulli distribution. The mask created here needs to be stored in the appropriate mask\n",
    "        # variable defined in the __init__ function for further use by the backward pass. (2.5 points)\n",
    "        #TODO\n",
    "        self.mask_ = self.r.binomial(1,self.dropout,size = self.n_hidden+1)\n",
    "        #self.mask_ = np.matmul(self.mask_,np.ones(()))\n",
    "        a2 = a2 * self.mask_[:,np.newaxis] * (1 / self.dropout)\n",
    "        \n",
    "        z3 = w2.dot(a2)\n",
    "        a3 = self._sigmoid(z3)\n",
    "        \n",
    "        return a1, z2, a2, z3, a3\n",
    "\n",
    "    def _L2_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L2-regularization cost\"\"\"\n",
    "        \n",
    "        return (lambda_/2.0) * (np.sum(w1[:, 1:] ** 2) +\n",
    "                                np.sum(w2[:, 1:] ** 2))\n",
    "\n",
    "    def _L1_reg(self, lambda_, w1, w2):\n",
    "        \"\"\"Compute L1-regularization cost\"\"\"\n",
    "        \n",
    "        return (lambda_/2.0) * (np.abs(w1[:, 1:]).sum() +\n",
    "                                np.abs(w2[:, 1:]).sum())\n",
    "\n",
    "    def _get_cost(self, y_enc, output, w1, w2):\n",
    "        \"\"\"Compute cost function.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, shape = (n_labels, n_samples)\n",
    "            one-hot encoded class labels.\n",
    "        output : array, shape = [n_output_units, n_samples]\n",
    "            Activation of the output layer (feedforward)\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float\n",
    "            Regularized cost.\n",
    "\n",
    "        \"\"\"\n",
    "        np.seterr(divide='ignore')\n",
    "        term1 = -y_enc * (np.log(output))\n",
    "        term2 = (1.0 - y_enc) * np.log(1.0 - output)\n",
    "        \n",
    "        term1[np.isneginf(term1)] = 0\n",
    "        term2[np.isneginf(term2)] = 0\n",
    "        \n",
    "        cost = np.sum(term1 - term2)\n",
    "        L1_term = self._L1_reg(self.l1, w1, w2)\n",
    "        L2_term = self._L2_reg(self.l2, w1, w2)\n",
    "        cost = cost + L1_term + L2_term\n",
    "        \n",
    "        return cost\n",
    "\n",
    "    def _get_gradient(self, a1, a2, a3, z2, y_enc, w1, w2):\n",
    "        \"\"\" Compute gradient step using backpropagation.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        a1 : array, shape = [n_samples, n_features+1]\n",
    "            Input values with bias unit.\n",
    "        a2 : array, shape = [n_hidden+1, n_samples]\n",
    "            Activation of hidden layer.\n",
    "        a3 : array, shape = [n_output_units, n_samples]\n",
    "            Activation of output layer.\n",
    "        z2 : array, shape = [n_hidden, n_samples]\n",
    "            Net input of hidden layer.\n",
    "        y_enc : array, shape = (n_labels, n_samples)\n",
    "            one-hot encoded class labels.\n",
    "        w1 : array, shape = [n_hidden_units, n_features]\n",
    "            Weight matrix for input layer -> hidden layer.\n",
    "        w2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Weight matrix for hidden layer -> output layer.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        grad1 : array, shape = [n_hidden_units, n_features]\n",
    "            Gradient of the weight matrix w1.\n",
    "        grad2 : array, shape = [n_output_units, n_hidden_units]\n",
    "            Gradient of the weight matrix w2.\n",
    "        \"\"\"\n",
    "        \n",
    "        # backpropagation\n",
    "        sigma3 = a3 - y_enc\n",
    "        z2 = self._add_bias_unit(z2, how='row')\n",
    "        sigma2 = w2.T.dot(sigma3) * self._sigmoid_gradient(z2)\n",
    "        \n",
    "        # Implement dropout for the backward pass, use class variables for mask and dropout for this task (2.5 points)\n",
    "        # TODO\n",
    "        # Using the mask, and also multiplying by the (1/dropout) for proper inverted dropout\n",
    "        sigma2 = sigma2 * self.mask_[:,np.newaxis] * (1 / self.dropout)\n",
    "        \n",
    "        sigma2 = sigma2[1:, :]\n",
    "        grad1 = sigma2.dot(a1)\n",
    "        grad2 = sigma3.dot(a2.T)\n",
    "\n",
    "        # regularize\n",
    "        grad1[:, 1:] += self.l2 * w1[:, 1:]\n",
    "        grad1[:, 1:] += self.l1 * np.sign(w1[:, 1:])\n",
    "        grad2[:, 1:] += self.l2 * w2[:, 1:]\n",
    "        grad2[:, 1:] += self.l1 * np.sign(w2[:, 1:])\n",
    "\n",
    "        return grad1, grad2\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, shape = [n_samples]\n",
    "            Predicted class labels.\n",
    "\n",
    "        \"\"\"\n",
    "        if len(X.shape) != 2:\n",
    "            raise AttributeError('X must be a [n_samples, n_features] array.\\n'\n",
    "                                 'Use X[:,None] for 1-feature classification,'\n",
    "                                 '\\nor X[[i]] for 1-sample classification')\n",
    "        # Setting dropout to 1 before predicting\n",
    "        # When we are predicting using the network, we should not be using the dropout probability or mask. \n",
    "        # We use all the neurons for this purpose.\n",
    "        self.dropout = 1\n",
    "        a1, z2, a2, z3, a3 = self._feedforward(X, self.w1, self.w2)\n",
    "        y_pred = np.argmax(z3, axis=0)\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def fit(self, X, y, print_progress=False):\n",
    "        \"\"\" Learn weights from training data.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            Input layer with original features.\n",
    "        y : array, shape = [n_samples]\n",
    "            Target class labels.\n",
    "        print_progress : bool (default: False)\n",
    "            Prints progress as the number of epochs\n",
    "            to stderr.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.cost_ = []\n",
    "        X_data, y_data = X.copy(), y.copy()\n",
    "        y_enc = self._encode_labels(y, self.n_output)\n",
    "\n",
    "        delta_w1_prev = np.zeros(self.w1.shape)\n",
    "        delta_w2_prev = np.zeros(self.w2.shape)\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            # adaptive learning rate\n",
    "            self.eta /= (1 + self.decrease_const*i)\n",
    "\n",
    "            if print_progress:\n",
    "                print('\\nEpoch: %d/%d' % (i+1, self.epochs))\n",
    "\n",
    "            if self.shuffle:\n",
    "                idx = np.random.permutation(y_data.shape[0])\n",
    "                X_data, y_enc = X_data[idx], y_enc[:, idx]\n",
    "\n",
    "            mini = np.array_split(range(y_data.shape[0]), self.minibatches)\n",
    "            for idx in mini:\n",
    "\n",
    "                # feedforward\n",
    "                a1, z2, a2, z3, a3 = self._feedforward(X_data[idx],\n",
    "                                                       self.w1,\n",
    "                                                       self.w2)\n",
    "                cost = self._get_cost(y_enc=y_enc[:, idx],\n",
    "                                      output=a3,\n",
    "                                      w1=self.w1,\n",
    "                                      w2=self.w2)\n",
    "                self.cost_.append(cost)\n",
    "\n",
    "                # compute gradient via backpropagation\n",
    "                grad1, grad2 = self._get_gradient(a1=a1, a2=a2,\n",
    "                                                  a3=a3, z2=z2,\n",
    "                                                  y_enc=y_enc[:, idx],\n",
    "                                                  w1=self.w1,\n",
    "                                                  w2=self.w2)\n",
    "\n",
    "                delta_w1, delta_w2 = self.eta * grad1, self.eta * grad2\n",
    "                self.w1 -= (delta_w1 + (self.alpha * delta_w1_prev))\n",
    "                self.w2 -= (delta_w2 + (self.alpha * delta_w2_prev))\n",
    "                delta_w1_prev, delta_w2_prev = delta_w1, delta_w2\n",
    "\n",
    "        return self\n",
    "\n",
    "\n",
    "# Load train and test data\n",
    "X_train, y_train = load_mnist('mnist/', kind='train')\n",
    "r = np.random.RandomState(1)\n",
    "idx = r.choice(y_train.shape[0],5000,replace=False)\n",
    "X_train, y_train = X_train[idx], y_train[idx]\n",
    "print('Rows: %d, columns: %d' % (X_train.shape[0], X_train.shape[1]))\n",
    "    \n",
    "X_test, y_test = load_mnist('mnist/', kind='t10k')\n",
    "idx = r.choice(y_test.shape[0],5000,replace=False)\n",
    "X_test, y_test = X_test[idx], y_test[idx]\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))\n",
    "  \n",
    "vals = np.arange(0.1, 1.0, 0.2)\n",
    "for dropout in vals:\n",
    "    # create a MLP object\n",
    "    print('Dropout: %.2f' % dropout)\n",
    "    nn = MLP(n_output=10, \n",
    "             n_features=X_train.shape[1], \n",
    "             n_hidden=50, \n",
    "             l2=0.0, \n",
    "             l1=0.0, \n",
    "             epochs=500, \n",
    "             eta=0.001,\n",
    "             alpha=0.001,\n",
    "             decrease_const=0.00001,\n",
    "             minibatches=50, \n",
    "             shuffle=False,\n",
    "             random_state=1,\n",
    "             dropout = dropout)\n",
    "    \n",
    "    # Fit the data with NN\n",
    "    nn.fit(X_train, y_train, print_progress=False)\n",
    "    \n",
    "    # Print the training accuracy and test accuracy in percentage for each value of dropout. This part is same as \n",
    "    # a question asked in Exercise 2, so you may use the code from there. (0 points) \n",
    "    # TODO\n",
    "    pred_labels_training = nn.predict(X_train)\n",
    "    train_acc = 0\n",
    "    #print (pred_labels_training)\n",
    "    #print (y_train)\n",
    "    for index,val in np.ndenumerate(pred_labels_training):\n",
    "        if pred_labels_training[index] == y_train[index]:\n",
    "            train_acc += 1\n",
    "    train_acc /= y_train.shape[0]\n",
    "    print('Training accuracy: %.2f%%' % (train_acc * 100))\n",
    "    \n",
    "    pred_labels_test = nn.predict(X_test)\n",
    "    test_acc = 0\n",
    "    #print (pred_labels_test)\n",
    "    #print (y_test)\n",
    "    for index,val in np.ndenumerate(pred_labels_test):\n",
    "        if pred_labels_test[index] == y_test[index]:\n",
    "            test_acc += 1\n",
    "    test_acc /= y_test.shape[0]\n",
    "    print('Test accuracy: %.2f%%' % (test_acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Intuitively, L1 and L2 minimize the interdependence and the value of feature weights by penalising the loss function. In the same vein, what kind of interdependence does dropout affect? (0.5 points)\n",
    "\n",
    "b) Why can Dropout be considered as an approximation to Bagging? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers ###\n",
    "\n",
    "a) Dropout basically trains a bagged ensemble of models. However, it also trains an ensemble of models that actually ends up sharing hidden units. So, for proper performance, each hidden unit must perform to it's best, irrespective of the presence or absence of other hidden units. As we have an ensemble of models that eventually share many hidden units, these units must be trained such that they can be interchanged or swapped and still give proper prediction capabilities. Dropout thus regularizes each hidden unit to be a good feature for a model, and also to be a good prediction feature for multiple contexts. Hence, *dropout* ends up affecting the *interdependence between the hidden units* that are used in the parent network.\n",
    "\n",
    "b) Bagging is simply creating multiple data sets by sampling of our entire set of training data, then training multiple models on these k samples and taking the average over all these models to get our final prediction. We can consider dropout to be an approximation and a more *efficient* way of doing the bagging. In dropout, we generate models by applying a probability on each hidden unit to be active or inactive. We can consider putting a probabilistic mask on the hidden units, and thereby create multiple models which we can train together and which share some family of parameters. Also we do stochastic learning on the models using minibatch algorithms, and that actually approximates the sampling of the training set. Eventually, we make predictions by taking a sort of average over multiple stochastic decisions. Hence, we can clearly say that dropout is a sort of stochastic approximation of bagging with parameter sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Points**: 7.0 of 7.0\n",
    "**Comments**:\n",
    "> - None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading: 20.0 of 20.0 points. Well done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "You should provide a single Jupyter notebook as a solution. The naming should include the assignment number and matriculation IDs of all team members in the following format:\n",
    "**assignment-7_matriculation1_matriculation2_matriculation3.ipynb** (in case of 3 team members). \n",
    "Make sure to keep the order matriculation1_matriculation2_matriculation3 the same for all assignments.\n",
    "\n",
    "Please, submit your solution to your tutor (with **[NNIA][assignment-7]** in email subject):\n",
    "1. Maksym Andriushchenko s8mmandr@stud.uni-saarland.de\n",
    "2. Marius Mosbach s9msmosb@stud.uni-saarland.de\n",
    "3. Rajarshi Biswas rbisw17@gmail.com\n",
    "4. Marimuthu Kalimuthu s8makali@stud.uni-saarland.de\n",
    "\n",
    "**If you are in a team, please submit only 1 solution to only 1 tutor.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
