{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNIA 18/19 Project 1:  PCA and & (Multiple) Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deadline: 07. December 2018, 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important:** For all implementations in this project, make sure to use NumPy whenever possible. Most computations on vectors and matrices can be implemented very efficiently using the NumPy API. There is no need for looping over vectors etc. As a simple example, in order to compute the mean of a vector, just use `numpy.mean()`. If you are not familiar with NumPy please consult the NumPy tutorial that you find in the CMS under *Materials*. Further, in case of any doubts, the Forum is the best place to ask questions and discuss the project.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Principal Component Analysis $~$ (15.0 points)\n",
    "\n",
    "Features are our friends for prediction. For example, knowing the weight of a person is helpful for predicting his/her height. Knowing both the weight and age may improve the accuracy of our prediction. However, too many features could harm: each feature can be thought of as a dimension, $n$ features correspond to a $n$ dimensional space. In a high dimensional space ($n$ is very large), our data will distribute sparsely: if you draw unit grids in the space, the most of them will contain no data, which obstructs the learning process. This is called the *curse of dimensionality* (for more detail about the curse of dimensionality, see [here](http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)).\n",
    "\n",
    "\n",
    "To overcome the curse of dimensionality, we simply reduce the dimensionality (i.e. reduce the number of features). We can either select a subset of all features, or we can apply PCA on our dataset. By specifying a $d$ ($d$<$n$), PCA will project our $n$ dimensional data onto a $d$ dimensional (affine) space. The assumption is that classification/regression should be easier in this $d$ dimensional space.\n",
    "\n",
    "A tutorial of PCA can be found here: [PCA Tutorial](http://www.iro.umontreal.ca/~pift6080/H09/documents/papers/pca_tutorial.pdf) \n",
    "\n",
    "The goal of this exercise is to apply PCA on a small dataset: [The Digits Dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Data Normalization\n",
    "Before applying PCA to our data, we first need to center the features by subtracting the mean of each feature. That is, given a design matrix $X \\in \\mathbb{R}^{n \\times d}$ ($n$ samples, each with $d$ dimensional features), we apply the following formula:\n",
    "\n",
    "\\begin{equation*}\n",
    "  \\hat{x_{ij}} = x_{ij} - \\mu(x_i) \\textrm{ for } i = 1,\\cdots, d, j = 1,\\cdots,n\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\mu(x_i)$ is the mean of the $i$-th column of $X$.\n",
    "\n",
    "**Question 1:** Is this centering step important regarding the performance of the PCA? What might happen if we don't center our data? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1:** Implement the following function which performs data normalization (i.e. normalize the columns of $X$) according to the description above. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_normalization(data):\n",
    "    # TODO: implement\n",
    "\n",
    "    # data_normalized = \n",
    "    \n",
    "    return data_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2  Apply PCA on Digits Dataset\n",
    "In the following we will apply PCA on the digits dataset from sklearn. This dataset consists of 1797 images of size 8X8, each images contain a handwritten digit (0-9).\n",
    "The digits look as follows:\n",
    "![Image of digits](https://scikit-learn.org/stable/_images/sphx_glr_plot_digits_classification_001.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the digits dataset\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# perform data normalization\n",
    "data_matrix = datasets.load_digits().data\n",
    "targets = datasets.load_digits().target\n",
    "data_normalized = data_normalization(data_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying PCA, let's take a look on a subset of original features and analyse whether they are discriminative for digits classification.\n",
    "\n",
    "**Task 2:** Implement the following using matplotlib: (2 points)\n",
    "1. Construct a 2D scatter plot. Plot the value of feature 0 (on $x$ axis) and feature 2 (on $y$ axis) for each data point, use different colors indicating different classes. \n",
    "2. Set corresponding labels: assign label \"feature 0\" for $x$ axis and \"feature 2\" for $y$ axis.\n",
    "3. Set the title of the plot as \"A subset of original features\".\n",
    "4. Show legends for each class.\n",
    "5. After you have finished the first four steps and plotted the graph, you might observe that many data points overlap. This is because many data points have exactly the same value w.r.t. feature 0 and feature 2. In order to have a better visualization, please jitter the data points in your scatter plot, this step is called jittering. For more details about jittering, see [here](https://stats.stackexchange.com/questions/253009/why-jitter-continuous-value-in-a-scatterplot).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Construct a 2D scatter plot\n",
    "x = data_normalized[:,0] \n",
    "y = data_normalized[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** What is the dimensionality of the new feature space? Is is easy to perform digit classification in this feature space? Justify your answer. (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3:** Implement the function `PCA()` below. The input of this function is a normalized dataset and a parameter $d$, which specifies the output dimension (dimension of the projected space). The output should be a dataset/matrix of $\\mathbb{R}^{n \\times d}$. (Hint: `numpy.linalg.eig()` might be helpful here.) (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def PCA(data, d):\n",
    "    \"\"\"\n",
    "    perform PCA on a dataset\n",
    "    :param data: input dataset with shape (n,k).\n",
    "    :param d: dimension of the output space.\n",
    "    :return: a matrix of shape (n,d). where each row represents the (PCA) projection of each data point.\n",
    "    \"\"\"\n",
    "    # TODO: implement\n",
    "    # REMARK: note the eigenvalues/eigenvectors returned by np.linalg.eig() might not be ordered. You may want to order them first. \n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $4.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4:** Apply `PCA()` on the normalized dataset using $d=2$, then construct a 2D scatter plot presenting the data points in the projected space. (2 points)\n",
    "\n",
    "Requirements for the 2D scatter plot:\n",
    "1. Plot the value of the first principle components (on $x$ axis) and the second principle components (on $y$ axis) of each data point, use different colors for each class. \n",
    "2. Set corresponding labels: assign label \"first principle component\" for $x$ axis and \"second principle component\" for $y$ axis.\n",
    "3. Set the title of the plot as \"2 component PCA\".\n",
    "4. Add legends for each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: perform PCA on mean centered dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO: Construct a 2D scatter plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Is it easier to perform classification on the transformed space? Justify your answer. (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** Now, modify the function  `data_normalization()` you implemented above: in additional to centering, divide each feature/column by its standard deviation. Then apply `PCA()` on this standardized data. Does the performance of PCA improve? Describe your findings (instead of modifying the original function you implemented above, please implement it again in the cell below). (2 points)\n",
    "\n",
    "***Remark:*** One problem could occur when you divide the feature by its standard deviation: the standard deviation might be zero and division by zero is undefined. However, a standard deviation of zero can only be possible when all the values of a feature/column are the same (all equal to the mean). In this case, those features have no discriminative power so they can be removed from the analysis. Your implementation should be able to handle this case: all columns with zero standard deviation should be removed before standardize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_normalization_modified(data):\n",
    "    # TODO: implement\n",
    "\n",
    "    # data_normalized = \n",
    "    \n",
    "    return data_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, consider and the following (artificially generated) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.random.normal(loc=0.0, scale=2.0, size=100)\n",
    "y = np.random.binomial(1, 0.5, size=100) - 0.5\n",
    "\n",
    "fig = plt.figure(figsize = (7, 7))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('feature 0', fontsize = 15)\n",
    "ax.set_ylabel('feature 1', fontsize = 15)\n",
    "ax.scatter(x,y,s = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** Is PCA suitable for the above dataset? Justify your answer. (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression (15.0 points)\n",
    "\n",
    "In this exercise we will deal with *multiple linear regression*. Performing regression on one independent (or explanatory) variable and a scalar dependent variable is called **simple linear regression**.\n",
    "But, when there are more than one explanatory variable (i.e. $x^{(1)}, x^{(2)}, ...,x^{(k)}$), and a single scalar dependent variable (*y*), then it's called **multiple linear regression**. (Please don't confuse this with *multivariate linear regression* where we predict more than one (correlated) dependent variable.)\n",
    "\n",
    "Here, we will implement a **multiple linear regression** model in Python/NumPy using the *Gradient Descent* algorithm. Particularly, we will be using **stochastic gradient descent** (*SGD*) where one performs the update step using a small set of training samples of size *batch_size* which we will set to 64, i.e. we go through the training samples sampling 64 at a time and perform gradient descent. Such a procedure is sometimes called **mini-batch gradient descent** in the deep learning community.\n",
    "\n",
    "Going through all the training samples *once* is called an **epoch**. Ideally, the algorithm has to go through multiple epochs over the training samples, each time shuffling it, until a convergence criterion has been satisfied.\n",
    "\n",
    "Here, we will set a *tolerance value* for the difference in error (i.e. change in mean squared error (MSE) values between subsequent epochs) that we will accept. Once this difference falls below the *tolerance value*, we terminate our training phase and return the parameters. \n",
    "\n",
    "We repeat the above training procedure for all possible hyperparameter combinations in order to find the best parameters (i.e. weights) for our model. For this so called *hyperparameter tuning* we will be using the validation data. \n",
    "\n",
    "As a next step, we will combine training data and validation data and make it as our *new training data*. We keep the test data as it is. Using the hyperparameter combination (for the least MSE) that we found above, we train the model *again* with the *new training data* and obtain the parameter (*i.e. weight vector*) after convergence according to our *tolerance value*.\n",
    "\n",
    "Phew! That will be our much desired *weight vector*. This is then used on the *test data*, which has not been seen by our algorithm so far, to make a prediction. The resulting MSE value will be the so-called [*generalization error*](https://en.wikipedia.org/wiki/Generalization_error). It is this *generalization error* that we want it to be as low as possible for *unseen data* (implies that we can achieve higher accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Dataset\n",
    "For our task, we will be using the *Wine Quality* dataset and predict the quality of white wine based on 11 features such as acidity, citric acid content, residual sugar etc. . You can take a glance of the data using functions like *data.head()*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get data\n",
    "data_url = 'http://mlr.cs.umass.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv'\n",
    "data = pd.read_csv(data_url, sep=';')\n",
    "\n",
    "# inspect data\n",
    "display(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. Loss function\n",
    "We will use a *regularized* form of the MSE loss function. In matrix form it can be written as follows:\n",
    "\n",
    "\\begin{equation*}\n",
    "    J(\\textbf{w}) = \\frac{1}{2} \\Vert{X\\textbf{w}-\\textbf{y}}\\Vert^{2} + \\frac{\\lambda}{2}\\Vert{\\textbf{w}}\\Vert^{2}\n",
    "\\end{equation*}\n",
    "\n",
    "It's important to note that, in the above equation, $X$, called **design matrix**, consists of data points in our dataset. Each row corresponds to a data point, each column correspondings to a feature. Therefore, the dimension of $X$ is *(number of data points, number of features)*. $X$ can be also thought as of the horizontal concatenation of shape *(batch_size, num_features)*. To make things easier, you can add the *bias* term as the first column of $X$. Take care to have the *weight* vector $\\textbf{w}$ with matching dimensions. (Hint: see [Design_matrix#Multiple_regression](https://en.wikipedia.org/wiki/Design_matrix#Multiple_regression) for how $X$ with 2 features looks like for $1^{st}$ degree polynomial.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5:** Derive the gradient (w.r.t $\\textbf{w}$) for the regularized loss function given in 2.2. (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3. Matrix format for higher order polynomial\n",
    "\n",
    "Written in matrix form, a linear regression model for second order would look like: <br />\n",
    "$$\\hat{\\textbf{y}} = X\\textbf{w}_{1} + X^{2}\\textbf{w}_{2} + \\textbf{b}$$\n",
    "\n",
    "where $X^{2}$ is the element-wise squaring of the original design matrix $X$, $\\textbf{w}_1$ and $\\textbf{w}_2$ are the *weight* vectors, and **b** is the *bias* vector.\n",
    "\n",
    "**Task 6:** Please write down the matrix format for a $9^{th}$ order linear regression model. (0.5 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4. Hyperparameters\n",
    "Next, we will experiment with three hyperparameters:\n",
    "\n",
    "i) regularization parameter $\\lambda$ <br />\n",
    "ii) learning rate $\\epsilon$ <br />\n",
    "iii) order of polynomial *p*\n",
    "\n",
    "And do a grid search over the values that these hyperparameters can take in order to select the best combination (i.e. the one that achieves lowest test error). This approach is called **hyperparameter optimization or tuning**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fix possible hyperparameters\n",
    "polynomial_orders = [1, 5, 9]\n",
    "learning_rates = [1e-5, 1e-8]\n",
    "lambdas = [0.1, 0.8]\n",
    "\n",
    "# Fix batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Get data as NumPy array\n",
    "data_np = data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5. Normalization\n",
    "First of all, inspect the data, and understand its structure and features. Ideally, before starting to train our learning algorithm, we would want the data to be normalized. Here, we normalize the data (i.e. normalize each column) using the formula:\n",
    "\n",
    "\\begin{equation*}\n",
    "  norm\\_x_i = \\frac{x_i - min(x)}{max(x) - min(x)}\n",
    "\\end{equation*}\n",
    "where $x_i$ is the $i^{th}$ sample in feature $x$. Note that this formula is different to the formula in the PCA part.\n",
    "\n",
    "**Task 7:** Complete the following function which performs normalization (i.e. normalizes columns of $X$). (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_normalization(data):\n",
    "    # TODO: implement\n",
    "    pass\n",
    "\n",
    "# Perform data normalization\n",
    "data_np = data_normalization(data_np)\n",
    "print(data_np[:5, ])  # print first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** How is the normalization here different from the standardization implemented in the PCA exercise (see Question 4 in the PCA part)? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test data\n",
    "def split_data(data, n_train=3898, n_val=500, n_test=500):\n",
    "    # (in-place) shuffling of data_npr along axis 0\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    n_val = n_train + n_val\n",
    "    n_test = n_train + n_test\n",
    "    \n",
    "    X_train = data[0:n_train, 0:-1]\n",
    "    Y_train = data[0:n_train, -1]\n",
    "    \n",
    "    X_val = data[n_train:n_val, 0:-1]\n",
    "    Y_val = data[n_train:n_val, -1]\n",
    "    \n",
    "    X_test = data[n_test:, 0:-1]\n",
    "    Y_test = data[n_test:, -1]\n",
    "    \n",
    "    return [(X_train, Y_train), (X_val, Y_val), (X_test, Y_test)]\n",
    "\n",
    "\n",
    "# Shuffle only the training data along axis 0\n",
    "def shuffle_train_data(X_train, Y_train):\n",
    "    \"\"\"called after each epoch\"\"\"\n",
    "    perm = np.random.permutation(len(Y_train))\n",
    "    Xtr_shuf = X_train[perm]\n",
    "    Ytr_shuf = Y_train[perm]\n",
    "    \n",
    "    return Xtr_shuf, Ytr_shuf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6. Implementation of required functions\n",
    "\n",
    "**Task 8:** Complete the following function which computes the MSE value. You can ignore the regularization term and also the constants $\\frac{1}{2}$. (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute Mean Squared Error \n",
    "def compute_mse(prediction, ground_truth):\n",
    "    '''\n",
    "    :param prediction: a nx1 vector represents the prediciton of your model\n",
    "    :param ground_truth: a nx1 vector represents the ground_truth\n",
    "    :return: MSE loss\n",
    "    '''\n",
    "    # TODO: implement\n",
    "    mse = 0.0\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 9:** Implement a function which computes the prediction of your model. (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prediction(X, W):\n",
    "    '''\n",
    "    Given a design matrix X (could be a batch) and parameters W, calculate the prediction Y.\n",
    "    :param X: desgin matrix X of dimension nxd, where n is the number of data points (in the batch).\n",
    "    :param W: parameters\n",
    "    :return: the predictions\n",
    "    '''\n",
    "    # TODO: implement\n",
    "    pred = None\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 10:** Implement a function which computes the gradient of your loss function. That is, implement the gradient computed in Task 5. (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_gradient(X, Y, Yhat, W, lambda_):\n",
    "    '''\n",
    "    :param X: designmatrix X\n",
    "    :param Y: ground truth labels correspoinding to X\n",
    "    :param Yhat: predicted labels\n",
    "    :param W: parameters\n",
    "    :param lambda_: coefficient for the regularizer\n",
    "    :return: gradient w.r.t W\n",
    "    '''\n",
    "    # TODO: implement\n",
    "    grad = None\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 11:** Implement a function which performs a single update step of SGD. (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hint: avoid in-place modification\n",
    "def sgd(gradient, lr, cur_W):\n",
    "    '''\n",
    "    :param gradient: gradient at cur_W\n",
    "    :param lr: learning rate\n",
    "    :param cur_W: current value of parameters\n",
    "    :return: apply gradient descent\n",
    "    '''\n",
    "    # TODO: implement\n",
    "    new_weights = None\n",
    "    return new_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 12:** Complete the following function which reformats your data as a design matrix. (0.5 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# concatenate X acc. to order of polynomial; likewise do it for W\n",
    "# where X is design matrix, W is the corresponding weight vector\n",
    "# concatenate matrix X horizontally, concatenate W vertically. That is, after this transformation, X gets broader and W gets longer\n",
    "# [1 X X^2 X^3], [1 W1 W2 W3].T.\n",
    "# \n",
    "def prepare_data_matrix(X, W, order):\n",
    "    # TODO: implement\n",
    "    '''\n",
    "    :param X: design matrix X\n",
    "    :param W: weight vector W\n",
    "    :param order: order of the polynomial\n",
    "    :return: extended X and W\n",
    "    '''\n",
    "    X, W = None, None\n",
    "    return X, W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7. Training\n",
    "\n",
    "**Task 13:** Complete the code in the following cell such that it performs **mini-batch gradient descent** on the training data for all possible hyperparameter combinations. (5 points)\n",
    "\n",
    "Note: You can also define a function, named appropriately, which performs training. But, take care to do correct bookkeeping of hyperparameter combinations, weight vectors, and the MSE values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the data\n",
    "splits = split_data(data_np)\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = itertools.chain(*splits)\n",
    "\n",
    "# Set tolerance value\n",
    "tolerance = 1e-3\n",
    "start = 1\n",
    "\n",
    "# Initialize weight vector\n",
    "W_init = np.random.randn(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X_train, W_init, polynomial_orders, learning_rates, lambdas):\n",
    "    # Compute all hyperparameter combination\n",
    "    comb_gen = itertools.product(*(polynomial_orders, learning_rates, lambdas))\n",
    "    hparams_comb = list(comb_gen)\n",
    "\n",
    "    # Cache weights for each hyperparam combination\n",
    "    weights_hist = {hpm:0 for hpm in hparams_comb}\n",
    "    mse_dict = {hpm:0 for hpm in hparams_comb}\n",
    "    mse_hist = []\n",
    "    \n",
    "    # Find optimal hyperparameters\n",
    "    for order in polynomial_orders:\n",
    "        for lr in learning_rates:\n",
    "            for lamb in lambdas:\n",
    "                # Initialize \n",
    "                mse_hist.append(np.inf)\n",
    "                mse_diff = np.inf\n",
    "                epochs = 1\n",
    "                # TODO: Prepare data matrix\n",
    "                \n",
    "                \n",
    "                while True:\n",
    "                    # TODO: shuffle the data\n",
    "                    Xtr_shuf, Ytr_shuf = None\n",
    "                    iteration = 1\n",
    "                    nsamples = Xtr_shuf.shape[0]\n",
    "                    bs = 0\n",
    "                    \n",
    "                    # Perform a single epoch\n",
    "                    while bs < nsamples:\n",
    "                        Xtr = Xtr_shuf[bs: bs+batch_size]\n",
    "                        Ytr = Ytr_shuf[bs: bs+batch_size]\n",
    "                        \n",
    "                        # TODO: do one step gradient descent\n",
    "\n",
    "                        \n",
    "                        bs += batch_size\n",
    "                        iteration += 1\n",
    "\n",
    "                    # TODO: do predcition on the entire (shuffled) dataset, compute mse and append it to the mse_hist\n",
    "                    \n",
    "                    mse = None\n",
    "                    mse_hist.append(mse)\n",
    "                    #print(\"MSE after epoch {} is {}\".format(epochs, round(mse, 5)))\n",
    "\n",
    "                    #TODO: Stopping criterion: check whether diff-in-mse < tolerance\n",
    "                    mse_diff = None\n",
    "                    if mse_diff < tolerance:\n",
    "                        #cache weight vector\n",
    "                        weights_hist[(order, lr, lamb)] = W_vec\n",
    "                        mse_dict[(order, lr, lamb)] = mse_hist[-1]\n",
    "                        print(\"order: {} , learning rate: {} , regularizer: {} \".format(order, lr, lamb))\n",
    "                        print(\"Convergence after epoch {} with MSE {}\".format(epochs, round(mse_hist[-1], 5)), \"\\n\")\n",
    "                        break\n",
    "                    epochs += 1\n",
    "    \n",
    "    return weights_hist, mse_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $5.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 14:** Complete the following function which selects the best hyperparameter combination given a list of weights (i.e. the one that gives lowest MSE on **validation data**). (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select hparams of minimum MSE on Validation data\n",
    "def select_best_hparams(X_val, W):\n",
    "    hpm_best = ()\n",
    "    mse_best = np.inf\n",
    "    \n",
    "    # TODO: Implement\n",
    "\n",
    "    return hpm_best, mse_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Train the model with all possible hyperparameter combinations\n",
    "W_init = np.random.randn(X_train.shape[1])\n",
    "\n",
    "# TODO: Find best hyperparameter combination\n",
    "best_hpm_combination, best_mse = None\n",
    "\n",
    "print('Best hyperamarameters (on validation data): ', best_hpm_combination)\n",
    "print('Corresponding MSE (on validation data): ', best_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.8. Re-Training on Train + Validation data\n",
    "**Task 15:** Complete the following function which does re-training on the combined training and validation data. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Re-run the training on X_train + X_val combined\n",
    "\n",
    "# TODO: implement\n",
    "\n",
    "X_train_comb = None\n",
    "Y_train_comb = None\n",
    "\n",
    "# Initialize weight vector\n",
    "W_comb_init = np.random.randn(X_train_comb.shape[1])\n",
    "\n",
    "# Use already found best hparam comb\n",
    "best_order, best_lr, best_lamb = best_hpm_combination\n",
    "\n",
    "# Get data\n",
    "X_mat, W_vec = prepare_data_matrix(X_train_comb, W_comb_init, best_order)\n",
    "\n",
    "# Run training\n",
    "# TODO: implement\n",
    "weights, mse_cache = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's plot the convergence of MSE values on the train+validation dataset using matplotlib, i.e. #epochs on X-axis and MSE values on Y-axis\n",
    "fig, axes = plt.subplots()\n",
    "axes.plot(mse_cache)\n",
    "axes.set_yscale('linear')\n",
    "axes.set_xlabel(\"epochs\")\n",
    "axes.set_ylabel(\"MSE\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.9. Evaluation on Test set\n",
    "**Task 16:** Evaluate your model on test data. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test your model on X_test with the weight vector that you found above\n",
    "# this will be the generalization error of our model.\n",
    "\n",
    "# TODO: implement\n",
    "print(\"MSE achieved on X_test is : {}\".format(round(mse_test, 6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.10. Results\n",
    "**Task 17:** Report the MSE value on the test data. (0.5 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 6:**  Which hyperparameter combination turned out to be the best? In your understanding, why do you think such a combination turned out to be the best for this task? (0.5 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "You should provide a single Jupyter notebook (.ipynb file) as the solution. Put the names and student ids of your team members below. **Make sure to submit only 1 solution to only 1 tutor.**\n",
    "\n",
    "- Jane Doe, 123456\n",
    "- Jane Doe, 123456\n",
    "- John Doe, 123456"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points: 0.0 of 30.0 points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
