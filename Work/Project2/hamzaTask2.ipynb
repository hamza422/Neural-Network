{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNIA 18/19 Project 2:  Gradient Descent & Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deadline: 4. January 2018, 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multinomial Logistic Regression and Cross Validation $~$ (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement a [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) model with tensorflow for Fashion-MNIST dataset. Cross Validation will be used to find the best **regularization parameter** $\\lambda$ for the L2-regularization term. Fashion-MNIST dataset is similar to the sklearn Digit dataset you used in the Project 1. It contains 60,000 training images and 10,000 testing images. Each example is a 28×28 grayscale image, associated with a label from 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural Network](https://s3-eu-central-1.amazonaws.com/zalando-wp-zalando-research-production/2017/08/fashion-mnist-sprite.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial logistic regression is a probabilistic, linear classifier. It is parametrized by a weight matrix $W$ and a bias vector $b$. Classification is done by projecting an input vector onto a set of hyperplanes, each of which corresponds to a class. The distance from the input to a hyperplane reflects the probability that the input is a member of the corresponding class.\n",
    "\n",
    "Mathematically, the probability that an input vector $\\bf{x} \\in \\mathbb{R}^p$ is a member of a class $i$ can be written as:\n",
    "$$P(Y=i|\\textbf{x}, W, b) = softmax(W\\textbf{x} + b)_i = \\frac{e^{W_i\\textbf{x} + b_i}}{\\sum_j{e^{W_j\\textbf{x} + b_j}}}$$\n",
    "where $W \\in \\mathbb{R}^{c \\times p}$, $b \\in \\mathbb{R}^c$ and $W_i \\in \\mathbb{R}^p$.\n",
    "\n",
    "The model’s prediction $y_{pred}$ is the class whose probability is maximal, specifically:\n",
    "$$y_{pred} = argmax_iP(Y=i|\\textbf{x}, W, b)$$\n",
    "\n",
    "We use cross-entropy loss with L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load **Fashion-MNIST** dataset and normalized it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_trainval, Y_trainval), (X_test, Y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The X_trainval has the following shape:\n",
      "Rows: 60000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_trainval = np.reshape(X_trainval, (X_trainval.shape[0],  X_trainval.shape[1] *  X_trainval.shape[2]))\n",
    "print('The X_trainval has the following shape:')\n",
    "print('Rows: %d, columns: %d' % (X_trainval.shape[0], X_trainval.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The X_test has the following shape:\n",
      "Rows: 10000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_test = np.reshape(X_test, (X_test.shape[0],  X_test.shape[1] *  X_test.shape[2]))\n",
    "print('The X_test has the following shape:')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data. Subtract the mean and divide by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_normalization(X_trainval, X_test):\n",
    "    train = np.copy(X_trainval)\n",
    "    test  = np.copy(X_test)\n",
    "    for row in np.transpose(train):\n",
    "        deviation = np.std(row)\n",
    "        if(deviation != 0):\n",
    "            row= row / deviation\n",
    "        row -= np.mean(row)\n",
    "        \n",
    "    for row in np.transpose(test):\n",
    "        deviation = np.std(row)\n",
    "        if(deviation != 0):\n",
    "            row = row / deviation\n",
    "        row -= np.mean(row)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# The normalization should be done on X_train and X_test. \n",
    "# The normalized data should have the exactly same shape as the original data matrix.\n",
    "\n",
    "X_trainval, X_test = data_normalization(X_trainval, X_test)\n",
    "print(np.shape(X_trainval),np.shape(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define the Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the global configuration of this program is \n",
    "# defined, which you shouldn't change.\n",
    "\n",
    "class global_config(object):\n",
    "    lr = 0.0001  # learning rate\n",
    "    img_h = 28  # image height\n",
    "    img_w = 28  # image width\n",
    "    num_class = 10  # number of classes\n",
    "    num_epoch = 20  # number of training epochs\n",
    "    batch_size = 16  # batch size\n",
    "    K = 3  # K-fold cross validation\n",
    "    num_train = None  # the number of training data\n",
    "    lambd = None  # the factor for the L2-regularization\n",
    "\n",
    "config = global_config()\n",
    "config.num_train = X_trainval.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(X_trainval, Y_trainval, i, K):\n",
    "    \"\"\"\n",
    "    sklearn library is not allowed to use here.\n",
    "    \n",
    "    K is the total number of folds and i is the current fold.\n",
    "    \n",
    "    Think about how to deal with the case when the number of \n",
    "    training data can't be divided by K evenly.\n",
    "    #TODO: Implement\"\"\"\n",
    "    elements,_ = np.shape(Y_trainval)\n",
    "    surplus = elements % K\n",
    "    size_big = (elements/surplus)+1\n",
    "    size_small = elements\n",
    "    \n",
    "    if i < surplus:\n",
    "        slice_beg = i*size_big\n",
    "        slice_end = slice_beg + size_big\n",
    "        \n",
    "    if i >= surplus:\n",
    "        slice_beg = surplus*size_big+(i-surplus)*size_small\n",
    "        slice_end = slice_beg + size_small\n",
    "        \n",
    "    X_val = X_trainval[slice_beg:slice_end]\n",
    "    Y_val = Y_trainval[slice_beg:slice_end]\n",
    "    \n",
    "    X_train_lower = X_trainval[:slice_beg]\n",
    "    X_train_upper = X_trainval[slice_end:]\n",
    "    X_train       = np.concatenate((X_train_lower, X_train_upper))\n",
    "    \n",
    "    Y_train_lower = Y_trainval[:slice_beg]\n",
    "    Y_train_upper = Y_trainval[slice_end:]\n",
    "    Y_train       = np.concatenate((Y_train_lower, Y_train_upper))\n",
    "\n",
    "    \n",
    "        \n",
    "    return X_train, X_val, Y_train, Y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def shuffle_train_data(X_train, Y_train):\n",
    "    \"\"\"called after each epoch\"\"\"\n",
    "    perm = np.random.permutation(len(Y_train))\n",
    "    Xtr_shuf = X_train[perm]\n",
    "    Ytr_shuf = Y_train[perm]\n",
    "    return Xtr_shuf, Ytr_shuf\n",
    "\n",
    "np.shape(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "training\n",
    "\"\"\"\n",
    "class logistic_regression(object):\n",
    "    \n",
    "    def __init__(self, X, Y_gt, config, name):\n",
    "        \"\"\"\n",
    "        :param X: the training batch, which has the shape [batch_size, n_features].\n",
    "        :param Y_gt: the corresponding ground truth label vector.\n",
    "        :param config: the hyper-parameters you need for the implementation.\n",
    "        :param name: the name of this logistic regression model which is used to\n",
    "                     avoid the naming confict with the help of tf.variable_scope and reuse.\n",
    "       \n",
    "        Define the computation graph within the variable_scope here. \n",
    "        First define two variables W and b with tf.get_variable.\n",
    "        Then do the forward pass.\n",
    "        Then compute the cross entropy loss with tensorflow, don't forget the L2-regularization.\n",
    "        The Adam optimizer is already given. You shouldn't change it.\n",
    "        Finally compute the accuracy for one batch\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "            #TODO: Define two variables and the forward pass.\n",
    "            W = tf.get_variable(\"W\",shape=[samples, features]) \n",
    "            b = tf.get_variable(\"b\",shape=[features])\n",
    "    forwardPass = np.add (np.dot(np.transpose(W),X),b)\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(forwardPass)\n",
    "    l2regularizer = tf.nn.l2_loss(W)\n",
    "    loss = loss + lamda*l2regularizer\n",
    "    self._loss = np.reduce_mean(loss)\n",
    "            #TODO: Compute the cross entropy loss with L2-regularization.\n",
    "            i = 0\n",
    "            for yhat in self.train_step\n",
    "                if (yhat ==Y_gt[i])\n",
    "                self._num_acc = self._num_acc + 1 \n",
    "                \n",
    "                i++ \n",
    "            self._num_acc = self._num_acc/ i \n",
    "            # Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent \n",
    "            # to update network weights iteratively.\n",
    "            # It will be introduced in the lecture when talking about the optimization algorithms.\n",
    "            self._train_step = tf.train.AdamOptimizer(config.lr).minimize(self._loss)\n",
    "            \n",
    "            #TODO: Compute the accuracy\n",
    "            \n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_step\n",
    "    \n",
    "    @property\n",
    "    def loss(self):\n",
    "        return self._loss\n",
    "    \n",
    "    @property\n",
    "    def num_acc(self):\n",
    "        return self._num_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model, X_test, Y_test, config):\n",
    "    \"\"\" \n",
    "    Go through the X_test and use sess.run() to compute the loss and accuracy.\n",
    "    \n",
    "    Return the total loss and the accuracy for X_test.\n",
    "    \n",
    "    Note that this function will be used for the validation data\n",
    "    during training and the test data after training.\n",
    "    \"\"\"\n",
    "    num_test = X_test.shape[0]\n",
    "    total_cost = 0\n",
    "    accs = 0\n",
    "    #TODO: Implement\n",
    "    return total_cost / len(Y_test), accs / len(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, X_val, Y_train, Y_val, config):\n",
    "    \"\"\"\n",
    "    Train the model with sess.run().\n",
    "    \n",
    "    You should shuffle the data after each epoch and\n",
    "    evaluate training and validation loss after each epoch.\n",
    "    \n",
    "    Return the lists of the training/validation loss and accuracy.\n",
    "    \"\"\"\n",
    "    cost_trains = []\n",
    "    acc_trains = []\n",
    "    cost_vals = []\n",
    "    acc_vals = []\n",
    "    \n",
    "    for i in range(config.num_epoch):\n",
    "       #TODO: Implement\n",
    "        cost_trains.append(cost_train)\n",
    "        acc_trains.append(acc_train)\n",
    "        print(\"Epoch: %d :\" % (i + 1))\n",
    "        print(\"Train Loss: %f\" %  cost_train)\n",
    "        print(\"Training acc: %f\" % acc_train)\n",
    "        cost_vals.append(cost_val)\n",
    "        acc_vals.append(acc_val)\n",
    "        print(\"Validation Loss: %f\" % cost_val)\n",
    "        print(\"Validation acc: %f\" % acc_val)\n",
    "    return cost_trains, acc_trains, cost_vals, acc_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement cross validation to find an optimal value of $\\lambda$. The optimal hyper-parameters should be determined by the validation accuracy. The test set should only be used in the very end after all other processing, e.g. hyper-parameter choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda is 100.000000\n",
      "(784, 60000) (60000,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-cd4d70adec30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Prepare the training and validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_val_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trainval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_trainval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# For each lambda and K, we build a new model and train it from scratch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-f3433bde2dfc>\u001b[0m in \u001b[0;36mtrain_val_split\u001b[0;34m(X_trainval, Y_trainval, i, K)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#TODO: Implement\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trainval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_trainval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0melements\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_trainval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0msurplus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melements\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0msize_big\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0melements\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msurplus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Initialization\n",
    "\"\"\"\n",
    "# Use cross validation to choose the best lambda for the L2-regularization from the list below\n",
    "lambda_list = [100, 1, 0.1]\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, config.img_h * config.img_w])\n",
    "Y_gt = tf.placeholder(tf.int64, [None, ])\n",
    "\n",
    "for lambd in lambda_list:\n",
    "    val_loss_list = []\n",
    "    config.lambd = lambd\n",
    "    print(\"lambda is %f\" % lambd)\n",
    "    \n",
    "    for i in range(config.K):\n",
    "        # Prepare the training and validation data\n",
    "        X_train, X_val, Y_train, Y_val = train_val_split(X_trainval, Y_trainval, i, config.K)\n",
    "        \n",
    "        # For each lambda and K, we build a new model and train it from scratch\n",
    "        model = logistic_regression(X, Y_gt, config, name=str(lambd)+'_'+str(config.K))\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            # Initialize the variables of the model\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            # Train the model\n",
    "            cost_trains, acc_trains, cost_vals, acc_vals = train(model, X_train, X_val, Y_train, Y_val, config)\n",
    "            \n",
    "        val_loss_list.append(cost_vals[-1])\n",
    "        \n",
    "    print(\"The validation loss for lambda %f is %f\" % (lambd, np.mean(val_loss_list)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Combine Train and Validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the hyper-parameters you choose from the cross validation to re-train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.lambd =  #TODO: Choose the best lambda\n",
    "model = logistic_regression(X, Y_gt, config, name='trainval')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cost_trains, acc_trains, cost_tests, acc_tests = train(model, X_trainval, X_test, Y_trainval, Y_test, config)\n",
    "\n",
    "print(\"The final test acc is %f\" % acc_tests[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the impact of k in k-fold cross validation?\n",
    "\n",
    "2. What will happen to the training if you change the $\\lambda$ for L2-regularization?\n",
    "\n",
    "3. Why do we perform the gradient descent on a batch of the data rather than all of the data?\n",
    "\n",
    "4. Why does the loss increase, when the learning rate is too large?\n",
    "\n",
    "5. Do we apply L2-regularization for the bias $b$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting to know Back-Propagation in details $~$ (18 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise you would build a **feed-forward network** from scratch using **only** Numpy. For this, you also have to implement **Back-propagation** in python. Additionally, this network should have the option of **L2 regularization** enabled within it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before you start**: In this exercise you will implement a single hidden layer feedforward neural network. In case you are unfamiliar with the terminology and notation used here, please consult chapter 6 of the Deep Learning Book before you proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, a feedword neural network with a single hidden layer can be represented by the following function $$ f(x;\\theta) = f^{(2)}(f^{(1)}(f^{(0)}(x)))$$ where $f^{(0)}(x)$ is the input layer, $f^{(1)}(.)$ is the so called hidden layer, and $f^{(2)}(.)$ is the ouput layer of the network. $\\theta$ represents the parameters of the network whose values will be learned during the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network that you will implement in this exercise has the following layers:\n",
    "* $f^{(0)}(x) = \\mathbf{X}$, with $\\mathbf{X} \\in \\mathbb{R}^{b,p}$ where $b$ is the batch size and $p$ is the number of features.\n",
    "* $f^{(1)}(.) = \\sigma(\\mathbf{X} \\mathbf{W_1}+b_1)$, with $\\mathbf{X} \\in \\mathbb{R}^{b, p}$, $\\mathbf{W_1} \\in \\mathbb{R}^{p,u_1}$, $\\textbf{b}_1 \\in \\mathbb{R}^{u_1}$ where $u_1$ is the number of **hidden units**. Additonally, $\\sigma(x) = \\frac{1}{1 + \\exp{(-x})}$ is the **sigmoid** function.\n",
    "* $f^{(2)}(.) = softmax(\\mathbf{X} \\mathbf{W_2}+b_2)$, with $\\mathbf{X} \\in \\mathbb{R}^{b, u_1}$, $\\mathbf{W_2} \\in \\mathbb{R}^{u_1,u_2}$, $\\textbf{b}_2 \\in \\mathbb{R}^{u_2}$ where $u_2$ is the number of **output classes** in this particular layer.\n",
    "\n",
    "Note that both, $\\sigma(x)$ are applied **elementwise**. Further, the addition with the bias vector is also applied **elementwise** to each row of the matrix $\\mathbf{X} \\mathbf{W}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "import math\n",
    "from scipy.special import expit as sigmoid1\n",
    "from scipy.stats import logistic\n",
    "\n",
    "class Fully_connected_Neural_Network(object):\n",
    "    \"\"\" Fully-connected neural network with one hidden layer.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_output : int\n",
    "        Number of class labels.\n",
    "        \n",
    "    n_features : int\n",
    "        Number of input features.\n",
    "        \n",
    "    n_hidden : int\n",
    "        Number of hidden units.\n",
    "        \n",
    "    l2 : float\n",
    "        regularization parameter\n",
    "        0 means no regularization\n",
    "        \n",
    "    epochs : int\n",
    "        One Epoch is when the entire dataset is passed forward and backward through the neural network only once.\n",
    "        \n",
    "    lr : float\n",
    "        Learning rate.\n",
    "        \n",
    "    batchsize : int\n",
    "        Total number of training examples present in a single batch.\n",
    "        \n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w1 : array, shape = [n_features, n_hidden_units]\n",
    "        Weight matrix for input layer -> hidden layer.\n",
    "    w2 : array, shape = [n_hidden_units, n_output_units]\n",
    "        Weight matrix for hidden layer -> output layer.\n",
    "    b1 : array, shape = [n_hidden_units, ]\n",
    "        Bias for input layer-> hidden layer.\n",
    "    b2 : array, shape = [n_output_units, ]\n",
    "        Bias for hidden layer -> output layer.\n",
    "\n",
    "    \"\"\"\n",
    "    # Points: 2.0\n",
    "    def __init__(self, n_output, n_features, n_hidden=30,\n",
    "                 l2=0.0, epochs=50, lr=0.001, batchsize=1):\n",
    "        self.n_output = n_output\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.batchsize = batchsize\n",
    "        #TODO Initialize weights and biases with np.random.uniform or np.random.normal and specify the shape\n",
    "        w1 = np.random.uniform(low = 0, high = 11, size= self.n_features*self.n_hidden)\n",
    "        self.w1 = w1.reshape(self.n_features, self.n_hidden)\n",
    "        w2 = np.random.uniform(low = 0, high = 11, size= self.n_hidden*self.n_output)\n",
    "        self.w2 = w2.reshape(self.n_hidden, self.n_output)\n",
    "        \n",
    "        self.cost_iter = np.zeros([epochs * batchsize])\n",
    "        #bias mai dimension hidden aur output ka hoga \n",
    "        self.b1 = np.random.uniform(low = 0, high = 1, size = self.n_hidden)\n",
    "        self.b2 = np.random.uniform(low = 0, high = 1, size = self.n_output)\n",
    "\n",
    "        \n",
    "    def labelsEncoding(self, y, k):\n",
    "\n",
    "        #res = np.zeros([k])\n",
    "        #res[y] = 1\n",
    "        res = np.zeros([y.shape[0],k])\n",
    "        for i in range(y.shape[0]):\n",
    "            res[i,y[i]] = 1 \n",
    "        return res\n",
    "        \n",
    "        \n",
    "    # Points: 0.5\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Compute sigmoid function\"\"\"\n",
    "        return  sigmoid1(z)\n",
    "        #TODO Implement\n",
    "      \n",
    "\n",
    "    # Points: 0.5\n",
    "    def sigmoid_gradient(self, z):\n",
    "        \"\"\"Compute gradient of the sigmoid function\"\"\"\n",
    "        return logistic.pdf(z)\n",
    "\n",
    "    \n",
    "    # Points: 1.0\n",
    "    def softmax(self, z):\n",
    "        \"\"\"Compute softmax function.\n",
    "        Implement a stable version which \n",
    "        takes care of overflow and underflow.\n",
    "        \"\"\"        \n",
    "        #TODO Implement\n",
    "        numerator = np.exp(z - np.amax(z))\n",
    "        denominator = np.sum(numerator)\n",
    "        softmax_output = np.exp(z - np.amax(z))/denominator \n",
    "        return softmax_output\n",
    "    \n",
    "    def softmax_computation(self, x):\n",
    "        \n",
    "        #TODO Implement\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        z = e_x / e_x.sum(axis=0)\n",
    "        y = np.zeros(z.shape)\n",
    "        for i in range(z.shape[0]):\n",
    "            y[i][np.argmax(z[i])] = 1\n",
    "            \n",
    "        return y \n",
    "\n",
    "        \n",
    "    # Points: 2.0\n",
    "    def forward(self, X):\n",
    "        \"\"\"Compute feedforward step\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        z2 : array,\n",
    "            Input of the hidden layer.\n",
    "        a2 : array,\n",
    "            Output of the hidden layer.\n",
    "        z3 : array,\n",
    "            Input of the output layer.\n",
    "        a3 : array,\n",
    "            Output of the output layer.\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO Implement\n",
    "        z2 = np.dot(X, self.w1) + self.b1        \n",
    "        \n",
    "        a2 = self.sigmoid(z2)        \n",
    "        \n",
    "        z3 = np.dot(a2, self.w2) + self.b2      \n",
    "        \n",
    "        a3 = self.softmax(z3)\n",
    "        \n",
    "        # Checking other method for softmax calculation\n",
    "        temp = self.softmax_computation(z3)\n",
    "        \n",
    "        return z2, a2, z3, a3\n",
    "        \n",
    "    # Points: 0.5\n",
    "    def L2_regularization(self, lambd):\n",
    "        \"\"\"Implement L2-regularization loss\"\"\"\n",
    "        \n",
    "        x = lambd * np.sum(np.square(self.w1))\n",
    "        y = lambd * np.sum(np.square(self.w2))\n",
    "        \n",
    "        return x, y\n",
    "        \n",
    "    # Points: 2.0\n",
    "    def loss(self, y_enc, output, epsilon=1e-12):\n",
    "        \"\"\"Implement total loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, one-hot encoded labels.\n",
    "        \n",
    "        output : array, output of the output layer\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float, total loss.\n",
    "\n",
    "        \"\"\"\n",
    "        #TODO Implement        \n",
    "        regul21, regul22 = self.L2_regularization(self.l2)\n",
    "        regul1 = regul21\n",
    "        regul2 = regul22\n",
    "        sub = np.sum(np.square(np.subtract(y_enc, output)))\n",
    "        cost =  sub + regul1 + regul2\n",
    "        return cost\n",
    "        \n",
    "    \n",
    "    # Points: 4.0\n",
    "    def compute_gradient(self, X, a2, a3, z2, z3, y_enc):\n",
    "        \"\"\" Compute gradient using backpropagation.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        X : array, Input.\n",
    "        a2 : array, output of the hidden layer.\n",
    "        a3 : array, output of the output layer.\n",
    "        z2 : array, input of the hidden layer.\n",
    "        y_enc : array, one-hot encoded labels.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        grad1 : array, Gradient of the weight matrix w1.\n",
    "        grad2 : array, Gradient of the weight matrix w2.\n",
    "        grad3 : array, Gradient of the bias vector b1.\n",
    "        grad4 : array, Gradient of the bias vector b2.\n",
    "        \"\"\"\n",
    "        #TODO Implement\n",
    "        dz3   = np.subtract(a3, y_enc)\n",
    "        \n",
    "        \n",
    "        ##Compute Gradient of Softmax\n",
    "        e_x = np.exp(np.subtract(z3, np.max(z3)))\n",
    "        # print(e_x)\n",
    "        h2_ = (e_x / np.sum(e_x, axis=0)) + np.square((e_x)/(np.sum(e_x,axis=0)))\n",
    "\n",
    "        delta2 = np.multiply(h2_, np.subtract(y_enc, a3))\n",
    "        \n",
    "        h1_ = self.sigmoid_gradient(z2)\n",
    "        delta1 = np.multiply(np.matmul(self.w2, np.transpose(delta2)), np.transpose(h1_))\n",
    "        \n",
    "        grad1 = np.matmul(np.transpose(X), np.transpose(delta1))\n",
    "        grad2 = np.matmul(np.transpose(a2), delta2)\n",
    "        \n",
    "        # regularize\n",
    "        regul21 = 2 * self.l2 * self.w1\n",
    "        regul22 = 2 * self.l2 * self.w2\n",
    "        \n",
    "        res1 = np.add (grad1, regul21)\n",
    "        res2 = np.add (grad2, regul22)\n",
    "\n",
    "        grad3 = self.b1\n",
    "        grad4 = self.b2\n",
    "        \n",
    "        #res1 is the Gradient of the weight matrix w1 with L2 reg\n",
    "        #res2 is the Gradient of the weight matrix w2 with L2 reg\n",
    "        return res1, res2, grad3, grad4 \n",
    "        \n",
    "    # Points: 1.0\n",
    "    def inference(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, Predicted labels.\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO Implement\n",
    "        z2 = np.dot(X, self.w1)         \n",
    "        \n",
    "        a2 = self.sigmoid(z2)        \n",
    "        \n",
    "        z3 = np.dot(a2, self.w2)       \n",
    "        \n",
    "        a3 = self.softmax(z3)       \n",
    "                \n",
    "        y_pred = a3 > 0.5\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def shuffle_train_data(self, X, Y):\n",
    "        \"\"\"called after each epoch\"\"\"\n",
    "        perm = np.random.permutation(Y.shape[0])\n",
    "        X_shuf = X[perm]\n",
    "        Y_shuf = Y[perm]\n",
    "        return X_shuf, Y_shuf\n",
    "    \n",
    "    # Points: 2.0\n",
    "    def train(self, X_train, Y_train, verbose=False):\n",
    "        \"\"\" Fit the model.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input.\n",
    "        y : array, Ground truth class labels.\n",
    "        verbose : bool, Print the training progress\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        #TODO Initialization\n",
    "        self.cost_ = []\n",
    "        \n",
    "\n",
    "        for e in range(self.epochs):\n",
    "            batch_size = math.floor(X_train.shape[0]/self.batchsize)\n",
    "            acc_grad1 = np.zeros(self.w1.shape)\n",
    "            acc_grad2 = np.zeros(self.w2.shape)\n",
    "            Y_hot = self.labelsEncoding(Y_train, 10)\n",
    "            \n",
    "            for i in range(self.batchsize):\n",
    "                X_batch = X_train[i * batch_size:(i+1) * batch_size - 1]\n",
    "                Y_batch = Y_hot[i*batch_size:(i+1)*batch_size-1]\n",
    "                \n",
    "                # feedforward\n",
    "                z2, a2, z3, a3 = self.forward(X_batch)\n",
    "                grad1, grad2, grad3, grad4 = self.compute_gradient(X_batch,a2, a3 ,z2,z3, Y_batch)\n",
    "                acc_grad1 = np.add(acc_grad1, grad1)\n",
    "                acc_grad2 = np.add(acc_grad2, grad2)\n",
    "                \n",
    "                _,_,_, output1 = self.forward(X_batch)\n",
    "                self.cost_iter[(e * self.batchsize + i)] = self.loss(Y_batch, output1)\n",
    "        \n",
    "            if verbose:\n",
    "                print('\\nEpoch: %d/%d' % (e+1, self.epochs))\n",
    "\n",
    "\n",
    "                # feedforward and loss computation\n",
    "                _,_,_, output= self.forward(X_train)\n",
    "\n",
    "                #self.cost_.append(cost)\n",
    "                                \n",
    "                self.cost_.append(self.loss(Y_hot, output))\n",
    "                self.w1 = np.subtract(self.w1, np.multiply(self.lr, acc_grad1))\n",
    "                self.w2 = np.subtract(self.w2, np.multiply(self.lr, acc_grad2))\n",
    "\n",
    "                # compute gradient via backpropagation and update the weights\n",
    "                \n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $15.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Fully_connected_Neural_Network(n_output=10, \n",
    "                                    n_features= X_trainval.shape[1], \n",
    "                                    n_hidden=50, \n",
    "                                    l2=0.1, \n",
    "                                    epochs=100, \n",
    "                                    lr=0.001,\n",
    "                                    batchsize=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/5\n",
      "\n",
      "Epoch: 2/5\n",
      "\n",
      "Epoch: 3/5\n",
      "\n",
      "Epoch: 4/5\n",
      "\n",
      "Epoch: 5/5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Fully_connected_Neural_Network at 0x211581db358>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.train(X_trainval, Y_trainval, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training error for every iteration\n",
    "# in every epoch\n",
    "\n",
    "# TODO Implement\n",
    "x_axis = np.arange(nn.cost_iter.shape[0])\n",
    "plt.plot(x_axis, nn.cost_iter)\n",
    "# # # Name the plot\n",
    "# # # TODO Implement\n",
    "plt.title(\"training error for every iteration in every epoch\")\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('costs')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzt3Xd8FNX6x/FPIFTpRAUpUi4WrCAo/i5ee0OlCEqvShGlKaFYQa8loSmgUqR3pOtVsYFdICiCqGiwgSKCKKAgCnl+f5yTy5KbskA2s0m+79frvDJz5szsswObJ2fm7JkYM0NERCTaFAg6ABERkfQoQYmISFRSghIRkaikBCUiIlFJCUpERKKSEpSIiEQlJSjJDcYBD0SgbW5UDHgB2A08H3Asec1U4N9BByGHxQYdgOR53wK3A68fxzF6RKhtbtQCOBkoDxwMOBaRiFIPSoKWF/9IiuF/P1vp1WUlvXNzKvAlx5acgjzXefHfWSJMCUoiaQZQFXdJ6ndgAFANMOA24HvgTd/2eeAn3KWrt4GzQo4zlcOXXi4DtgL3AD8D24DOx9i2vI9tD7DG7/duJu+nAfA+8BvwiT9+qpXAo8B7wD6gRgZ1pwDLgF1AMtA15BhDgAXATB9TpzSvPxR4EGiJO5+34T7D9wPf+fc4HSjt21cj/XOd1o3AOv++3gfO9fWDfDyhngJG++XSwCTcef0Bd/4K+m2d/Pse5d/rI/7nOSHHOgnYD5yYQVxdgM+BX4HluOScyoDewNfATmAYh3+fZXZOABpy+N9xC0ee57LAf4C9wCqgZgaxSU4wMxWVSJZvzeyqkPVq5kw3sxPMrJiv72JmJc2siJk9aWbrQvaZamb/9suXmdlBM3vYzAqZWSMz22dmZY+h7VxfiptZbTPbYmbvZvA+KpnZL/4YBczsar9+ot++0sy+N7OzzCzWv156dW+Z2TNmVtTMzjezHWZ2pT/GEDP728ya+tcoZv8bxxAzmxmy3sXMks2shpmVMLNFZjYji3MdWuqa2c9mdpGZFTSzjv7frIiZnerPVynftqCZbTOzBn59iZmN98c+ycxWm1l3v62TP/e9/Hsv5t93Qshr9zGzFzI43039+zrT73+/mb0fst3MbIWZlTOzqmb2pZndHsY5qWpme82stf/3KO//HVL/7+wyswv9a84y9/8j6M9Qvi2BB6CS58u3ln6CqpHJPmV8m9J+faodmXT2m/sFktr+Zzv8SzPctgXNJYPTQ7b92zJOUAPt8C+51LLc3C90zCWjh9NsT1tXxcwOmUvEqXWP+5gxl3zezuJ8DrEjE9QbZtYzZP10/75iwzzXz5rZI2nqNpnZpX75XTPr4JevNrPNfvlkMztgRya91uaSBuYS1PdpjnuRuT8CCvj1JDO7NYO4Xjaz20LWC5hLlqf6dTOz60K29/TnIqtzMtjMFmfwmlPN7LmQ9UZm9kUW/x4qESy6xCdB2RKyXBB4AtiMu7T1ra+Py2DfXzjyHsw+oMRRtj0Rd18kNI7Q5bROBW7BXRZKLQ2BilnsH1p3Cu4y196Quu+ASmHGkJ5T/DFCjxeLG0gRzjFPxV0CDX1fVfxxAWYDrf1yG7+eul8h3OW91P3G4y7bZfS6q4A/gEuBM4B/4C53ZhTXUyHH3oW7j5fRufouJObMzkkV3P+zjPwUspzZ/yvJAbpxKZGW0XT5ofVtgCbAVbjkVBp33yEmgnHtwCWuyrhBB+B+eWVkC+6eWtdM2qT3XkPrfgTKASU5nKSq4u7fZHaMzPzIkfdmquLe13bce8vqmFtw98kezWD788AIf6xmwMUh+x3A/RGR0YCN9F53GtAOlwgWAH9mEdesTGKvAmz0y1Vx5wIyPydbgAszOaZEEfWgJNK24wYHZKYk7pfdL0Bx4LFIBwUcAhbhBiYUx/1F3yGT9jOBm4BrcT2+orhBEpUz2SetLbib84/7/c/FDWDI7JdwVuYA/YDquL/2HwPmEf4ov4m4ofkX4f4gOAG4AfdvAi6RrwSmAN/gBi2A6zm9iktepXC/S2riekeZmYFLdO1wgxcyMg4YzOHBMqVxPdhQ8bhBDVWAPrj3DZmfk1m4P4Ruxf2BXh44P4uYJSBKUBJpj+NGVP0G9M+gzXTcZZgfgM+AD3MmNO7C/eL7CfeLcw4uUaZnC66Xdy/ul/YW3C/Io/0MtcaNrvsRWAw8BLx2lMcINRkX+9u4BPIn0Oso9k/C9QrH4nqtyfzv6MHZuF/qs9PUdwAK4/7NfsX1iCqSua3AR7je1TuZtFsMJABzcZd9PwWuT9NmKbAWNwLxP7gRhZD5OfkeaIS7rLnL73teFjFLQGLM9MBCES8BqAB0DDqQPG4yLkHffxzHMKAWLqFKHqV7UJKfnYHrAWwA6uMut90eaER5XzXgZqBOwHFILqBLfJKflcTdh/oDmI+7n7I00Ijytkdwl+qG4S69iWRKl/hERCQqqQclIiJRSfegvB07dth3332XdUMRETku9erV20nGczD+lxKU991331G/fv2gwxARyfPMLKzegC7xiYhIVFKCEhGRqKQEJSIiUUkJSkREopISlIiIRCUlKBERiUpKUCIiEpWUoLLBuVdfzknVT826oYiIhE0J6jgVjI2lcXxv7lkwnSu7dqRAbMGgQxIRyROUoI7ToYMHebJ1Fz59820a9e5B3zmTqXTmaUGHJSKS6ylBZYPff/mVGfEPMKXPQEqWL0ef2ZO4oe8dxBYpEnRoIiK5lhJUNvr0zbdJbNqGpKUvccVtHei/YDo1Ljg/6LBERHIlJahstn/PXuYPeZxxXXtTIDaWO6c+y8339afICcWDDk1EJFdRgoqQrz5cw/Cb2/LWjLlcfGsz4hfP4oyGDYIOS0Qk11CCiqC/9v/JssSnGNuhOwf27afrs6No/eiDFC9dKujQRESiXiQTVBVgBfA5sBHo4+uHAV8A64HFQJmQfQYDycAm4NqQ+ut8XTIwKKS+OrAK+AqYBxT29UX8erLfXi173tKx+e6TTxl5S0deHTeZOtdfzYClczjv2iuDDElEJPqZWaRKRTOr65dLmtmXZlbbzK4xs1hfn+ALftsnZlbEzKqb2WYzK+jLZjOrYWaFfZvafp/5ZtbKL48zszv8ck+/jt8+L6t416xZY0DES8XTalrfuZNtxIYPrNOTT1ipE+Ny5HVVVFRUoqWYWVI4eSSSPahtwEd+eS+uJ1UJeBU46Os/BCr75SbAXOAA8A2u93OhL8nA18Bfvk0TIAa4Aljg958GNA051jS/vAC40rcP3LYvNzO6bVdeGDGWM/7ZgAFLZnNhs5uCDktEJOrk1D2oakAd3OW2UF2Al/1yJWBLyLatvi6j+vLAbxxOdqn1aY91ENjt20eFlEOHWDl1FsObt+PHL5Np+fC9dJ84mnKVTwk6NBGRqJETCaoEsBDoC+wJqb8Plzxm+fX0ejh2DPWZHSutbkASkBQXF5fO5sja+f1Wnu1yJwseTqTq2bXpv3Aml7RrSUwBjV0REYn0b8JCuOQ0C1gUUt8RuBFoy+HEsRU3sCJVZeDHTOp34gZYxKapT3usWKA0sCud+CYA9YB6O3fuPLp3lk3MjA+eX8ywpm3YnPQRTQf2pdf08Zxcs3og8YiIRItIJqgYYBLu3tPIkPrrgIFAY2BfSP0yoBVuBF51oBawGljjl6vjRum18m0NN0qwhd+/I7A05Fgd/XIL4E3S70FFjd+2/8ykO/sza9BDxFWtzN3PT+Oq7p0pGBub9c4iInlRBEfxNTRnvZmt86WRmSWb2ZaQunEh+9znR+xtMrPrQ+ob+VGAm32b1PoaZrbaH/N5PwIQMyvq15P99hrRMoovnFKiXFlrlzDURmz4wO5ZOMOqnHVm4DGpqKioZFcJdxRfjJkhkJSUZPXr1w86jCOcdVlDmt8/gJJx5Xhr2hyWP/scf/95IOiwRESOi5mtxd1eyZTuxkexjSvfJbFZG1YtfoHLu7TjngUzqFGvTtBhiYjkCCWoKPfn3t9ZMDSBZ2+7i5gCMdw55Rma3x+vyWdFJM9TgsolklevZfjN7Vg5bTYNWjRhwJLZnHnJ/wUdlohIxChB5SJ//3mAF4aPYUz7buzf+zu3PzOCNo8/xAllSgcdmohItlOCyoW+3/AZo27txPJnnuO8a69kwNI5nH/dVUGHJSKSrZSgcqlDBw/y6rOTGNWyM7t+2Eb7YY/QZXQipU46MejQRESyhRJULvfTV5sZ3a4ry4aNplaD+gxYMpuLmjcOOiwRkeOmBJUHWEoKb02fw/Dm7fnh803cOmQwPZ4bQ/nKlbLeWUQkSilB5SG/bNnKuNt78fzQJ6hc+wz6L5rJvzq00uSzIpIr6TdXHmNmfLhgKcOateGrVUk0ie9DrxkTqPCPGkGHJiJyVJSg8qjd23cwuVc8M+IfoHzlU+g3fyrX9OiiyWdFJNdQgsrj1r3yOolNWrP+1Te59s6u9Js/lSpn1w46LBGRLClB5QN//LabWYOG8Nyd/SlWqiS9Z07gpnt6UahokaBDExHJkBJUPvL52+8xrGkbPly4jMs6taH/opnUrF836LBERNKlBJXP/Pn7Hyx8JJFnOvcEg56Tn6bFgwMpWuKEoEMTETmCElQ+tTnpY4Y3b8eKKbO46OabGLBkDrUvbRh0WCIi/6UElY/9/ecBXhw5ltFtu/LH7t3cNnYY7RKGckLZMkGHJiKiBCWwZePnPNmyMy+PncA5V1/OwKVzqNPomqDDEpF8TglKADf57OvjpzDylo7s/H4r7RKGctvY4ZQ5+aSgQxORfEoJSo6wffM3jOnQnSUJT1Kzfl3il8ymwS1NiYmJCTo0EclnlKDkf1hKCu/MnMfw5u34/tPPuOXBgfSYNJa4qpWDDk1E8hElKMnQrq0/Mr5rb+Y9+BiVTq9F/4UzuaxjGwoULBh0aCKSDyhBSZZWL36BxKZt2PT+h9zUvxe9Zk6g4mk1gw5LRPK4SCaoKsAK4HNgI9DH19/i11OAeiHtCwHTgA1+n8Eh264DNgHJwKCQ+urAKuArYB5Q2NcX8evJfnu17HlL+deeHTuZ0mcQ0/vfT9mKFeg3dyrX3tmVgoUKBR2aiORVZhapUtHM6vrlkmb2pZnVNrMzzex0M1tpZvVC2rcxs7l+ubiZfWtm1cysoJltNrMaZlbYzD7xx8HM5ptZK788zszu8Ms9/Tp++7ys4l2zZo0BKmGU4qVLWevHHrQRGz6w+MWzrOq5ZwUek4qKSu4pZpYUTh6JZA9qG/CRX96L6xVV8j83pdPegBOAWKAY8BewB7gQ1xP62tfNBZoAMcAVwAK//zSgqV9u4tfx26/07SUb7Nu9hzn3PszEnndTtMQJ9JoxgcYD+lC4WNGgQxORPCSn7kFVA+rgLrdlZAHwBy6xfQ8MB3bhktqWkHZbfV154DfgYJp60uxzENjt20s2+uKdD0hs2oYP5i/m0vat6L9oJrUuqpf1jiIiYciJBFUCWAj0xfWIMnIhcAg4BXdv6R6gBun3fCyTerLYFqobkAQkxcXFZRKaZOTAH/tY9Ohwnu50BykHD9HjuTHcOmQwRUuWCDo0EcnlIp2gCuGS0yxgURZt2wCvAH8DPwPv4QZRbMUNuEhVGfgR2AmUwV0SDK0nzT6xQGlcbyytCf416u3cuTPc9yTp+HrtOoa36MCbk6ZTr0kjBiyZzVmXXxJ0WCKSi0UyQcUAk3D3nEaG0f573D2lGNy9qAbAF8AaoBauV1UYaAUsw/WIVgAt/P4dgaV+eZlfx29/k/R7UJKNDh44wH+efJbRbW/n912/0mV0Iu2HPUKJ8mWDDk1EcqMIjuJraM56M1vnSyMza2ZmW83sgJltN7Plvn0JM3vezDaa2WdmFh9yrEZ+FOBmM7svpL6Gma02s2S/bxFfX9SvJ/vtNTSKL2dLgdiCdmXXjpaw9i17+J1XrO6N1wYek4qKSnSUcEfxxZgZAklJSVa/fv2gw8hzTqp+Ki0fvo9q55/D5++8z4KHE/ntp+1BhyUiATKztRz5Pdh0aSYJiaifv/mOsR17sPjxkdS4oA7xS2bxfy1v1uSzIpIlJSiJOEtJ4d3ZzzP85rZ8v34jze+P544pTxN3apWsdxaRfEsJSnLMrh+2Mb5bH+Y+8G8q1qpJ/4UzuLxLO00+KyLpUoKSHLdmyX9IbNKGz9/5gBv73Unv2c9R6czTgg5LRKKMEpQEYu/OX5jWbzDT7r6X0ifG0Wf2JG68+y4KFS0SdGgiEiWUoCRQ619bQWLTNqxe8iKXd25L/OJZnHaxRlOKiBKURIH9e/ayYGgCT3fuyaG/D9J9wmhaP/ogJ5QpHXRoIhIgJSiJGl8nfcyIFh14bfwU6lx/NQOWzqHujdcGHZaIBEQJSqLKwb/+4pWxExh5a0d2btlK28eH0PXZUZSrVDHo0EQkhylBSVT6KflrxnbowaLHRlCtzjn0XzSLSzu01pB0kXxECUqilqWk8N6cBQxr0obkVUk0ju+tIeki+YgSlES937b/zOTeA5h2z32UPulEDUkXySeUoCTXWP/qmyQ2ac2aJf/RkHSRfEAJSnKV/Xv28vzQJ44Ykt7q3w9oSLpIHqQEJbnSf4ekT5hC3UbXuCHpN1wTdFgiko2UoCTXOvjXX7wyZgIjW3bily0/0PaJoXR9dhRlT6kQdGgikg2UoCTX++mrzYzp0J3Fj7sh6fGLZ/OvDq00JF0kl1OCkjzBPXPKD0lfvZYm8X3oPWsip5xeK+jQROQYKUFJnvLb9p+Z3CveDUk/+ST6zp3MDf16aki6SC6kBCV5UuiQ9Cu6tKf/opnUaqAh6SK5iRKU5FmpQ9Kf6dwTO5RCj4luSHrx0qWCDk1EwqAEJXne5qSPGd68/X+HpA9cNldD0kVyASUoyRfSG5J++7MjNSRdJIopQUm+cnhI+khq1D1PQ9JFolgkE1QVYAXwObAR6OPrb/HrKUC9NPucC3zgt28Aivr6C/x6MjAaiPH15YDXgK/8z7K+Psa3SwbWA3Wz721JbueGpD9Pooaki0Q3M4tUqWhmdf1ySTP70sxqm9mZZna6ma00s3oh7WPNbL2ZnefXy5tZQb+82swuNrMYM3vZzK739YlmNsgvDzKzBL/cyLeLMbMGZrYqq3jXrFljgEo+LOdde6U9tOJFS/z4HbuhX08rVLRI4DGpqOTlYmZJ4eSRSPagtgEf+eW9uJ5UJf9zUzrtr8H1dj7x678Ah4CKQClcz8qA6UBT36YJMM0vT0tTP923/xAo448j8j8+Wf4GiU1ak7T0JTckfaGGpItEg5y6B1UNqAOsyqTNabiEshyX2Ab4+krA1pB2W30dwMm4RIj/eVLIPlsy2CdUNyAJSIqLiwvjbUhetX/PXuYPedwNSU9JHZJ+v4akiwQoJxJUCWAh0BfYk0m7WKAh0Nb/bAZcyeH7TaEsi9cMd58JuPtg9Xbu3JnFISU/2Jz0McNbdOD1CVOp2+haBiydQ51GGpIuEoRIJ6hCuOQ0C1iURdutwFvATmAf8BJucMNWoHJIu8rAj355O4cv3VUEfg45VpUM9hHJ1MEDB3h5zHhGterErh+20S5BQ9JFghDJBBUDTMLdcxoZRvvluFF8xXG9qUuBz3CX7vYCDfwxOwBL/T7LgI5+uWOa+g6+fQNgN4cvBYqEZduXmxnTvtuRQ9LbtyKmgL6dIZIjIjiKr6E5681snS+NzKyZmW01swNmtt3Mlofs087MNprZp36EXmp9PV+32czG+tF5qSP93jCzr/zPcr4+xsye9u03pBktqFF8KkddylQ42W4bO9xGbPjA+syZZBVP+0fgMamo5NYS7ii+GDNDICkpyerX18gtydz5115J08F3U7x0Kd6aNpvlz07m4IEDQYclkquY2Vr+93uw/0PXKkSOwrrlb5DQuDVJy17mits6EL9oJrUuyvJzJiLHQAlK5Cjt37OH+Q89xjNd7sTM6PHcGFo+cp+GpItkMyUokWO0ec1HDG/entcnTuOCG65zQ9KvvzrosETyDCUokeNw8MABXh49jlGtOvHrjz/RLvFhbn9mBGUraki6yPFSghLJBtu+3Mzodl1Z8sQoalxwPvFLZnFJu5Yaki5yHML99CTi5sMrBLyB+zJtu0gFJZIbWUoK78yaz7Cmbdmc9DFNB/al98yJVDztH0GHJpIrhZugrsFNU3QjbpaG04D4SAUlkpv9uu0nJt3Znxn976fsKRXoN28KjfrcQWyRIkGHJpKrhJugCvmfjYA5wK7IhCOSd4QOSb/y9g70XziDf1x4QdBhieQa4SaoF4AvcF+segM4EfgzUkGJ5BWhQ9IB7pg0lpYP30exUhqSLpKVo5lJoizuMt8h3Hx5pYCfIhRXjtNMEhJpsUWKcHX3zlzeqS379uxhyROjWPfK60GHJZLjwp1J4mgS1P/hnusUG1I3/agji1JKUJJTKp5Wk1uH3EvVc2rz2dvvsfCRYfz20/agwxLJMdmdoGYANYF1uB4UuEn/eh9rgNFGCUpyUkyBAjRs3YLre3cH4OXR43l3zgIsJSXgyEQiL7sT1OdAbVxSypOUoCQIZStW4Ob7+1P7X//k+w2fMX/I42z7MjnosEQiKrsni/0U0FfjRbLZf4ekxz/ghqTP1ZB0kVRZ9aBewPWaSgLnA6uB0GcLNI5caDlLPSgJWrFSpWjcvxcXNruRHd9tYcHDCSSvXht0WCLZLrsu8V2axf5vHU1Q0UwJSqLFPy68gFseGkRc1cqsXvwiy4aPYf+ePUGHJZJtsvseVHXcI9NTv/tUDDgZ+PYY44s6SlASTWKLFOGaHl24rFMb9u3WkHTJW7L7HtTzQOjwokO+TkQi4OCBA7z01LOMatmZX3/8ifbDHuG2scMpU+HkoEMTyTHhJqhY4K+Q9b+AwtkfjoiE2vZlspslPeFJatavw4Cls7mk7a2aJV3yhXD/l+/gyAERTXAzmotIhFlKCu/MnMewpm35eu06mg7qR68ZE6hQq2bQoYlEVLj3oGoCs4BKfn0L0B7YHKG4cpzuQUluUef6q2kysC/FS5VixdRZvDZ+CgcPHMh6R5EoEYmpjgBKADHA3mOMK2opQUluUrx0KW7q34sLm97Izi1bWfTv4Wx6f1XQYYmEJbsHSZQGRgIrgRXACF8nIgHYt3sP8x54lGc69yTl4CG6jX+StglDKVm+XNChiWSbcBPUZFyv6VZf9gBTstinCi6ZfQ5sBPr4+lv8egrpZ9CqwO9A/5C664BNQDIwKKS+OrAK+AqYx+GBG0X8erLfXi2LWEVypc1JHzO8eXteeXoi5151GQOWzaHBLU2JiYkJOjSR42dm4ZR1YdaFlopmVtcvlzSzL82stpmdaWanm9lKM6uXzn4Lzex5M+vv1wua2WYzq2Fmhc3sE38czGy+mbXyy+PM7A6/3NOv47fPy+o9rlmzxnCzZqio5MpyYrWq1uO5MTZiwwfWa8YEq1CrZuAxqaikV8wsKZzcE24Paj/QMGT9n74uM9uAj/zyXlxPqpL/uSmDfZoCX+N6WKkuxPWEvsYNb5+LG0UYA1wBLPDtpvn98dun+eUFwJW+vUietePb7xl3ey9mDx5KXNXK3D1vKo363EGhoprXT3KncBPUHcDTuJkjvgPGAt2P4nWqAXVwl9sycgIwEBiapr4SbtRgqq2+rjzwG3AwTX3afQ4Cu337tLoBSUBSXFxcGG9DJPqtffEVEhq3IukF96j5+MWzOKNhg6DDEjlq4SaodcB5wLnAObhksz7MfUsAC4G+uHtXGRkKjMLdfwqVXs/HMqnPbJ+0JuDug9XbuVNf65K8Y99u96j5pzvdwcG//qbrs6NoP+wRSsal93eaSHQKN0GVB0ZzeBTfU6TfI0mrEC45zQIWZdH2IiAR10vrC9wL3IXrGVUJaVcZ+BH3ReEyHH7Cb2o9afaJxY043BVGvCJ5ytdr1zGiRQdeHjOesy6/hIHL5nLxrc00iEJyhXAT1FzcbBLNgRZ+eV4W+8QAk3D3nEaG8RqX4C4FVgOeBB7DXUpcA9TCjdgrDLQCluF6RCt8PAAdgaV+eZlfx29/k/R7UCJ53qG//+b1CVMZfnM7tmz8nBYPDOCuGeOpeJpmopAoF+YovrXp1GU1CqOhOev9iL91ZtbIzJqZ2VYzO2Bm281seTr7DgkZxYff70s/mu++kPoaZrbazJL9yL8ivr6oX0/222toFJ+Kiit1b7jGhqz8jyV+/I7d2O9OK1ysaOAxqeSvEu4ovnBnkhiOG0ww36+3AM4CHgpn59xAM0lIflKsVClu7NeTBi2asOuHbSx8dBhfvPNB0GFJPmHZPNXRXqA4hx+5URD4I/W1gFLHEGNUUYKS/Kh6nXNp8eBAKvyjBuuWv8HShCfZs0MDhiSywk1QRzPVUSfgEdzAh2rAVbhHwef65CSSX33z8XpG3tKRl54ax1mXNmTA0jn8s1VzPc5DokK4/wufBhoArf36XtwABhHJ5Q4dPMgbz01jWLO2fL/+U26+rz+9Z07klNNrBR2a5HPhJqiLgDs5/Mj3X9EDC0XylF+2/sCEHv2YOeBByp5Sgb5zJ3PTPb0oXKxY0KFJPhVugvobd98p9YbViRz5CHgRySM+fvk1Ehq3YvXiF7msUxvil8yi9qUNs95RJJuFm6BGA4uBk4BHgXdx31MSkTxo/569LHg4gTHtu3Pgj33cNnYYHUc+RqmTTgw6NMlHjuaBhWdweNLVN3BfwM0zNIpPJH0FYgtyaYfWXNPjNlIOHeLlMeN4b+4iLEUXUeTYZPcw8zxPCUokc+Uqn0Lz++I5o2EDvv/0MxYMTeCHL74MOizJhbJ7mLmI5HO7tv7IxDv6MaP//ZSpcDJ9506mcXxvDaKQiFGCEpGjsm75GyQ0bsWHC5ZySbuWDFg6m7MuvyTosCQPUoISkaP2597fWfjvYYzt0J39e3+ny+hEOj35BGVOPino0CQPUYISkWP23SefMqplJ14cOZbT/+8i4pfO5pJ2LSlQsGDQoUkeoAQlIscl5eAhVkyZxbBmbfh67TqaDuxL79nPUbn26UGHJrmcEpSIZItdP2xj0p39mXb3vZSKK0+f2ZNoMqAvRYoXDzo0yaWUoESD7Qm8AAAViUlEQVQkW61/bQWJTVrz/vzFNGx7CwOWzeHsK/4VdFiSCylBiUi2+/P3P1j82AjGtOvKvt920/mpBDqPTqBMhZODDk1yESUoEYmY7zd8xqhWnXlh+BhqXVSfAUtn868OrTSIQsKiBCUiEZVy8BArp81mWNM2bF7zMU3i+9B3zmSqnHVm0KFJlFOCEpEc8eu2n5h0V3+m9htMiXJl6T37OZoNvpsiJ2gQhaRPCUpEctSG11eS0KQV781ZwP+1as7ApXM556rLgg5LopASlIjkuAN/7GPJE6MY3eZ29v6yi06jHqfLmGGUrVgh6NAkiihBiUhgtmz8nKfa3MbSxKf4x4V1iV8ym8s6tqFArAZRiBKUiAQs5dAh3p4xl8QmbUhelcRN/XvRd85kqp5TO+jQJGCRTFBVgBW4BxtuBPr4+lv8egpHPg/kamAtsMH/vCJk2wW+Phn3dN8YX18OeA34yv8s6+tjfLtkYD1QN/velohEwm8/bWdy7wFM6TOIE8qWodfMidx8X3+Kljgh6NAkKGYWqVLRzOr65ZJm9qWZ1TazM83sdDNbaWb1QtrXMbNT/PLZZvZDyLbVZnaxmcWY2ctmdr2vTzSzQX55kJkl+OVGvl2MmTUws1VZxbtmzRoDVFRUoqAUKV7cmgzoa8PWvWsPvrHMzr3misBjUsm+YmZJ4eSRSPagtgEf+eW9uJ5UJf9zUzrtPwZ+9MsbgaJAEaAiUAr4APfmpgNNfbsmwDS/PC1N/XTf/kOgjD+OiOQCB/btY2nikzzV5nb27PyFjiMe5banh1P2FA2iyE9y6h5UNaAOsCrM9s1xCesALqltDdm21dcBnIxLhPifqQ+jqQRsyWCfUN2AJCApLi4uzNBEJKds/ewLRre5nSVPjKLGBeczYMkcLu/cVoMo8omcSFAlgIVAX2BPGO3PAhKA7n49Jp02lsUxwt1nAu4+WL2dO3eGEZqI5LSUQ4d4Z9Z8Epu05ov3PuTGu++i37ypnHre2UGHJhEW6QRVCJecZgGLwmhfGVgMdAA2+7qtvj60TeqlwO0cvnRXEfg5ZJ8qGewjIrnQ7u07mNZvMJN7D6B4qZLcNX08ze+Pp2jJEkGHJhESyQQVA0zC3XMaGUb7MsB/gMHAeyH123D3sBr4Y3YAlvpty4COfrljmvoOvn0DYDeHLwWKSC62ccU7JDZpwzsz59GgRRMGLpvL+ddeGXRYEgkRHMXX0Jz1ZrbOl0Zm1szMtprZATPbbmbLffv7zeyPkLbrzOwkv62emX1qZpvNbKwfnYeZlTezN8zsK/+znK+PMbOnffsNaUYLahSfikoeKZXOPM36zJlkIzZ8YF2fHWXlKp8SeEwqWZdwR/HFmBkCSUlJVr9+/aDDEJGjFFOgAP9sdTPX9+pBwdhYXh03iZXTZpNy8FDQoUkGzGwtR34PNl2aSUJEcjVLSeHd2QtIbNqaz995nxv69uTu+dOodt45QYcmx0kJSkTyhN3bdzDt7nuZdFc8RU4oTq+ZE2jx4ECKlSoZdGhyjJSgRCRP+eytdxnWtC0rpsziwmY3MnDZXOpcf3XQYckxUIISkTznr/37eXHkWJ5s1YVdP2yjXeLDdBv/JOUrp/d9fYlWSlAikmf9uOkrxrTvxqJHh3PquWcTv3gWV97ekYKxsUGHJmFQghKRPM1SUnhv7kISGrdi41vv0qhPD+5+fhrV654XdGiSBSUoEckX9uzYyYz+9/Ncz3soXKwYd00bR8uH7+OEsmWCDk0yoAQlIvnK5++8z7BmbXjjuelccON1DFw2l4uaNyYmJr0pPCVISlAiku/8tf9PXnrqWUa0aM+2rzZz65DB3DVjPKecXivo0CSEEpSI5Fvbv/6WZ7vcyezBQylfuRL95k2hyYC+FDmheNChCUpQIiKsffEVEhq34oPnl9Cw7S0MXDqX8zQBbeCUoEREgP179rLo0eGMbtuVPb/8Qofh/6bbuFHEVa2c9c4SEUpQIiIhtnz6GU+1vo1Fj42gqv/u1LU9bye2cOGgQ8t3lKBERNKwlBTem7OAhJtasv61FVxzx23EL57F6f9sEHRo+YoSlIhIBvb+sotZg4Yw7vZepBw6RLdxo+gw4lFKnXRi0KHlC0pQIiJZ+GpVEsObt+el0eOo/a9/MnDZHP7VoRUFChYMOrQ8TQlKRCQMh/7+mzcmTiOxaWu+TlpHk/g+9Js3hWrnnxt0aHmWEpSIyFHY9cM2Jt3Vnyl93LOmes0Yz61DBnNCmdJBh5bnKEGJiByDT998m8QmbVgxeSb1Gjdi4LK5XNjsJk2ZlI2UoEREjtFf+/fz4qinGXlrR376+htaPnwvd04bR8XTagYdWp6gBCUicpx+Sv6aZzr1ZM59j3DiqVXoN28qjeN7U6S4pkw6HkpQIiLZJGnZSzxxUytWLXqBS9q1ZMCyOZx79eVBh5VrKUGJiGSj/Xv2sPCRRMa078Yfu36j48jH6PrsKMpX0ZRJRyuSCaoKsAL4HNgI9PH1t/j1FKBemn0GA8nAJuDakPrrfF0yMCikvjqwCvgKmAekzkVSxK8n++3VsuH9iIiE7fv1G3mydRcWPz6SauefQ/zimVzTo4umTDoaZhapUtHM6vrlkmb2pZnVNrMzzex0M1tpZvVC2tc2s0/MrIiZVTezzWZW0JfNZlbDzAr7NrX9PvPNrJVfHmdmd/jlnn4dv31eVvGuWbPGABUVFZVsL6VOjLN2CUNtxIYPbNCL8+20iy8MPKYgi5klhZNHItmD2gZ85Jf34npSlfzPTem0bwLMBQ4A3+B6Pxf6kgx8Dfzl2zQBYoArgAV+/2lA05BjTfPLC4ArfXsRkRy3Z8dOZg58iPHdeoMZ3Sc8Rfthj2jKpCzk1D2oakAd3OW2jFQCtoSsb/V1GdWXB34DDqapT3usg8Bu3z6tbkASkBQXFxfeOxEROUZffrCGYTe34+WxEzjr8ksYuGwOl7RrqSmTMpATCaoEsBDoC+zJpF16PRw7hvrMjpXWBNx9sHo7d+7MJDQRkexx6O+/eX38FIY1bcs3H31C04F96Tt3Mqeed3bQoUWdSCeoQrjkNAtYlEXbrbiBFakqAz9mUr8TKAPEpqlPe6xYoDSw65jegYhIBPyy9Qee63kPU/sN5oSyZeg9cyK3PDSI4qVLBR1a1IhkgooBJuHuOY0Mo/0yoBVuBF51oBawGljjl6vjRum18m0NN0qwhd+/I7A05Fgd/XIL4E3S70GJiARqw+srSWzcmpVTZ1O/6Q0MXDaX+k1v0JRJQCRH8TU0Z72ZrfOlkZk1M7OtZnbAzLab2fKQfe7zI/Y2mdn1IfWN/CjAzb5Nan0NM1ttZslm9rwfAYiZFfXryX57DY3iU1FRifZSoVZNu2vaOBux4QO7a9o4q1CrZuAxRaKEO4ovxswQSEpKsvr16wcdhojkczExMdRr0oib7r6LoiVL8M7M+Sx/5jn+2r8/6NCyjZmt5X+/B/s/NJOEiEgUMTPWLPkPT9zUktVLXuSyTm0YuGwO51x1WdCh5TglKBGRKLRv9x4WDE1gdLuu/PHbbjqNepzbnxlB+cqVst45j1CCEhGJYt998ilPturCkoQnqV73POIXz+Kq7p0pWKhQ0KFFnBKUiEiUSzl0iHdmziOhcWs2vvUu19/VjfhFM6nVIG/fN1eCEhHJJfb8vIMZ/e9nQve+EBNDj4mjaZf4MKVOzJsz4ShBiYjkMpveX8Xwm9ux/OmJnH3Fvxi4bC6XtL01z02ZpAQlIpILHfzrL14dN5lhzdrx7boNNB3Ujz6zJ1H1nNpBh5ZtlKBERHKxX7ZsZeId/Zh2z32ULF+OXjMn0uLBgRQrlfunTFKCEhHJA9a/+iYJjVvxzsx5XNjsRga9MJf6TRoFHdZxUYISEckjDuzbx7JhoxnVsjM7v99Kq38/QM+pz1DhHzWCDu2YKEGJiOQx275MZmyH7sx78DEq1KzB3fOncWO/OylcrFjQoR0VJSgRkTzIzFi9+AUSbmpJ0gsvc3mXdgxYOpuzr7g06NDCpgQlIpKH/fHbbuY/9Bhj2ndn/97f6fzUE9w2djjlKp8SdGhZUoISEckHvl23nlEtO7F02FPUrF+HAYtnc2XXjlE9ZZISlIhIPpFy8BBvT59LQuNWfPb2ezTq3YP+C2dQ66Isn3wRCCUoEZF8Zvf2HUy/5z4m3tGPArEF6fHcGNomDKVkXPmgQzuCEpSISD71xbsfMqxZO159dhLnXnUZA5fNpWGbFsQUiI7UEB1RiIhIIA4eOMDyZ55j2M3t2PLpZzQbfA995kyiytnBT5mkBCUiIuz8bgvju/VhRv/7KRVXnt6zJtL8/niKlSoZWExKUCIi8l/rlr9BQuNWvDvreRq0aMLAZXO54KbrA4lFCUpERI5w4I99LE18klEtO/PL1h9o89iD9JzyDCfXrJ6jcShBiYhIun7c9BVj23fn+aFPULFWTe55fjo39L2DwsWK5sjrK0GJiEiGzIwPFyzliZtasvY/r3DFbR2IXzKb0iefGPHXjmSCqgKsAD4HNgJ9fH054DXgK/+zrK8vDbwAfOLbdw45Vkff/iu/nOoCYAOQDIwGYrJ4DREROQZ//Pob8x54lLEde/DFux+ye/uOyL+omUWqVDSzun65pJl9aWa1zSzRzAb5+kFmluCX7w1ZPtHMdplZYTMrZ2Zf+59l/XJZ3261mV1sZjFm9rKZXe/rM3qNDMuaNWsMUFFRUVGJcDGzpHDySCR7UNuAj/zyXlxPqhLQBJjm66cBTf2yASVxvaASwC7gIHAtrhe0C/jVL18HVARKAR/4faeHHCuj1xARkVwiNodepxpQB1gFnIxLXvifJ/nlscAy4EdcomoJpOCS2paQY231dZX8ctp6MnmNtLr5Qlxc3FG/KRERiZycGCRRAlgI9AX2ZNLuWmAdcApwPi5hleLwfaVQlkn90ZgA1APq7dy58yh3FRGRSIp0giqES06zgEW+bjvu8hz+589+ubNvY7hBD98AZ+B6RlVCjlkZ18va6pfT1mf2GiIikktEMkHFAJNw955GhtQv4/BIvI7AUr/8PXClXz4ZOB34GlgOXIMbiVfWLy/HXbrbCzTwr9Uh5FgZvYaIiOQWERzF19Cc9Wa2zpdGZlbezN4ws6/8z3K+/Slm9qqZbTCzT82sXcixuphZsi+dQ+rr+babzWysH81HJq+hUXwqKioqAZdwR/HFmBkCSUlJVr9+/aDDEBHJ88xsLe7+f6Y0k4SIiEQl9aAO2wF8dxz7xwHROhQwmmMDxXc8ojk2iO74ojk2iO74jje2U4Es50pSgso+SYTRZQ1INMcGiu94RHNsEN3xRXNsEN3x5UhsusQnIiJRSQlKRESiUsEhQ4YEHUNesjboADIRzbGB4jse0RwbRHd80RwbRHd8EY9N96BERCQq6RKfiIhEJSUoERGJSkpQR+86YBNuQttB6WwvAszz21fhHjWSU7KKrRPu+17rfLk9xyKDybhJez/NYHsM7qnIycB6oG4OxZUqq/guA3Zz+Nw9mDNhARk/nTpUUOcvnNguI7hzVxRYzeEndQ9Np02Qn9lw4utEcJ9bgILAx8CL6WyL7LmL4Fx8ebEU9PP+1TD3tN9PzD0lOLRNTzMb55dbmdm8KIqtk7k5C4M4d/8y94TlTzPY3sjcU5FjzKyBma2KsvguM7MXAzp3GT2dOhrOXzixBXnuYsyshF8u5M9LgzRtgvrMhhtfJwvuc4uZ3W1mszP4N4zouVMP6uhciPtL4WvgL2Au7um9oUKf5rsAN0N7es+uCiK2IL2NeypyRprgnopswIdAGQ4/MiUnZBVfkDJ6OnWooM5fOLEFyYDf/XIhX9KODAvqMwvhxRekysANwHMZbI/ouVOCOjoZPd03ozYHcZc2ykc+tLBiA2iOuwS0gCOfsxW0cOMP0sW4SzEvA2cFFEM1Dj+dOlQ0nL9qpB8bBHvuCuIujf0MvEbm5y4nP7OpsooPgvvcPgkMwD3dPD0RPXdKUEcnnKf4ZseTfo9FOK/7Au6XyLnA6xz+yycaBHXewvURbv6w84AxwJIAYsjs6dRBn7/MYgv63B3CPaW7Mu5Kw9lptgd97rKKL6jP7Y24pJnZ950ieu6UoI5ORk/3zahNLFCanLl0FE5svwAH/PJE4IIciCtc4cQfpD0cvhTzEu5STFwOvn56T6cOFeT5yyq2oM9dqt+AlbjBRKGC+symlVF8QX1u/wk0Br7F3TK4ApiZpk1Ez50S1NFZA9QCqgOFgVa4p/eGCn2abwvgTXLmr7FwYgu9J9EYd78gWizDPRU5BveU5N24+xvRogKH/1q8EPfZ+SWHXjujp1OHCur8hRNbkOfuRNz9OIBiwFXAF2naBPWZhfDiC+pzOxj3h0413O+TN4F2adpE9NzFZteB8omDwF24R84XxA1N3gg8jJvddxnuwzoDN2BhF+4fNlpi6437D37Qx9Yph2IDmIMbbhyH+6vrIdxf0gDjcH9ZN8Kdt31A5xyMLZz4WgB34M7dfty/a079Evsn0B7YgLtXAXAvUDUkvqDOXzixBXnuKuIuiRXEJcb5uOHS0fCZDTe+ID+36cmxc6epjkREJCrpEp+IiEQlJSgREYlKSlAiIhKVlKBERCQqKUGJiEhUUoISibz3/c9qQJtsPva9GbyWSK6nYeYiOecyoD9uCplwFcRNhZOR33HTDInkOepBiURe6jQ/TwCX4L7Q2g+XfIbhZgFZD3T37S7DPWNpNu4LsODmr1uL+/J1t5DjFfPHm5XmtWL8sT/1x2gZcuyVuElHv/D7pc7y8ATwmY9l+LG/XZHsoZkkRHLOII7sQXXDTUlUH/fgt/eAV/221ElDv/HrXXDf1C+GS2gL/fHuwk00mtbNvv483OwYa3CPFAE34/hZuLn63sPNBvEZ0Aw4AzfLQxlEAqYelEhwrsHNn7cO94iF8rj5FME9ZfWbkLa9cY+r+BA3OWctMtcQN33TIWA78BYuEaYeeyvuEQrrcPfG9gB/4p77czNuuiSRQClBiQQnBuiF6+mcj5voN7UH9UdIu8twk4hejOsRfYx7VHhWx87IgZDlQ7grKQdxvbaFQFPglXDegEgkKUGJ5Jy9QMmQ9eW4SVRTJ6U9DTghnf1KA7/iejVn4GYrT/V3yP6h3sbddyqImzH7X7ieU0ZK+Nd5CfdMp/QuG4rkKN2DEsk563E9lU+AqcBTuMtrH+F6PDtwvZe0XgF6+P034S7zpZrg6z8C2obUL+bwU2wN91TUn3AJLj0lgaW4nlkMbhCHSKA0zFxERKKSLvGJiEhUUoISEZGopAQlIiJRSQlKRESikhKUiIhEJSUoERGJSkpQIiISlf4fAsCQR6/r99kAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "dark"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training error in every epoch\n",
    "# TODO Implement\n",
    "x_axis = np.arange(len(nn.cost_))\n",
    "plt.plot(x_axis, nn.cost_)\n",
    "# # # Name the plot\n",
    "# # # TODO Implement\n",
    "plt.title(\"training error for every epoch\")\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 10.00%\n",
      "Test accuracy: 10.00%\n"
     ]
    }
   ],
   "source": [
    "# Compute Training Accuracy\n",
    "# TODO Implement\n",
    "# Compute Training Accuracy\n",
    "# TODO Implement\n",
    "y_pred = nn.inference(X_trainval)\n",
    "\n",
    "\n",
    "count = 0\n",
    "index = 0\n",
    "for i in y_pred:\n",
    "    if Y_trainval[index] == np.argmax(i) :\n",
    "        count = count+1\n",
    "    index = index+1\n",
    "\n",
    "acc=count/index\n",
    "\n",
    "\n",
    "print('Training accuracy: %.2f%%' % (acc * 100))\n",
    "\n",
    "# Compute Test Accuracy\n",
    "# TODO Implement\n",
    "y_pred1 = nn.inference(X_test)\n",
    "\n",
    "count1 = 0\n",
    "indx1 = 0\n",
    "for i in y_pred1:\n",
    "   # print(i)\n",
    "   # print(y_train[indx], np.max(i), i)\n",
    "    if Y_test[indx1] == np.argmax(i) :\n",
    "        count1 = count1+1\n",
    "    indx1 = indx1+1\n",
    "\n",
    "acc1=count1/indx1\n",
    "print('Test accuracy: %.2f%%' % (acc1 * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "You should provide a single Jupyter notebook (.ipynb file) as the solution. Put the names and student ids of your team members below. **Make sure to submit only 1 solution to only 1 tutor.**\n",
    "\n",
    "- Jane Doe, 123456\n",
    "- Jane Doe, 123456\n",
    "- Jane Doe, 123456"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points: 0.0 of 30.0 points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
