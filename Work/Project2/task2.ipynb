{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNIA 18/19 Project 2:  Gradient Descent & Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deadline: 4. January 2018, 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multinomial Logistic Regression and Cross Validation $~$ (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement a [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) model with tensorflow for Fashion-MNIST dataset. Cross Validation will be used to find the best **regularization parameter** $\\lambda$ for the L2-regularization term. Fashion-MNIST dataset is similar to the sklearn Digit dataset you used in the Project 1. It contains 60,000 training images and 10,000 testing images. Each example is a 28×28 grayscale image, associated with a label from 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural Network](https://s3-eu-central-1.amazonaws.com/zalando-wp-zalando-research-production/2017/08/fashion-mnist-sprite.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial logistic regression is a probabilistic, linear classifier. It is parametrized by a weight matrix $W$ and a bias vector $b$. Classification is done by projecting an input vector onto a set of hyperplanes, each of which corresponds to a class. The distance from the input to a hyperplane reflects the probability that the input is a member of the corresponding class.\n",
    "\n",
    "Mathematically, the probability that an input vector $\\bf{x} \\in \\mathbb{R}^p$ is a member of a class $i$ can be written as:\n",
    "$$P(Y=i|\\textbf{x}, W, b) = softmax(W\\textbf{x} + b)_i = \\frac{e^{W_i\\textbf{x} + b_i}}{\\sum_j{e^{W_j\\textbf{x} + b_j}}}$$\n",
    "where $W \\in \\mathbb{R}^{c \\times p}$, $b \\in \\mathbb{R}^c$ and $W_i \\in \\mathbb{R}^p$.\n",
    "\n",
    "The model’s prediction $y_{pred}$ is the class whose probability is maximal, specifically:\n",
    "$$y_{pred} = argmax_iP(Y=i|\\textbf{x}, W, b)$$\n",
    "\n",
    "We use cross-entropy loss with L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load **Fashion-MNIST** dataset and normalized it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_trainval, Y_trainval), (X_test, Y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The X_trainval has the following shape:\n",
      "Rows: 60000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_trainval = np.reshape(X_trainval, (X_trainval.shape[0],  X_trainval.shape[1] *  X_trainval.shape[2]))\n",
    "print('The X_trainval has the following shape:')\n",
    "print('Rows: %d, columns: %d' % (X_trainval.shape[0], X_trainval.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The X_test has the following shape:\n",
      "Rows: 10000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_test = np.reshape(X_test, (X_test.shape[0],  X_test.shape[1] *  X_test.shape[2]))\n",
    "print('The X_test has the following shape:')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data. Subtract the mean and divide by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_normalization(X_trainval, X_test):\n",
    "    train = np.copy(X_trainval)\n",
    "    test  = np.copy(X_test)\n",
    "    for row in np.transpose(train):\n",
    "        deviation = np.std(row)\n",
    "        if(deviation != 0):\n",
    "            row= row / deviation\n",
    "        row -= np.mean(row)\n",
    "        \n",
    "    for row in np.transpose(test):\n",
    "        deviation = np.std(row)\n",
    "        if(deviation != 0):\n",
    "            row = row / deviation\n",
    "        row -= np.mean(row)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "# The normalization should be done on X_train and X_test. \n",
    "# The normalized data should have the exactly same shape as the original data matrix.\n",
    "\n",
    "X_trainval, X_test = data_normalization(X_trainval, X_test)\n",
    "print(np.shape(X_trainval),np.shape(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define the Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the global configuration of this program is \n",
    "# defined, which you shouldn't change.\n",
    "\n",
    "class global_config(object):\n",
    "    lr = 0.0001  # learning rate\n",
    "    img_h = 28  # image height\n",
    "    img_w = 28  # image width\n",
    "    num_class = 10  # number of classes\n",
    "    num_epoch = 20  # number of training epochs\n",
    "    batch_size = 16  # batch size\n",
    "    K = 3  # K-fold cross validation\n",
    "    num_train = None  # the number of training data\n",
    "    lambd = None  # the factor for the L2-regularization\n",
    "\n",
    "config = global_config()\n",
    "config.num_train = X_trainval.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(X_trainval, Y_trainval, i, K):\n",
    "    \"\"\"\n",
    "    sklearn library is not allowed to use here.\n",
    "    \n",
    "    K is the total number of folds and i is the current fold.\n",
    "    \n",
    "    Think about how to deal with the case when the number of \n",
    "    training data can't be divided by K evenly.\n",
    "    #TODO: Implement\"\"\"\n",
    "    elements,_ = np.shape(Y_trainval)\n",
    "    surplus = elements % K\n",
    "    size_big = (elements/surplus)+1\n",
    "    size_small = elements\n",
    "    \n",
    "    if i < surplus:\n",
    "        slice_beg = i*size_big\n",
    "        slice_end = slice_beg + size_big\n",
    "        \n",
    "    if i >= surplus:\n",
    "        slice_beg = surplus*size_big+(i-surplus)*size_small\n",
    "        slice_end = slice_beg + size_small\n",
    "        \n",
    "    X_val = X_trainval[slice_beg:slice_end]\n",
    "    Y_val = Y_trainval[slice_beg:slice_end]\n",
    "    \n",
    "    X_train_lower = X_trainval[:slice_beg]\n",
    "    X_train_upper = X_trainval[slice_end:]\n",
    "    X_train       = np.concatenate((X_train_lower, X_train_upper))\n",
    "    \n",
    "    Y_train_lower = Y_trainval[:slice_beg]\n",
    "    Y_train_upper = Y_trainval[slice_end:]\n",
    "    Y_train       = np.concatenate((Y_train_lower, Y_train_upper))\n",
    "\n",
    "    \n",
    "        \n",
    "    return X_train, X_val, Y_train, Y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def shuffle_train_data(X_train, Y_train):\n",
    "    \"\"\"called after each epoch\"\"\"\n",
    "    perm = np.random.permutation(len(Y_train))\n",
    "    Xtr_shuf = X_train[perm]\n",
    "    Ytr_shuf = Y_train[perm]\n",
    "    return Xtr_shuf, Ytr_shuf\n",
    "\n",
    "np.shape(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "training\n",
    "\"\"\"\n",
    "class logistic_regression(object):\n",
    "    \n",
    "    def __init__(self, X, Y_gt, config, name):\n",
    "        \"\"\"\n",
    "        :param X: the training batch, which has the shape [batch_size, n_features].\n",
    "        :param Y_gt: the corresponding ground truth label vector.\n",
    "        :param config: the hyper-parameters you need for the implementation.\n",
    "        :param name: the name of this logistic regression model which is used to\n",
    "                     avoid the naming confict with the help of tf.variable_scope and reuse.\n",
    "       \n",
    "        Define the computation graph within the variable_scope here. \n",
    "        First define two variables W and b with tf.get_variable.\n",
    "        Then do the forward pass.\n",
    "        Then compute the cross entropy loss with tensorflow, don't forget the L2-regularization.\n",
    "        The Adam optimizer is already given. You shouldn't change it.\n",
    "        Finally compute the accuracy for one batch\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "            #TODO: Define two variables and the forward pass.\n",
    "            W = tf.get_variable(\"W\",shape=[samples, features]) \n",
    "            b = tf.get_variable(\"b\",shape=[features])\n",
    "    forwardPass = np.add (np.dot(np.transpose(W),X),b)\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(forwardPass)\n",
    "    l2regularizer = tf.nn.l2_loss(W)\n",
    "    loss = loss + lamda*l2regularizer\n",
    "    self._loss = np.reduce_mean(loss)\n",
    "            #TODO: Compute the cross entropy loss with L2-regularization.\n",
    "            i = 0\n",
    "            for yhat in self.train_step\n",
    "                if (yhat ==Y_gt[i])\n",
    "                self._num_acc = self._num_acc + 1 \n",
    "                \n",
    "                i++ \n",
    "            self._num_acc = self._num_acc/ i \n",
    "            # Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent \n",
    "            # to update network weights iteratively.\n",
    "            # It will be introduced in the lecture when talking about the optimization algorithms.\n",
    "            self._train_step = tf.train.AdamOptimizer(config.lr).minimize(self._loss)\n",
    "            \n",
    "            #TODO: Compute the accuracy\n",
    "            \n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_step\n",
    "    \n",
    "    @property\n",
    "    def loss(self):\n",
    "        return self._loss\n",
    "    \n",
    "    @property\n",
    "    def num_acc(self):\n",
    "        return self._num_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model, X_test, Y_test, config):\n",
    "    \"\"\" \n",
    "    Go through the X_test and use sess.run() to compute the loss and accuracy.\n",
    "    \n",
    "    Return the total loss and the accuracy for X_test.\n",
    "    \n",
    "    Note that this function will be used for the validation data\n",
    "    during training and the test data after training.\n",
    "    \"\"\"\n",
    "    num_test = X_test.shape[0]\n",
    "    total_cost = 0\n",
    "    accs = 0\n",
    "    #TODO: Implement\n",
    "    return total_cost / len(Y_test), accs / len(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, X_val, Y_train, Y_val, config):\n",
    "    \"\"\"\n",
    "    Train the model with sess.run().\n",
    "    \n",
    "    You should shuffle the data after each epoch and\n",
    "    evaluate training and validation loss after each epoch.\n",
    "    \n",
    "    Return the lists of the training/validation loss and accuracy.\n",
    "    \"\"\"\n",
    "    cost_trains = []\n",
    "    acc_trains = []\n",
    "    cost_vals = []\n",
    "    acc_vals = []\n",
    "    \n",
    "    for i in range(config.num_epoch):\n",
    "       #TODO: Implement\n",
    "        cost_trains.append(cost_train)\n",
    "        acc_trains.append(acc_train)\n",
    "        print(\"Epoch: %d :\" % (i + 1))\n",
    "        print(\"Train Loss: %f\" %  cost_train)\n",
    "        print(\"Training acc: %f\" % acc_train)\n",
    "        cost_vals.append(cost_val)\n",
    "        acc_vals.append(acc_val)\n",
    "        print(\"Validation Loss: %f\" % cost_val)\n",
    "        print(\"Validation acc: %f\" % acc_val)\n",
    "    return cost_trains, acc_trains, cost_vals, acc_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement cross validation to find an optimal value of $\\lambda$. The optimal hyper-parameters should be determined by the validation accuracy. The test set should only be used in the very end after all other processing, e.g. hyper-parameter choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda is 100.000000\n",
      "(784, 60000) (60000,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-cd4d70adec30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Prepare the training and validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_val_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trainval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_trainval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# For each lambda and K, we build a new model and train it from scratch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-f3433bde2dfc>\u001b[0m in \u001b[0;36mtrain_val_split\u001b[0;34m(X_trainval, Y_trainval, i, K)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#TODO: Implement\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_trainval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_trainval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0melements\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_trainval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0msurplus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melements\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0msize_big\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0melements\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0msurplus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Initialization\n",
    "\"\"\"\n",
    "# Use cross validation to choose the best lambda for the L2-regularization from the list below\n",
    "lambda_list = [100, 1, 0.1]\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, config.img_h * config.img_w])\n",
    "Y_gt = tf.placeholder(tf.int64, [None, ])\n",
    "\n",
    "for lambd in lambda_list:\n",
    "    val_loss_list = []\n",
    "    config.lambd = lambd\n",
    "    print(\"lambda is %f\" % lambd)\n",
    "    \n",
    "    for i in range(config.K):\n",
    "        # Prepare the training and validation data\n",
    "        X_train, X_val, Y_train, Y_val = train_val_split(X_trainval, Y_trainval, i, config.K)\n",
    "        \n",
    "        # For each lambda and K, we build a new model and train it from scratch\n",
    "        model = logistic_regression(X, Y_gt, config, name=str(lambd)+'_'+str(config.K))\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            # Initialize the variables of the model\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            # Train the model\n",
    "            cost_trains, acc_trains, cost_vals, acc_vals = train(model, X_train, X_val, Y_train, Y_val, config)\n",
    "            \n",
    "        val_loss_list.append(cost_vals[-1])\n",
    "        \n",
    "    print(\"The validation loss for lambda %f is %f\" % (lambd, np.mean(val_loss_list)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Combine Train and Validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the hyper-parameters you choose from the cross validation to re-train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.lambd =  #TODO: Choose the best lambda\n",
    "model = logistic_regression(X, Y_gt, config, name='trainval')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cost_trains, acc_trains, cost_tests, acc_tests = train(model, X_trainval, X_test, Y_trainval, Y_test, config)\n",
    "\n",
    "print(\"The final test acc is %f\" % acc_tests[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the impact of k in k-fold cross validation?\n",
    "\n",
    "2. What will happen to the training if you change the $\\lambda$ for L2-regularization?\n",
    "\n",
    "3. Why do we perform the gradient descent on a batch of the data rather than all of the data?\n",
    "\n",
    "4. Why does the loss increase, when the learning rate is too large?\n",
    "\n",
    "5. Do we apply L2-regularization for the bias $b$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting to know Back-Propagation in details $~$ (18 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise you would build a **feed-forward network** from scratch using **only** Numpy. For this, you also have to implement **Back-propagation** in python. Additionally, this network should have the option of **L2 regularization** enabled within it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before you start**: In this exercise you will implement a single hidden layer feedforward neural network. In case you are unfamiliar with the terminology and notation used here, please consult chapter 6 of the Deep Learning Book before you proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, a feedword neural network with a single hidden layer can be represented by the following function $$ f(x;\\theta) = f^{(2)}(f^{(1)}(f^{(0)}(x)))$$ where $f^{(0)}(x)$ is the input layer, $f^{(1)}(.)$ is the so called hidden layer, and $f^{(2)}(.)$ is the ouput layer of the network. $\\theta$ represents the parameters of the network whose values will be learned during the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network that you will implement in this exercise has the following layers:\n",
    "* $f^{(0)}(x) = \\mathbf{X}$, with $\\mathbf{X} \\in \\mathbb{R}^{b,p}$ where $b$ is the batch size and $p$ is the number of features.\n",
    "* $f^{(1)}(.) = \\sigma(\\mathbf{X} \\mathbf{W_1}+b_1)$, with $\\mathbf{X} \\in \\mathbb{R}^{b, p}$, $\\mathbf{W_1} \\in \\mathbb{R}^{p,u_1}$, $\\textbf{b}_1 \\in \\mathbb{R}^{u_1}$ where $u_1$ is the number of **hidden units**. Additonally, $\\sigma(x) = \\frac{1}{1 + \\exp{(-x})}$ is the **sigmoid** function.\n",
    "* $f^{(2)}(.) = softmax(\\mathbf{X} \\mathbf{W_2}+b_2)$, with $\\mathbf{X} \\in \\mathbb{R}^{b, u_1}$, $\\mathbf{W_2} \\in \\mathbb{R}^{u_1,u_2}$, $\\textbf{b}_2 \\in \\mathbb{R}^{u_2}$ where $u_2$ is the number of **output classes** in this particular layer.\n",
    "\n",
    "Note that both, $\\sigma(x)$ are applied **elementwise**. Further, the addition with the bias vector is also applied **elementwise** to each row of the matrix $\\mathbf{X} \\mathbf{W}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Fully_connected_Neural_Network(object):\n",
    "    \"\"\" Fully-connected neural network with one hidden layer.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_output : int\n",
    "        Number of class labels.\n",
    "        \n",
    "    n_features : int\n",
    "        Number of input features.\n",
    "        \n",
    "    n_hidden : int\n",
    "        Number of hidden units.\n",
    "        \n",
    "    l2 : float\n",
    "        regularization parameter\n",
    "        0 means no regularization\n",
    "        \n",
    "    epochs : int\n",
    "        One Epoch is when the entire dataset is passed forward and backward through the neural network only once.\n",
    "        \n",
    "    lr : float\n",
    "        Learning rate.\n",
    "        \n",
    "    batchsize : int\n",
    "        Total number of training examples present in a single batch.\n",
    "        \n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w1 : array, shape = [n_features, n_hidden_units]\n",
    "        Weight matrix for input layer -> hidden layer.\n",
    "    w2 : array, shape = [n_hidden_units, n_output_units]\n",
    "        Weight matrix for hidden layer -> output layer.\n",
    "    b1 : array, shape = [n_hidden_units, ]\n",
    "        Bias for input layer-> hidden layer.\n",
    "    b2 : array, shape = [n_output_units, ]\n",
    "        Bias for hidden layer -> output layer.\n",
    "\n",
    "    \"\"\"\n",
    "    # Points: 2.0\n",
    "    def __init__(self, n_output, n_features, n_hidden=30,\n",
    "                 l2=0.0, epochs=50, lr=0.001, batchsize=1):\n",
    "        self.n_output = n_output\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.batchsize = batchsize\n",
    "        #TODO Initialize weights and biases with np.random.uniform or np.random.normal and specify the shape\n",
    "        w1 = np.random.uniform(low = 0, high = 11, size= self.n_features*self.n_hidden)\n",
    "        self.w1 = w1.reshape(self.n_features, self.n_hidden)\n",
    "        w2 = np.random.uniform(low = 0, high = 11, size= self.n_hidden*self.n_output)\n",
    "        self.w2 = w2.reshape(self.n_hidden, self.n_output)\n",
    "        #bias mai dimension hidden aur output ka hoga \n",
    "        self.b1 = np.random.uniform(low = 0, high = 1, size = self.n_hidden)\n",
    "        self.b2 = np.random.uniform(low = 0, high = 1, size = self.n_output)\n",
    "\n",
    "    # Points: 0.5\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Compute sigmoid function\"\"\"\n",
    "        sigmoid_value = 1. / (1. + np.exp(-x))\n",
    "        return sigmoid_value\n",
    "        #TODO Implement\n",
    "\n",
    "\n",
    "    # Points: 0.5\n",
    "    def sigmoid_gradient(self, z):\n",
    "        sigmoid_grad = self.sigmoid(z)*(1 - self.sigmoid(z))\n",
    "        return sigmoid_grad\n",
    "        \"\"\"Compute gradient of the sigmoid function\"\"\"\n",
    "        #TODO Implement\n",
    "\n",
    "    \n",
    "    # Points: 1.0\n",
    "    def softmax(self, z):\n",
    "        \"\"\"Compute softmax function.\n",
    "        Implement a stable version which \n",
    "        takes care of overflow and underflow.\n",
    "        \"\"\"        \n",
    "        #TODO Implement\n",
    "        numerator = np.exp(z - np.amax(z))\n",
    "        denominator = np.sum(numerator)\n",
    "        softmax_output = np.exp(z - np.amax(z))/denominator \n",
    "        return softmax_output\n",
    "\n",
    "        \n",
    "    # Points: 2.0\n",
    "    def forward(self, X):\n",
    "        \"\"\"Compute feedforward step\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        z2 : array,\n",
    "            Input of the hidden layer.\n",
    "        a2 : array,\n",
    "            Output of the hidden layer.\n",
    "        z3 : array,\n",
    "            Input of the output layer.\n",
    "        a3 : array,\n",
    "            Output of the output layer.\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO Implement\n",
    "        a = np.dot(X, self.w1)\n",
    "        b = self.b1\n",
    "        z2 = np.add(a,b)\n",
    "        \n",
    "        a2 = self.softmax(z2)\n",
    "        \n",
    "        z3= np.dot(a2, self.w2) + self.b2\n",
    "        \n",
    "        a3 = self.softmax(z3)    \n",
    "        \n",
    "        return z2, a2, z3, a3\n",
    "        \n",
    "    # Points: 0.5\n",
    "    def L2_regularization(self, lambd):\n",
    "        \"\"\"Implement L2-regularization loss\"\"\"\n",
    "        #TODO Implement\n",
    "        first_term = np.sum(np.square(self.w1)) \n",
    "        second_term = np.sum(np.square(self.w2))\n",
    "        constant_multiplier = lambd/2*self.n_output \n",
    "        regularizer = (first_term + secod_term)*constant_multiplier\n",
    "        return regularizer \n",
    "        \n",
    "    # Points: 2.0\n",
    "    def loss(self, y_enc, output, epsilon=1e-12):\n",
    "        \"\"\"Implement total loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, one-hot encoded labels.\n",
    "        \n",
    "        output : array, output of the output layer\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float, total loss.\n",
    "\n",
    "        \"\"\"\n",
    "        #TODO Implement\n",
    "        \n",
    "        print(output[1,:])\n",
    "        \n",
    "        \n",
    "        prob1 = np.multiply(y_enc, np.log(output))\n",
    "        prob2 = np.multiply((1-y_enc), np.log(1-output))\n",
    "        log_probability = prob1 + prob2 \n",
    "        cross_entropy = -(1/self.n_output)*np.sum(log_probability)\n",
    "        \n",
    "        cost = cross_entropy + self.L2_regularization(epsilon = 1e-12)\n",
    "        return cost\n",
    "        \n",
    "        \n",
    "    # Points: 4.0\n",
    "    def compute_gradient(self, X, a2, a3, z2, y_enc):\n",
    "        \"\"\" Compute gradient using backpropagation.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        X : array, Input.\n",
    "        a2 : array, output of the hidden layer.\n",
    "        a3 : array, output of the output layer.\n",
    "        z2 : array, input of the hidden layer.\n",
    "        y_enc : array, one-hot encoded labels.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        grad1 : array, Gradient of the weight matrix w1.\n",
    "        grad2 : array, Gradient of the weight matrix w2.\n",
    "        grad3 : array, Gradient of the bias vector b1.\n",
    "        grad4 : array, Gradient of the bias vector b2.\n",
    "        \"\"\"\n",
    "        #TODO Implement\n",
    "        total = self.n_output \n",
    "        output_layer_difference = a3 - y_enc\n",
    "        d_weight1 = (1/total)*np.matmul(output_layer_difference, np.transpose(a2))\n",
    "        d_bias1 = (1/total)*np.sum(output_layer_difference, axis =1, keepdims = True)\n",
    "        \n",
    "        hidden_layer_difference = np.matmul(np.transpose(self.w2), output_layer_difference)*(1 - np.power(a2,2))\n",
    "        d_weight2 = (1/total)*np.matmul(hidden_layer_difference, np.transpose(X))\n",
    "        d_bias2 = (1/total)*np.sum(hidden_layer_difference, axis = 1, keepdims = True)\n",
    "        \n",
    "        grad1 = d_weight2\n",
    "        grad2 = d_weight1\n",
    "        grad3 = d_bias2\n",
    "        grad4 = d_bias1\n",
    "\n",
    "        return grad1, grad2, grad3, grad4\n",
    "        \n",
    "    # Points: 1.0\n",
    "    def inference(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, Predicted labels.\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO Implement\n",
    "        z2, a2, z3, a3 = self.forward(X)\n",
    "        \n",
    "        y_pred = a3 > 0.5\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def shuffle_train_data(self, X, Y):\n",
    "        \"\"\"called after each epoch\"\"\"\n",
    "        perm = np.random.permutation(Y.shape[0])\n",
    "        X_shuf = X[perm]\n",
    "        Y_shuf = Y[perm]\n",
    "        return X_shuf, Y_shuf\n",
    "    \n",
    "    # Points: 2.0\n",
    "    def train(self, X_train, Y_train, verbose=False):\n",
    "        \"\"\" Fit the model.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input.\n",
    "        y : array, Ground truth class labels.\n",
    "        verbose : bool, Print the training progress\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        #TODO Initialization\n",
    "        self.cost_ = []\n",
    "        \n",
    "\n",
    "        for i in range(self.epochs):\n",
    "        \n",
    "            if verbose:\n",
    "                print('\\nEpoch: %d/%d' % (i+1, self.epochs))\n",
    "\n",
    "\n",
    "                # feedforward and loss computation\n",
    "                z2, a2, z3, a3 = self.forward(X_train)\n",
    "                cost = self.loss(Y_train, a3, epsilon = 1e-12)\n",
    "\n",
    "                self.cost_.append(cost)\n",
    "\n",
    "                # compute gradient via backpropagation and update the weights\n",
    "                grad1, grad2, grad3, grad4 = self.compute_gradient(X_train,a2, a3 ,z2, Y_train)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $15.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Fully_connected_Neural_Network(n_output=10, \n",
    "                                    n_features= X_trainval.shape[1], \n",
    "                                    n_hidden=50, \n",
    "                                    l2=0.1, \n",
    "                                    epochs=1000, \n",
    "                                    lr=0.001,\n",
    "                                    batchsize=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/1000\n",
      "[[8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]\n",
      " [8.45932766e-08 5.97238622e-08 3.98701902e-08 8.35054488e-08\n",
      "  4.18421866e-08 5.31153261e-08 6.47673269e-08 4.51052292e-08\n",
      "  5.52129351e-08 6.56035356e-08]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (60000,) (60000,28,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-00dbb181338e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_trainval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_trainval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-5-c1fb4894b0aa>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X_train, Y_train, verbose)\u001b[0m\n\u001b[0;32m    252\u001b[0m                 \u001b[1;31m# feedforward and loss computation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m                 \u001b[0mz2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m                 \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-c1fb4894b0aa>\u001b[0m in \u001b[0;36mloss\u001b[1;34m(self, y_enc, output, epsilon)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m         \u001b[0mprob1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_enc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m         \u001b[0mprob2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0my_enc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0mlog_probability\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprob1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mprob2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (60000,) (60000,28,10) "
     ]
    }
   ],
   "source": [
    "nn.train(X_trainval, Y_trainval, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training error for every iteration\n",
    "# in every epoch\n",
    "\n",
    "# TODO Implement\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training error in every epoch\n",
    "# TODO Implement\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Training Accuracy\n",
    "# TODO Implement\n",
    "\n",
    "print('Training accuracy: %.2f%%' % (acc * 100))\n",
    "\n",
    "# Compute Test Accuracy\n",
    "# TODO Implement\n",
    "\n",
    "print('Test accuracy: %.2f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "You should provide a single Jupyter notebook (.ipynb file) as the solution. Put the names and student ids of your team members below. **Make sure to submit only 1 solution to only 1 tutor.**\n",
    "\n",
    "- Jane Doe, 123456\n",
    "- Jane Doe, 123456\n",
    "- Jane Doe, 123456"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points: 0.0 of 30.0 points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
