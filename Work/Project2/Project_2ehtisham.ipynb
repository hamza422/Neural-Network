{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNIA 18/19 Project 2:  Gradient Descent & Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deadline: 4. January 2018, 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multinomial Logistic Regression and Cross Validation $~$ (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement a [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) model with tensorflow for Fashion-MNIST dataset. Cross Validation will be used to find the best **regularization parameter** $\\lambda$ for the L2-regularization term. Fashion-MNIST dataset is similar to the sklearn Digit dataset you used in the Project 1. It contains 60,000 training images and 10,000 testing images. Each example is a 28×28 grayscale image, associated with a label from 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural Network](https://s3-eu-central-1.amazonaws.com/zalando-wp-zalando-research-production/2017/08/fashion-mnist-sprite.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial logistic regression is a probabilistic, linear classifier. It is parametrized by a weight matrix $W$ and a bias vector $b$. Classification is done by projecting an input vector onto a set of hyperplanes, each of which corresponds to a class. The distance from the input to a hyperplane reflects the probability that the input is a member of the corresponding class.\n",
    "\n",
    "Mathematically, the probability that an input vector $\\bf{x} \\in \\mathbb{R}^p$ is a member of a class $i$ can be written as:\n",
    "$$P(Y=i|\\textbf{x}, W, b) = softmax(W\\textbf{x} + b)_i = \\frac{e^{W_i\\textbf{x} + b_i}}{\\sum_j{e^{W_j\\textbf{x} + b_j}}}$$\n",
    "where $W \\in \\mathbb{R}^{c \\times p}$, $b \\in \\mathbb{R}^c$ and $W_i \\in \\mathbb{R}^p$.\n",
    "\n",
    "The model’s prediction $y_{pred}$ is the class whose probability is maximal, specifically:\n",
    "$$y_{pred} = argmax_iP(Y=i|\\textbf{x}, W, b)$$\n",
    "\n",
    "We use cross-entropy loss with L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load **Fashion-MNIST** dataset and normalized it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import metrics\n",
    "import math\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_trainval, Y_trainval), (X_test, Y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The X_trainval has the following shape:\n",
      "Rows: 60000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_trainval = np.reshape(X_trainval, (X_trainval.shape[0],  X_trainval.shape[1] *  X_trainval.shape[2]))\n",
    "print('The X_trainval has the following shape:')\n",
    "print('Rows: %d, columns: %d' % (X_trainval.shape[0], X_trainval.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The X_test has the following shape:\n",
      "Rows: 10000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_test = np.reshape(X_test, (X_test.shape[0],  X_test.shape[1] *  X_test.shape[2]))\n",
    "print('The X_test has the following shape:')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data. Subtract the mean and divide by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_normalization(X_trainval, X_test):\n",
    "#     # TODO: Implement\n",
    "#     X_trainval_normalized = (X_trainval - np.mean(X_trainval,axis = 0))/X_trainval.std()\n",
    "#     X_test_normalized = (X_test - np.mean(X_test,axis = 0))/X_test.std()\n",
    "#     return X_trainval_normalized, X_test_normalized\n",
    "\n",
    "def data_normalization(X_trainval, X_test):\n",
    "    X_trainval_normalized = np.copy(X_trainval)\n",
    "    X_test_normalized = np.copy(X_test)\n",
    "    for i in range(0, X_trainval.shape[1]):\n",
    "        X_trainval_normalized[:, i] = np.array(np.divide((X_trainval[:, i] - np.mean(X_trainval[:, i])), np.std(X_trainval[:, i])))\n",
    "    for k in range(0, X_test.shape[1]):\n",
    "        X_test_normalized[:, k] = np.array(np.divide((X_test[:, k] - np.mean(X_test[:, k])), np.std(X_test[:, k])))\n",
    "\n",
    "    return X_trainval_normalized, X_test_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The X_trainval has the following shape after normalization:\n",
      "Rows: 60000, columns: 784\n",
      "The X_test has the following shape after normalization:\n",
      "Rows: 10000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "# The normalization should be done on X_train and X_test. \n",
    "# The normalized data should have the exactly same shape as the original data matrix.\n",
    "\n",
    "X_trainval, X_test = data_normalization(X_trainval, X_test)\n",
    "print('The X_trainval has the following shape after normalization:')\n",
    "print('Rows: %d, columns: %d' % (X_trainval.shape[0], X_trainval.shape[1]))\n",
    "print('The X_test has the following shape after normalization:')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define the Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the global configuration of this program is \n",
    "# defined, which you shouldn't change.\n",
    "\n",
    "class global_config(object):\n",
    "    lr = 0.0001  # learning rate\n",
    "    img_h = 28  # image height\n",
    "    img_w = 28  # image width\n",
    "    num_class = 10  # number of classes\n",
    "    num_epoch = 20  # number of training epochs\n",
    "    batch_size = 16  # batch size\n",
    "    K = 3  # K-fold cross validation\n",
    "    num_train = None  # the number of training data\n",
    "    lambd = None  # the factor for the L2-regularization\n",
    "\n",
    "config = global_config()\n",
    "config.num_train = X_trainval.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(X_trainval, Y_trainval, i, K):\n",
    "    \"\"\"\n",
    "    sklearn library is not allowed to use here.\n",
    "    \n",
    "    K is the total number of folds and i is the current fold.\n",
    "    \n",
    "    Think about how to deal with the case when the number of \n",
    "    training data can't be divided by K evenly.\n",
    "    \"\"\"\n",
    "    #TODO: Implement    \n",
    "    \n",
    "#     X_n = len(X_trainval)\n",
    "#     Y_n = len(Y_trainval)\n",
    "    \n",
    "#     X_dataset = X_trainval[X_n*(i-1)//K:X_n*i//K]\n",
    "#     Y_dataset = Y_trainval[Y_n*(i-1)//K:Y_n*i//K]\n",
    "    \n",
    "#     split_X = math.floor(X_dataset.shape[0]*0.8)\n",
    "#     split_Y = math.floor(Y_dataset.shape[0]*0.8)\n",
    "    \n",
    "    \n",
    "#     X_train = X_trainval[:split_X]\n",
    "#     X_val = Y_trainval[split_X:]\n",
    "    \n",
    "#     Y_train = dataset[:split_Y]\n",
    "#     Y_val = dataset[split_Y:]\n",
    "    \n",
    "   \n",
    "    \n",
    "    size = X_trainval.shape[0]\n",
    "    border1 = (i - 1) * (size // K)\n",
    "    border2 = i * (size // K)\n",
    "    \n",
    "    X_val = [X_trainval[i] for i in range(border1, border2)]\n",
    "    Y_val = [Y_trainval[i] for i in range(border1, border2)]\n",
    "    \n",
    "    X_train = np.concatenate((X_trainval[:border1,:], X_trainval[border2:,:]))\n",
    "    Y_train = np.concatenate((Y_trainval[:border1], Y_trainval[border2:]))\n",
    "#     print('Training set', X_train.shape, X_val.shape)\n",
    "#     print('Validation set', Y_train.shape, Y_val.shape)\n",
    "    \n",
    "    \n",
    "    return X_train, X_val, Y_train, Y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_train_data(X_train, Y_train):\n",
    "    \"\"\"called after each epoch\"\"\"\n",
    "    perm = np.random.permutation(len(Y_train))\n",
    "    Xtr_shuf = X_train[perm]\n",
    "    Ytr_shuf = Y_train[perm]\n",
    "    return Xtr_shuf, Ytr_shuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "training\n",
    "\"\"\"\n",
    "class logistic_regression(object):\n",
    "    \n",
    "    def __init__(self, X, Y_gt, config, name):\n",
    "        \"\"\"\n",
    "        :param X: the training batch, which has the shape [batch_size, n_features].\n",
    "        :param Y_gt: the corresponding ground truth label vector.\n",
    "        :param config: the hyper-parameters you need for the implementation.\n",
    "        :param name: the name of this logistic regression model which is used to\n",
    "                     avoid the naming confict with the help of tf.variable_scope and reuse.\n",
    "       \n",
    "        Define the computation graph within the variable_scope here. \n",
    "        First define two variables W and b with tf.get_variable.\n",
    "        Then do the forward pass.\n",
    "        Then compute the cross entropy loss with tensorflow, don't forget the L2-regularization.\n",
    "        The Adam optimizer is already given. You shouldn't change it.\n",
    "        Finally compute the accuracy for one batch\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "            #TODO: Define two variables and the forward pass.   \n",
    "            W = tf.get_variable(\"weights\", [X.shape[1], config.num_class])\n",
    "            b = tf.get_variable(\"bias\", [1, config.num_class])\n",
    "            \n",
    "            #TODO: Compute the cross entropy loss with L2-regularization.            \n",
    "            \n",
    "            # Training computation.\n",
    "            y_pred = tf.matmul(X, W) + b \n",
    "            Y_gt = tf.cast(Y_gt, tf.float32)\n",
    "            \n",
    "            Y_gt=tf.transpose(Y_gt)\n",
    "            print(Y_gt.shape)\n",
    "            # Original loss function\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_pred, labels=Y_gt) )\n",
    "            # Loss function using L2 Regularization\n",
    "            regularizer = tf.nn.l2_loss(W)\n",
    "            self._loss = tf.reduce_mean(loss + config.lambd * regularizer)\n",
    "     \n",
    "            # Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent \n",
    "            # to update network weights iteratively.\n",
    "            # It will be introduced in the lecture when talking about the optimization algorithms.\n",
    "            self._train_step = tf.train.AdamOptimizer(config.lr).minimize(self._loss)\n",
    "            \n",
    "            #TODO: Compute the accuracy        \n",
    "#             train_prediction = tf.nn.softmax(y_pred)\n",
    "#             y_pred_batch = session.run(y_pred,X)\n",
    "#             print(metrics.accuracy_score(Y_gt, y_pred_batch))\n",
    "            \n",
    "            correct_prediction = tf.equal(y_pred, Y_gt)\n",
    "            self._num_acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_step\n",
    "    \n",
    "    @property\n",
    "    def loss(self):\n",
    "        return self._loss\n",
    "    \n",
    "    @property\n",
    "    def num_acc(self):\n",
    "        return self._num_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model, X_test, Y_test, config):\n",
    "    \"\"\" \n",
    "    Go through the X_test and use sess.run() to compute the loss and accuracy.\n",
    "    \n",
    "    Return the total loss and the accuracy for X_test.\n",
    "    \n",
    "    Note that this function will be used for the validation data\n",
    "    during training and the test data after training.\n",
    "    \"\"\"\n",
    "    num_test = X_test.shape[0]\n",
    "    total_cost = 0\n",
    "    accs = 0\n",
    "    #TODO: Implement\n",
    "    with tf.Session() as session:\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        feed_dict = {X: X_test, Y_gt: Y_test}\n",
    "\n",
    "\n",
    "        for i in range(config.num_epoch):\n",
    "            session.run(model.train_op, feed_dict)\n",
    "            print(i, \"loss:\", model.loss.eval(feed_dict))\n",
    "    \n",
    "        total_cost, accs = session.run([model.loss, model.num_acc], feed_dict = {X: X_test, Y_gt: Y_test})\n",
    "    \n",
    "    return total_cost / len(Y_test), accs / len(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, X_val, Y_train, Y_val, config):\n",
    "    \"\"\"\n",
    "    Train the model with sess.run().\n",
    "    \n",
    "    You should shuffle the data after each epoch and\n",
    "    evaluate training and validation loss after each epoch.\n",
    "    \n",
    "    Return the lists of the training/validation loss and accuracy.\n",
    "    \"\"\"\n",
    "    cost_trains = []\n",
    "    acc_trains = []\n",
    "    cost_vals = []\n",
    "    acc_vals = []\n",
    "    \n",
    "    for i in range(config.num_epoch):\n",
    "       #TODO: Implement    \n",
    "        # Shuffle the Data\n",
    "        X_tr_shuffled, Y_tr_shuffled = shuffle_train_data(X_train, Y_train)\n",
    "        nsamples = X_tr_shuffled.shape[0]\n",
    "        bs = 0\n",
    "\n",
    "        # Perform a single epoch\n",
    "        while bs < nsamples:\n",
    "            X_tr_batch = X_tr_shuffled[bs: bs+config.batch_size]\n",
    "            Y_tr_batch = Y_tr_shuffled[bs: bs+config.batch_size]\n",
    "            #batch_loss, batch_acc = sess.run([model.loss, model.num_acc], feed_dict = {X: X_tr_batch, Y_gt: Y_tr_batch})\n",
    "            cost_train, acc_train = testing(model, X_tr_batch, Y_tr_batch, config)\n",
    "            bs += config.batch_size\n",
    "            \n",
    "        X_tr_shuffled, Y_tr_shuffled = shuffle_train_data(X_train, Y_train)\n",
    "        cost_train, acc_train = testing(model, X_tr_shuffled, Y_tr_shuffled, config)\n",
    "        \n",
    "        # Compute the cost and accuracy (val) of the epoch\n",
    "        cost_val, acc_val = testing(model, X_val, Y_val,config)\n",
    "        \n",
    "        cost_trains.append(cost_train)\n",
    "        acc_trains.append(acc_train)\n",
    "        print(\"Epoch: %d :\" % (i + 1))\n",
    "        print(\"Train Loss: %f\" %  cost_train)\n",
    "        print(\"Training acc: %f\" % acc_train)\n",
    "        cost_vals.append(cost_val)\n",
    "        acc_vals.append(acc_val)\n",
    "        print(\"Validation Loss: %f\" % cost_val)\n",
    "        print(\"Validation acc: %f\" % acc_val)\n",
    "    return cost_trains, acc_trains, cost_vals, acc_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement cross validation to find an optimal value of $\\lambda$. The optimal hyper-parameters should be determined by the validation accuracy. The test set should only be used in the very end after all other processing, e.g. hyper-parameter choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda is 100.000000\n",
      "(?,)\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "logits and labels must be broadcastable: logits_size=[16,10] labels_size=[1,16]\n\t [[Node: 100_3_2/softmax_cross_entropy_with_logits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](100_3_2/add, 100_3_2/softmax_cross_entropy_with_logits/Reshape_1)]]\n\nCaused by op '100_3_2/softmax_cross_entropy_with_logits', defined at:\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\asyncio\\base_events.py\", line 427, in run_forever\n    self._run_once()\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\asyncio\\base_events.py\", line 1440, in _run_once\n    handle._run()\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-18-cd4d70adec30>\", line 21, in <module>\n    model = logistic_regression(X, Y_gt, config, name=str(lambd)+'_'+str(config.K))\n  File \"<ipython-input-17-a9362587ca0e>\", line 36, in __init__\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_pred, labels=Y_gt) )\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1879, in softmax_cross_entropy_with_logits_v2\n    precise_logits, labels, name=name)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 7208, in softmax_cross_entropy_with_logits\n    name=name)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): logits and labels must be broadcastable: logits_size=[16,10] labels_size=[1,16]\n\t [[Node: 100_3_2/softmax_cross_entropy_with_logits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](100_3_2/add, 100_3_2/softmax_cross_entropy_with_logits/Reshape_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: logits and labels must be broadcastable: logits_size=[16,10] labels_size=[1,16]\n\t [[Node: 100_3_2/softmax_cross_entropy_with_logits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](100_3_2/add, 100_3_2/softmax_cross_entropy_with_logits/Reshape_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-cd4d70adec30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mcost_trains\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_trains\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mval_loss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcost_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-424419bd6f65>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, X_train, X_val, Y_train, Y_val, config)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mY_tr_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_tr_shuffled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;31m#batch_loss, batch_acc = sess.run([model.loss, model.num_acc], feed_dict = {X: X_tr_batch, Y_gt: Y_tr_batch})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0mcost_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtesting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_tr_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_tr_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m             \u001b[0mbs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-f904e2385b97>\u001b[0m in \u001b[0;36mtesting\u001b[1;34m(model, X_test, Y_test, config)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m             \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"loss:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1289\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1291\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1293\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: logits and labels must be broadcastable: logits_size=[16,10] labels_size=[1,16]\n\t [[Node: 100_3_2/softmax_cross_entropy_with_logits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](100_3_2/add, 100_3_2/softmax_cross_entropy_with_logits/Reshape_1)]]\n\nCaused by op '100_3_2/softmax_cross_entropy_with_logits', defined at:\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\asyncio\\base_events.py\", line 427, in run_forever\n    self._run_once()\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\asyncio\\base_events.py\", line 1440, in _run_once\n    handle._run()\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-18-cd4d70adec30>\", line 21, in <module>\n    model = logistic_regression(X, Y_gt, config, name=str(lambd)+'_'+str(config.K))\n  File \"<ipython-input-17-a9362587ca0e>\", line 36, in __init__\n    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=y_pred, labels=Y_gt) )\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\", line 1879, in softmax_cross_entropy_with_logits_v2\n    precise_logits, labels, name=name)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\", line 7208, in softmax_cross_entropy_with_logits\n    name=name)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 454, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3155, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\Hamza-pc\\Miniconda3\\envs\\nnia\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1717, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): logits and labels must be broadcastable: logits_size=[16,10] labels_size=[1,16]\n\t [[Node: 100_3_2/softmax_cross_entropy_with_logits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](100_3_2/add, 100_3_2/softmax_cross_entropy_with_logits/Reshape_1)]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Initialization\n",
    "\"\"\"\n",
    "# Use cross validation to choose the best lambda for the L2-regularization from the list below\n",
    "lambda_list = [100, 1, 0.1]\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, config.img_h * config.img_w])\n",
    "Y_gt = tf.placeholder(tf.int64, [None, ])\n",
    "\n",
    "for lambd in lambda_list:\n",
    "    val_loss_list = []\n",
    "    config.lambd = lambd\n",
    "    print(\"lambda is %f\" % lambd)\n",
    "    \n",
    "    for i in range(config.K):\n",
    "        # Prepare the training and validation data\n",
    "        X_train, X_val, Y_train, Y_val = train_val_split(X_trainval, Y_trainval, i, config.K)\n",
    "        \n",
    "        # For each lambda and K, we build a new model and train it from scratch\n",
    "        model = logistic_regression(X, Y_gt, config, name=str(lambd)+'_'+str(config.K))\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            # Initialize the variables of the model\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            # Train the model\n",
    "            cost_trains, acc_trains, cost_vals, acc_vals = train(model, X_train, X_val, Y_train, Y_val, config)\n",
    "            \n",
    "        val_loss_list.append(cost_vals[-1])\n",
    "        \n",
    "    print(\"The validation loss for lambda %f is %f\" % (lambd, np.mean(val_loss_list)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Combine Train and Validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the hyper-parameters you choose from the cross validation to re-train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.lambd =  #TODO: Choose the best lambda\n",
    "model = logistic_regression(X, Y_gt, config, name='trainval')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cost_trains, acc_trains, cost_tests, acc_tests = train(model, X_trainval, X_test, Y_trainval, Y_test, config)\n",
    "\n",
    "print(\"The final test acc is %f\" % acc_tests[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the impact of k in k-fold cross validation?\n",
    "\n",
    "2. What will happen to the training if you change the $\\lambda$ for L2-regularization?\n",
    "\n",
    "3. Why do we perform the gradient descent on a batch of the data rather than all of the data?\n",
    "\n",
    "4. Why does the loss increase, when the learning rate is too large?\n",
    "\n",
    "5. Do we apply L2-regularization for the bias $b$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* 1 k refers to the number of groups that a given data sample is to be split into.Choosing K will impact the variance. If k is high then it reduces the variance estimate .lower K is usually cheaper and more biased.The value for k is chosen such that each train/test group of data samples is large enough to be statistically representative of the broader dataset.If a value for k is chosen that does not evenly split the data sample, then one group will contain a remainder of the examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Increasing the lambda value strengthens the regularization effect.If your lambda value is too high, your model will \n",
    "be simple, but you run the risk of underfitting your data.The model will not learn enough about the training data \n",
    "to make useful predictions.If the lambda value is too low, the model will be more complex, and it will run the risk of \n",
    "overfitting  data. The model will learn too much about the particularities of the training data, and will not be able to \n",
    "generalize to new data.Setting lambda to zero removes regularization completely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3.Calculating gradient needs to sum over all the data points. If we have large amount of data, then it takes a long time.\n",
    "Too many gradient descent updates are required and  Each gradient descent step is too expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4. if the learning rate is set too high, it can cause undesirable divergent behavior in the loss function.\n",
    "Gradient descent can overshoot minimum and it may fail to converge or even diverge leading to higher loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.No,L2-regularization is not usually applied to the bias terms b. The motivation behind L2 is that by restricting the weights, constraining the network, it is  less likely to overfit. It makes little sense to restrict the weights of the biases since the biases are fixed (e.g. b = 1) thus work like neuron intercepts, which make sense to be given a higher flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting to know Back-Propagation in details $~$ (18 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise you would build a **feed-forward network** from scratch using **only** Numpy. For this, you also have to implement **Back-propagation** in python. Additionally, this network should have the option of **L2 regularization** enabled within it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before you start**: In this exercise you will implement a single hidden layer feedforward neural network. In case you are unfamiliar with the terminology and notation used here, please consult chapter 6 of the Deep Learning Book before you proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, a feedword neural network with a single hidden layer can be represented by the following function $$ f(x;\\theta) = f^{(2)}(f^{(1)}(f^{(0)}(x)))$$ where $f^{(0)}(x)$ is the input layer, $f^{(1)}(.)$ is the so called hidden layer, and $f^{(2)}(.)$ is the ouput layer of the network. $\\theta$ represents the parameters of the network whose values will be learned during the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network that you will implement in this exercise has the following layers:\n",
    "* $f^{(0)}(x) = \\mathbf{X}$, with $\\mathbf{X} \\in \\mathbb{R}^{b,p}$ where $b$ is the batch size and $p$ is the number of features.\n",
    "* $f^{(1)}(.) = \\sigma(\\mathbf{X} \\mathbf{W_1}+b_1)$, with $\\mathbf{X} \\in \\mathbb{R}^{b, p}$, $\\mathbf{W_1} \\in \\mathbb{R}^{p,u_1}$, $\\textbf{b}_1 \\in \\mathbb{R}^{u_1}$ where $u_1$ is the number of **hidden units**. Additonally, $\\sigma(x) = \\frac{1}{1 + \\exp{(-x})}$ is the **sigmoid** function.\n",
    "* $f^{(2)}(.) = softmax(\\mathbf{X} \\mathbf{W_2}+b_2)$, with $\\mathbf{X} \\in \\mathbb{R}^{b, u_1}$, $\\mathbf{W_2} \\in \\mathbb{R}^{u_1,u_2}$, $\\textbf{b}_2 \\in \\mathbb{R}^{u_2}$ where $u_2$ is the number of **output classes** in this particular layer.\n",
    "\n",
    "Note that both, $\\sigma(x)$ are applied **elementwise**. Further, the addition with the bias vector is also applied **elementwise** to each row of the matrix $\\mathbf{X} \\mathbf{W}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Fully_connected_Neural_Network(object):\n",
    "    \"\"\" Fully-connected neural network with one hidden layer.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_output : int\n",
    "        Number of class labels.\n",
    "        \n",
    "    n_features : int\n",
    "        Number of input features.\n",
    "        \n",
    "    n_hidden : int\n",
    "        Number of hidden units.\n",
    "        \n",
    "    l2 : float\n",
    "        regularization parameter\n",
    "        0 means no regularization\n",
    "        \n",
    "    epochs : int\n",
    "        One Epoch is when the entire dataset is passed forward and backward through the neural network only once.\n",
    "        \n",
    "    lr : float\n",
    "        Learning rate.\n",
    "        \n",
    "    batchsize : int\n",
    "        Total number of training examples present in a single batch.\n",
    "        \n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w1 : array, shape = [n_features, n_hidden_units]\n",
    "        Weight matrix for input layer -> hidden layer.\n",
    "    w2 : array, shape = [n_hidden_units, n_output_units]\n",
    "        Weight matrix for hidden layer -> output layer.\n",
    "    b1 : array, shape = [n_hidden_units, ]\n",
    "        Bias for input layer-> hidden layer.\n",
    "    b2 : array, shape = [n_output_units, ]\n",
    "        Bias for hidden layer -> output layer.\n",
    "\n",
    "    \"\"\"\n",
    "    # Points: 2.0\n",
    "    def __init__(self, n_output, n_features, n_hidden=30,\n",
    "                 l2=0.0, epochs=50, lr=0.001, batchsize=1):\n",
    "        self.n_output = n_output\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.batchsize = batchsize\n",
    "        #TODO Initialize weights and biases with np.random.uniform or np.random.normal and specify the shape\n",
    "\n",
    "        \n",
    "    # Points: 0.5\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Compute sigmoid function\"\"\"\n",
    "        #TODO Implement\n",
    "\n",
    "\n",
    "    # Points: 0.5\n",
    "    def sigmoid_gradient(self, z):\n",
    "        \"\"\"Compute gradient of the sigmoid function\"\"\"\n",
    "        #TODO Implement\n",
    "\n",
    "    \n",
    "    # Points: 1.0\n",
    "    def softmax(self, z):\n",
    "        \"\"\"Compute softmax function.\n",
    "        Implement a stable version which \n",
    "        takes care of overflow and underflow.\n",
    "        \"\"\"        \n",
    "        #TODO Implement\n",
    "\n",
    "        \n",
    "    # Points: 2.0\n",
    "    def forward(self, X):\n",
    "        \"\"\"Compute feedforward step\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        z2 : array,\n",
    "            Input of the hidden layer.\n",
    "        a2 : array,\n",
    "            Output of the hidden layer.\n",
    "        z3 : array,\n",
    "            Input of the output layer.\n",
    "        a3 : array,\n",
    "            Output of the output layer.\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO Implement\n",
    "        \n",
    "        \n",
    "        return z2, a2, z3, a3\n",
    "        \n",
    "    # Points: 0.5\n",
    "    def L2_regularization(self, lambd):\n",
    "        \"\"\"Implement L2-regularization loss\"\"\"\n",
    "        #TODO Implement\n",
    "        \n",
    "        \n",
    "    # Points: 2.0\n",
    "    def loss(self, y_enc, output, epsilon=1e-12):\n",
    "        \"\"\"Implement total loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, one-hot encoded labels.\n",
    "        \n",
    "        output : array, output of the output layer\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float, total loss.\n",
    "\n",
    "        \"\"\"\n",
    "        #TODO Implement\n",
    "        \n",
    "        return cost\n",
    "        \n",
    "        \n",
    "    # Points: 4.0\n",
    "    def compute_gradient(self, X, a2, a3, z2, y_enc):\n",
    "        \"\"\" Compute gradient using backpropagation.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        X : array, Input.\n",
    "        a2 : array, output of the hidden layer.\n",
    "        a3 : array, output of the output layer.\n",
    "        z2 : array, input of the hidden layer.\n",
    "        y_enc : array, one-hot encoded labels.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        grad1 : array, Gradient of the weight matrix w1.\n",
    "        grad2 : array, Gradient of the weight matrix w2.\n",
    "        grad3 : array, Gradient of the bias vector b1.\n",
    "        grad4 : array, Gradient of the bias vector b2.\n",
    "        \"\"\"\n",
    "        #TODO Implement\n",
    "        \n",
    "\n",
    "        return grad1, grad2, grad3, grad4\n",
    "        \n",
    "    # Points: 1.0\n",
    "    def inference(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, Predicted labels.\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO Implement\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def shuffle_train_data(self, X, Y):\n",
    "        \"\"\"called after each epoch\"\"\"\n",
    "        perm = np.random.permutation(Y.shape[0])\n",
    "        X_shuf = X[perm]\n",
    "        Y_shuf = Y[perm]\n",
    "        return X_shuf, Y_shuf\n",
    "    \n",
    "    # Points: 2.0\n",
    "    def train(self, X_train, Y_train, verbose=False):\n",
    "        \"\"\" Fit the model.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input.\n",
    "        y : array, Ground truth class labels.\n",
    "        verbose : bool, Print the training progress\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        #TODO Initialization\n",
    "        self.cost_ = []\n",
    "        \n",
    "\n",
    "        for i in range(self.epochs):\n",
    "        \n",
    "            if verbose:\n",
    "                print('\\nEpoch: %d/%d' % (i+1, self.epochs))\n",
    "\n",
    "\n",
    "                # feedforward and loss computation\n",
    "                \n",
    "\n",
    "                self.cost_.append(cost)\n",
    "\n",
    "                # compute gradient via backpropagation and update the weights\n",
    "                \n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $15.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Fully_connected_Neural_Network(n_output=10, \n",
    "                                    n_features=X_train.shape[1], \n",
    "                                    n_hidden=50, \n",
    "                                    l2=0.1, \n",
    "                                    epochs=1000, \n",
    "                                    lr=0.001,\n",
    "                                    batchsize=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.train(X_trainval, Y_trainval, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training error for every iteration\n",
    "# in every epoch\n",
    "\n",
    "# TODO Implement\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training error in every epoch\n",
    "# TODO Implement\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Training Accuracy\n",
    "# TODO Implement\n",
    "\n",
    "print('Training accuracy: %.2f%%' % (acc * 100))\n",
    "\n",
    "# Compute Test Accuracy\n",
    "# TODO Implement\n",
    "\n",
    "print('Test accuracy: %.2f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "You should provide a single Jupyter notebook (.ipynb file) as the solution. Put the names and student ids of your team members below. **Make sure to submit only 1 solution to only 1 tutor.**\n",
    "\n",
    "- Jane Doe, 123456\n",
    "- Jane Doe, 123456\n",
    "- Jane Doe, 123456"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points: 0.0 of 30.0 points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
